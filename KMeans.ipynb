{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import pandas as pd\r\n",
                "import glob\r\n",
                "import os\r\n",
                "import json\r\n",
                "from kneed import KneeLocator\r\n",
                "from sklearn.cluster import KMeans\r\n",
                "from scipy.spatial.distance import cdist\r\n",
                "from tqdm import tqdm\r\n",
                "from scipy.spatial.distance import cdist\r\n",
                "import numpy as np\r\n",
                "import matplotlib.pyplot as plt\r\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
                "from yellowbrick.cluster.elbow import kelbow_visualizer\r\n",
                "import random"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Hyperparameter"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "max_clusters = 10   #the maximum amount of subclusters, KMeans should generate for any man cluster\r\n",
                "min_df = 0.05   #for tfidf vectorization: ignore terms that appear in less then min_df (percent) articles\r\n",
                "max_df = 0.6    #for tfidf vectorization: ignore terms that appear in more than max_df (percent) articles\r\n",
                "backup_cluster_amount = 15  #if the elbow point can not be calculated, this value will be chosen as the optimal amount of clusters\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "def cluster_data(df):\r\n",
                "    processed_articles, amount_of_articles = count_articles(df)\r\n",
                "    tfidf, words = convert_to_tfidf(processed_articles)\r\n",
                "    knee, model = calculate_kMeans(tfidf, words)\r\n",
                "    cluster_words_list, df = gather_top_words(model, words, df)\r\n",
                "    df = assign_top_words(cluster_words_list, df)\r\n",
                "    df = date_month_publish(df)\r\n",
                "    df = drop_unwanted_columns(df)\r\n",
                "    create_cluster_files(df)\r\n",
                "    return amount_of_articles"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Load JSON Files into Pandas Dataframe"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Returns the preprocessed maintexts and count them. "
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "def count_articles(df):\r\n",
                "    amount_of_articles = len(df[\"filtered_maintext\"])\r\n",
                "    processed_articles = df['filtered_maintext']\r\n",
                "    return processed_articles, amount_of_articles"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Convert the filtered maintexts to a TF-IDF-Vector"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "def convert_to_tfidf(processed_articles):\r\n",
                "    tfidfconverter = TfidfVectorizer(lowercase=True, stop_words='english', min_df=min_df , max_df=max_df)  \r\n",
                "    tfidf = tfidfconverter.fit_transform(processed_articles)\r\n",
                "    words = tfidfconverter.get_feature_names()\r\n",
                "    return tfidf, words"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Create multiple KMeans models from the TF-IDF-Vector to find the best fitting clusters using the Elbow-method"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "def calculate_kMeans(tfidf, words):\r\n",
                "    distortions = []\r\n",
                "    all_kmeans_models = []\r\n",
                "\r\n",
                "    K = range(2,max_clusters)\r\n",
                "    X = np.matrix(tfidf.toarray())\r\n",
                "    \r\n",
                "    print(\"Finding the best K-Means model (searching elbow)...\")\r\n",
                "    try:\r\n",
                "        model = kelbow_visualizer(KMeans(max_iter=400), tfidf ,k=max_clusters)\r\n",
                "        model.fit(tfidf)\r\n",
                "        elbow = model.elbow_value_\r\n",
                "        print(\"Found elbow for the following amount of clusters: \" + str(elbow))\r\n",
                "        model.show(outpath = f'figure_{elbow}{random.randint(0,99)}.png')\r\n",
                "\r\n",
                "        print(\"Generating model with the optimal amount of clusters...\")\r\n",
                "\r\n",
                "        kMeans = KMeans(n_clusters=elbow, max_iter=400).fit(tfidf)\r\n",
                "        kMeans.predict(tfidf)\r\n",
                "        \r\n",
                "        return model.elbow_value_, kMeans\r\n",
                "    except:\r\n",
                "        print(\"Could not find an elbow point. Continuing with the backup cluster amount.\")\r\n",
                "        elbow = backup_cluster_amount\r\n",
                "\r\n",
                "        print(\"Generating model with the optimal amount of clusters...\")\r\n",
                "\r\n",
                "        kMeans = KMeans(n_clusters=elbow, max_iter=400).fit(tfidf)\r\n",
                "        kMeans.predict(tfidf)\r\n",
                "        return model.elbow_value_, kMeans"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "def calculate_knee(K, distortions, all_kmeans_models):\r\n",
                "    #if one elbow calculation fails, try again with different orientations of the curve\r\n",
                "    try:\r\n",
                "        kneedle = KneeLocator(K, distortions, S=1.0, curve=\"convex\", direction=\"decreasing\")\r\n",
                "        print(\"The elbow/knee point of the curve has distortion: \" + str(kneedle.knee_y) + \", K: \" + kneedle.knee)\r\n",
                "        kneedle.plot_knee()\r\n",
                "        return kneedle.knee\r\n",
                "    #except:\r\n",
                "        #print(\"Could not find an elbow point. Trying again with yellowbrick.\")\r\n",
                "        #pass\r\n",
                "    #try:\r\n",
                "        #X, y = load_nfl()\r\n",
                "        #model = KElbowVisualizer(KMeans(), k=10)\r\n",
                "        #model.fit(X)\r\n",
                "        #model.show()\r\n",
                "        #return kneedle.knee\r\n",
                "    except:\r\n",
                "        print(\"Could not find an elbow point. Continuing with the backup_cluster_amount.\")\r\n",
                "        plt.plot(K, distortions, 'bx-')\r\n",
                "        plt.xlabel('Amount of clusters')\r\n",
                "        plt.ylabel('Distortion')\r\n",
                "        plt.title('The distortion per amount of clusters')\r\n",
                "        plt.show()\r\n",
                "        return backup_cluster_amount"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Returns the top words foreach cluster"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "def gather_top_words(model, words, df):\r\n",
                "    # save the clusterIDs to the dataframe\r\n",
                "    # minus 2 because we start checking with 2 clusters\r\n",
                "    df[\"kMeans_ID\"] = model.labels_\r\n",
                "    cluster_words_list = []\r\n",
                "    common_words = model.cluster_centers_.argsort()[:,-1:-11:-1]\r\n",
                "    for num, centroid in enumerate(common_words):\r\n",
                "        cluster_words = []\r\n",
                "        for word in centroid:\r\n",
                "            cluster_words.append(words[word])\r\n",
                "        cluster_words_list.append(cluster_words)\r\n",
                "    return cluster_words_list, df"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "def assign_top_words(cluster_words_list, df):\r\n",
                "    row_words = []\r\n",
                "    for index, row in df.iterrows():\r\n",
                "        cluster = row.kMeans_ID\r\n",
                "        row_words.append(\", \".join(cluster_words_list[cluster]))\r\n",
                "\r\n",
                "    df['kMeans_topic_keywords'] = \", \".join(row_words)  #join list of top words to a single string sorted by frequency\r\n",
                "    return df\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Subcluster by release date\r\n",
                "\r\n",
                "We will determine each articles release date and sort them into individual json files."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "def getMonthYear(s):\r\n",
                "     return s.split('-')[0]+\"-\"+s.split('-')[1]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Generating folder structure\r\n",
                "\r\n",
                "The following code creates the desired folder hierarchy and names each cluster after the top 3 dominant words in each one. Within each cluster/folder we are subclustering all articles by their release date.\r\n",
                "The output json file has the format *year-month.json*. "
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Returns the year and month for the division into different events"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "def date_month_publish(df):\r\n",
                "    df['date_publish'] = pd.to_datetime(df['date_publish'])\r\n",
                "    df['month_year'] = df['date_publish'].apply(lambda x: getMonthYear(str(x)))\r\n",
                "    return df"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "generates the folder structure for the third level of clustering"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "def create_cluster_files(df):\r\n",
                "\r\n",
                "    item_LDA = df.iloc[0][\"LDA_topic_keywords\"]  # the top words from the lda clustering\r\n",
                "    item_LDA = item_LDA.split(\", \") # make string a list\r\n",
                "    cluster_id_LDA = df.iloc[0][\"LDA_ID\"]\r\n",
                "    level1_directory = f'./clustered_json/cluster_{cluster_id_LDA}-{item_LDA[0]}_{item_LDA[1]}_{item_LDA[2]}'\r\n",
                "    os.makedirs(level1_directory)   # create the directory for the LDa clustering level\r\n",
                "\r\n",
                "    for cluster_id_kMeans, data in df.groupby(df.kMeans_ID):\r\n",
                "        item_kMeans = \", \".join(data.kMeans_topic_keywords.tolist()[0])     # the top words from the kMeans clustering\r\n",
                "        print(f'{level1_directory}/cluster_{cluster_id_kMeans}-{item_kMeans[0]}_{item_kMeans[1]}_{item_kMeans[2]}_{item_kMeans[3]}_{item_kMeans[4]}')\r\n",
                "        os.makedirs(f'{level1_directory}/cluster_{cluster_id_kMeans}-{item_kMeans[0]}_{item_kMeans[1]}_{item_kMeans[2]}_{item_kMeans[3]}_{item_kMeans[4]}')     #create the directory for the level 2 kMeans clustering\r\n",
                "        for date, date_data in data.groupby(data.month_year):\r\n",
                "            json_data = date_data.to_json(orient='records', force_ascii=False, date_format='iso', date_unit='s')\r\n",
                "            parsed = json.loads(json_data)\r\n",
                "            with open(f'{level1_directory}/cluster_{cluster_id_kMeans}-{item_kMeans[0]}_{item_kMeans[1]}_{item_kMeans[2]}_{item_kMeans[3]}_{item_kMeans[4]}/{date}.json', 'w', encoding='utf-8') as f:  #create the json file for the level 3 timed events\r\n",
                "                f.write(json.dumps({\"data\": parsed}, indent=4, ensure_ascii=False))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "def drop_unwanted_columns(df):\r\n",
                "    df = df.drop(columns=[\"filtered_maintext\"], errors='ignore')   #drop unwanted attributes that should not be output into the json files.\\r\\n\"\r\n",
                "    return df"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "path = 'lda_clustered_json/'    # the path of the folder where the lda clustered articles (level 1) are in\r\n",
                "\r\n",
                "amount_of_articles = 0\r\n",
                "i = 0\r\n",
                "for filename in glob.glob(os.path.join(path, '*.json')):    #iterate through every json file in path\r\n",
                "    print(\"Executing code for main cluster \", str(i))\r\n",
                "    with open(filename, encoding='utf-8', mode='r') as currentFile:\r\n",
                "        df = pd.read_json(currentFile, orient='index')\r\n",
                "        amount_of_articles = cluster_data(df)\r\n",
                "    print(\"Finished multi level clustering of main cluster \" + str(i) + \". Processed \" + str(amount_of_articles) + \" articles.\")\r\n",
                "    i = i+1"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Executing code for main cluster  0\n",
                        "Finding the best K-Means model (searching elbow)...\n",
                        "Could not find an elbow point. Continuing with the backupo cluster amount.\n",
                        "Generating model with the optimal amount of clusters...\n"
                    ]
                },
                {
                    "output_type": "error",
                    "ename": "UnboundLocalError",
                    "evalue": "local variable 'model' referenced before assignment",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[1;32m<ipython-input-14-986a4dc1d853>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcurrentFile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrentFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mamount_of_articles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Finished multi level clustering of main cluster \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\". Processed \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamount_of_articles\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" articles.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m<ipython-input-3-c625a6dc134e>\u001b[0m in \u001b[0;36mcluster_data\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprocessed_articles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamount_of_articles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_articles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_tfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_articles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mknee\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_kMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mcluster_words_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgather_top_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0massign_top_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster_words_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m<ipython-input-6-8fb317abf879>\u001b[0m in \u001b[0;36mcalculate_kMeans\u001b[1;34m(tfidf, words)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Generating model with the optimal amount of clusters...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mkMeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melbow_value_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mkMeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'model' referenced before assignment"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.4",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.4 64-bit ('kccs': conda)"
        },
        "interpreter": {
            "hash": "d7b8994cdf06398bb1d6bc43ff8b2ebd874fbdc1b32ac998f9bfa73841ea6f5e"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}