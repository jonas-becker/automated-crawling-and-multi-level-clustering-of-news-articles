{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import pandas as pd\r\n",
                "import glob\r\n",
                "import os\r\n",
                "import json\r\n",
                "from kneed import KneeLocator\r\n",
                "from sklearn.cluster import KMeans\r\n",
                "from scipy.spatial.distance import cdist\r\n",
                "from tqdm import tqdm\r\n",
                "from scipy.spatial.distance import cdist\r\n",
                "import numpy as np\r\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "max_clusters = 20   #the maximum amount of subclusters, KMeans should generate for any man cluster\r\n",
                "min_df = 0.05   #for tfidf vectorization: ignore terms that appear in less then min_df (percent) articles\r\n",
                "max_df = 0.6    #for tfidf vectorization: ignore terms that appear in more than max_df (percent) articles\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "def cluster_data(df):\r\n",
                "    processed_articles, amount_of_articles = count_articles(df)\r\n",
                "    tfidf, words = convert_to_tfidf(processed_articles)\r\n",
                "    all_kmeans_models, K, distortions = calculate_kMeans(tfidf, words)\r\n",
                "    knee = calculate_knee(K, distortions)\r\n",
                "    cluster_words_list, df = gather_top_words(all_kmeans_models, knee, words, df)\r\n",
                "    df = assign_top_words(cluster_words_list, df)\r\n",
                "    df = date_month_publish(df)\r\n",
                "    df = drop_unwanted_columns(df)\r\n",
                "    create_cluster_files(df)\r\n",
                "    return amount_of_articles"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Load JSON Files into Pandas Dataframe"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "def count_articles(df):\r\n",
                "    amount_of_articles = len(df[\"filtered_maintext\"])\r\n",
                "    processed_articles = df['filtered_maintext']\r\n",
                "    return processed_articles, amount_of_articles"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "def convert_to_tfidf(processed_articles):\r\n",
                "    tfidfconverter = TfidfVectorizer(lowercase=True, stop_words='english', min_df=min_df , max_df=max_df)  \r\n",
                "    tfidf = tfidfconverter.fit_transform(processed_articles)\r\n",
                "    words = tfidfconverter.get_feature_names()\r\n",
                "    return tfidf, words"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "def calculate_kMeans(tfidf, words):\r\n",
                "    distortions = []\r\n",
                "    all_kmeans_models = []\r\n",
                "\r\n",
                "    K = range(2,max_clusters)\r\n",
                "    X = np.matrix(tfidf.toarray())\r\n",
                "    print(\"Generating the K-Means model...\")\r\n",
                "    for i in tqdm(K):\r\n",
                "        kMeans = KMeans(n_clusters=i, max_iter=400).fit(tfidf)\r\n",
                "        kMeans.predict(tfidf)\r\n",
                "        cluster_centers = kMeans.cluster_centers_\r\n",
                "        distortions.append(sum(np.min(cdist(X, cluster_centers, 'euclidean'), axis=1)) / X.shape[0])    #add distortion to list (will be used to calculate elbow)\r\n",
                "        all_kmeans_models.append(kMeans)    #add kmeans model to list\r\n",
                "\r\n",
                "    return all_kmeans_models, K, distortions"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "def calculate_knee(K, distortions):\r\n",
                "    try:\r\n",
                "        kneedle = KneeLocator(K, distortions, S=1.0, curve=\"convex\", direction=\"decreasing\")\r\n",
                "        print(\"The elbow/knee point of the curve has distortion: \" + str(kneedle.knee_y) + \", K: \" + kneedle.knee)\r\n",
                "        kneedle.plot_knee()\r\n",
                "        return kneedle.knee\r\n",
                "    except:\r\n",
                "        print(\"Could not find an elbow point. Continuing with the lowest distortion.\")\r\n",
                "        min_distortion = max(distortions)\r\n",
                "        min_index = distortions.index(min_distortion)\r\n",
                "        return K[min_index]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "def gather_top_words(all_kmeans_models, knee, words, df):\r\n",
                "    # save the clusterIDs to the dataframe\r\n",
                "    # minus 2 because we start checking with 2 clusters\r\n",
                "    df[\"kMeans_ID\"] = all_kmeans_models[knee-2].labels_\r\n",
                "    cluster_words_list = []\r\n",
                "    common_words = all_kmeans_models[knee-2].cluster_centers_.argsort()[:,-1:-11:-1]\r\n",
                "    for num, centroid in enumerate(common_words):\r\n",
                "        cluster_words = []\r\n",
                "        for word in centroid:\r\n",
                "            cluster_words.append(words[word])\r\n",
                "        cluster_words_list.append(cluster_words)\r\n",
                "    return cluster_words_list, df"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "def assign_top_words(cluster_words_list, df):\r\n",
                "    row_words = []\r\n",
                "    for index, row in df.iterrows():\r\n",
                "        cluster = row.kMeans_ID\r\n",
                "        row_words.append(cluster_words_list[cluster])\r\n",
                "    df['kMeans_topic_keywords'] = \", \".join(row_words)  #join list of top words to a single string sorted by frequency\r\n",
                "    return df\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Subcluster by release date\r\n",
                "\r\n",
                "We will determine each articles release date and sort them into individual json files."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "def getMonthYear(s):\r\n",
                "     return s.split('-')[0]+\"-\"+s.split('-')[1]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Generating folder structure\r\n",
                "\r\n",
                "The following code creates the desired folder hierarchy and names each cluster after the top 3 dominant words in each one. Within each cluster/folder we are subclustering all articles by their release date.\r\n",
                "The output json file has the format *year-month.json*. "
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "def date_month_publish(df):\r\n",
                "    df['date_publish'] = pd.to_datetime(df['date_publish'])\r\n",
                "    df['month_year'] = df['date_publish'].apply(lambda x: getMonthYear(str(x)))\r\n",
                "    return df"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "def create_cluster_files(df):\r\n",
                "\r\n",
                "    item_LDA = df.iloc[0][\"topic_keywords\"]  # the top words from the lda clustering\r\n",
                "    item_LDA = item_LDA.split(\", \") # make string a list\r\n",
                "    cluster_id_LDA = df.iloc[0][\"LDA_ID\"]\r\n",
                "    print(\"ILOC: \" + str(cluster_id_LDA))\r\n",
                "    level1_directory = f'./clustered_json/cluster_{cluster_id_LDA}-{item_LDA[0]}_{item_LDA[1]}_{item_LDA[2]}'\r\n",
                "    os.makedirs(level1_directory)   # create the directory for the LDa clustering level\r\n",
                "\r\n",
                "    for cluster_id_kMeans, data in df.groupby(df.kMeans_ID):\r\n",
                "        item_kMeans = data.kMeans_words.tolist()[0]     # the top words from the kMeans clustering\r\n",
                "        os.makedirs(f'{level1_directory}/cluster_{cluster_id_kMeans}-{item_kMeans[0]}_{item_kMeans[1]}_{item_kMeans[2]}_{item_kMeans[3]}_{item_kMeans[4]}')     #create the directory for the level 2 kMeans clustering\r\n",
                "        for date, date_data in data.groupby(data.month_year):\r\n",
                "            json_data = date_data.to_json(orient='records', force_ascii=False, date_format='iso', date_unit='s')\r\n",
                "            parsed = json.loads(json_data)\r\n",
                "            with open(f'{level1_directory}/cluster_{cluster_id_kMeans}-{item_kMeans[0]}_{item_kMeans[1]}_{item_kMeans[2]}_{item_kMeans[3]}_{item_kMeans[4]}/{date}.json', 'w', encoding='utf-8') as f:  #create the json file for the level 3 timed events\r\n",
                "                f.write(json.dumps({\"data\": parsed}, indent=4, ensure_ascii=False))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "def drop_unwanted_columns(df):\r\n",
                "    df = df.drop(columns=[\"filtered_maintext\", \"month_year\"], errors='ignore')   #drop unwanted attributes that should not be output into the json files.\\r\\n\"\r\n",
                "    return df"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "path = 'lda_clustered_json/'    # the path of the folder where the lda clustered articles (level 1) are in\r\n",
                "\r\n",
                "amount_of_articles = 0\r\n",
                "i = 0\r\n",
                "for filename in glob.glob(os.path.join(path, '*.json')):    #iterate through every json file in path\r\n",
                "    print(\"Executing code for main cluster \", str(i))\r\n",
                "    with open(filename, encoding='utf-8', mode='r') as currentFile:\r\n",
                "        df = pd.read_json(currentFile, orient='index')\r\n",
                "        amount_of_articles = cluster_data(df)\r\n",
                "    print(\"Finished multi level clustering of main cluster \" + str(i) + \". Processed \" + str(amount_of_articles) + \" articles.\")\r\n",
                "    i = i+1"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Executing code for main cluster  0\n",
                        "Generating the K-Means model...\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        " 17%|█▋        | 3/18 [01:38<08:45, 35.01s/it]"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.4",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.4 64-bit ('kccs': conda)"
        },
        "interpreter": {
            "hash": "d7b8994cdf06398bb1d6bc43ff8b2ebd874fbdc1b32ac998f9bfa73841ea6f5e"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}