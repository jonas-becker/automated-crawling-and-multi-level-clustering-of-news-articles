{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "import pandas as pd\r\n",
                "import glob\r\n",
                "import os\r\n",
                "import json\r\n",
                "from sklearn.cluster import KMeans\r\n",
                "import numpy as np\r\n",
                "import matplotlib.pyplot as plt\r\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
                "from yellowbrick.cluster.elbow import kelbow_visualizer"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Hyperparameter"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "max_clusters = 30   #the maximum amount of subclusters, KMeans should generate for any man cluster\r\n",
                "min_df = 0.05   #for tfidf vectorization: ignore terms that appear in less then min_df (percent) articles\r\n",
                "max_df = 0.6    #for tfidf vectorization: ignore terms that appear in more than max_df (percent) articles\r\n",
                "backup_cluster_amount = 10  #if the elbow point can not be calculated, this value will be chosen as the optimal amount of clusters\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Mainfunction for the kMeans clustering"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "def cluster_data(df, i):\r\n",
                "    processed_articles, amount_of_articles = count_articles(df)\r\n",
                "    tfidf, words = convert_to_tfidf(processed_articles)\r\n",
                "    model = calculate_kMeans(tfidf, i)\r\n",
                "    cluster_words_list, df = gather_top_words(model, words, df)\r\n",
                "    df = assign_top_words(cluster_words_list, df)\r\n",
                "    df = date_month_publish(df)\r\n",
                "    df = drop_unwanted_columns(df)\r\n",
                "    create_cluster_files(df)\r\n",
                "    return amount_of_articles"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Load JSON Files into Pandas Dataframe"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Returns the preprocessed maintexts and count them. "
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "def count_articles(df):\r\n",
                "    amount_of_articles = len(df[\"filtered_maintext\"])\r\n",
                "    processed_articles = df['filtered_maintext']\r\n",
                "    return processed_articles, amount_of_articles"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Convert the filtered maintexts to a TF-IDF-Vector"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "def convert_to_tfidf(processed_articles):\r\n",
                "    tfidfconverter = TfidfVectorizer(lowercase=True, stop_words='english', min_df=min_df , max_df=max_df)  \r\n",
                "    tfidf = tfidfconverter.fit_transform(processed_articles)\r\n",
                "    words = tfidfconverter.get_feature_names()\r\n",
                "    return tfidf, words"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Create multiple KMeans models from the TF-IDF-Vector to find the best fitting clusters using the Elbow-method"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "def calculate_kMeans(tfidf, i):\r\n",
                "\r\n",
                "    K = range(2,max_clusters)\r\n",
                "    X = np.matrix(tfidf.toarray())\r\n",
                "    \r\n",
                "    print(\"Finding the best K-Means model (searching elbow)...\")\r\n",
                "    try:\r\n",
                "        model = kelbow_visualizer(KMeans(max_iter=400), tfidf ,k=max_clusters)\r\n",
                "        elbow = model.elbow_value_\r\n",
                "        print(\"Found elbow for the following amount of clusters: \" + str(elbow))\r\n",
                "\r\n",
                "        print(\"Generating model with the optimal amount of clusters...\")\r\n",
                "\r\n",
                "        kMeans = KMeans(n_clusters=elbow, max_iter=400)\r\n",
                "        kMeans.fit(tfidf)\r\n",
                "        kMeans.predict(tfidf)\r\n",
                "        kMeans.finalize()\r\n",
                "        kMeans.show(outpath=f\"kmeans_png/kelbow_{i}.png\")\r\n",
                "        return kMeans\r\n",
                "    except:\r\n",
                "        print(\"Could not find an elbow point. Continuing with the backup cluster amount.\")\r\n",
                "        elbow = backup_cluster_amount\r\n",
                "\r\n",
                "        print(\"Generating model with the optimal amount of clusters...\")\r\n",
                "\r\n",
                "        kMeans = KMeans(n_clusters=elbow, max_iter=400).fit(tfidf)\r\n",
                "        kMeans.predict(tfidf)\r\n",
                "        return kMeans"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Returns the top words foreach cluster"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "source": [
                "def gather_top_words(model, words, df):\r\n",
                "    # save the clusterIDs to the dataframe\r\n",
                "    # minus 2 because we start checking with 2 clusters\r\n",
                "    df[\"kMeans_ID\"] = model.labels_\r\n",
                "    cluster_words_list = []\r\n",
                "    common_words = model.cluster_centers_.argsort()[:,-1:-11:-1]\r\n",
                "    for num, centroid in enumerate(common_words):\r\n",
                "        cluster_words = []\r\n",
                "        for word in centroid:\r\n",
                "            cluster_words.append(words[word])\r\n",
                "        cluster_words_list.append(cluster_words)\r\n",
                "\r\n",
                "    return cluster_words_list, df"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Assign top words of clusters to dataframe"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "source": [
                "def assign_top_words(cluster_words_list, df):\r\n",
                "    row_words = []\r\n",
                "    for index, row in df.iterrows():\r\n",
                "        cluster = row.kMeans_ID\r\n",
                "        row_words.append(\", \".join(cluster_words_list[cluster]))\r\n",
                "\r\n",
                "    df['kMeans_topic_keywords'] = row_words  #join list of top words to a single string sorted by frequency\r\n",
                "    return df\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Subcluster by release date\r\n",
                "\r\n",
                "We will determine each articles release date and sort them into individual json files."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "source": [
                "def getMonthYear(s):\r\n",
                "     return s.split('-')[0]+\"-\"+s.split('-')[1]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Generating folder structure\r\n",
                "\r\n",
                "The following code creates the desired folder hierarchy and names each cluster after the top 3 dominant words in each one. Within each cluster/folder we are subclustering all articles by their release date.\r\n",
                "The output json file has the format *year-month.json*. "
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Returns the year and month for the division into different events"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "source": [
                "def date_month_publish(df):\r\n",
                "    df['date_publish'] = pd.to_datetime(df['date_publish'])\r\n",
                "    df['year_month'] = df['date_publish'].apply(lambda x: getMonthYear(str(x)))\r\n",
                "    return df"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Generates the folder structure for the third level of clustering"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "source": [
                "def create_cluster_files(df):\r\n",
                "\r\n",
                "    item_LDA = df.iloc[0][\"LDA_topic_keywords\"]  # the top words from the lda clustering\r\n",
                "    item_LDA = item_LDA.split(\", \") # make string a list\r\n",
                "    cluster_id_LDA = df.iloc[0][\"LDA_ID\"]\r\n",
                "    level1_directory = f'./clustered_json/cluster_{cluster_id_LDA}-{item_LDA[0]}_{item_LDA[1]}_{item_LDA[2]}'\r\n",
                "    os.makedirs(level1_directory)   # create the directory for the LDa clustering level\r\n",
                "\r\n",
                "    for cluster_id_kMeans, data in df.groupby(df.kMeans_ID):\r\n",
                "        item_kMeans = data.iloc[0][\"kMeans_topic_keywords\"].split(\", \")     # the top words from the kMeans clustering\r\n",
                "        os.makedirs(f'{level1_directory}/cluster_{cluster_id_kMeans}-{item_kMeans[0]}_{item_kMeans[1]}_{item_kMeans[2]}_{item_kMeans[3]}_{item_kMeans[4]}')     #create the directory for the level 2 kMeans clustering\r\n",
                "        for date, date_data in data.groupby(data.year_month):\r\n",
                "            json_data = date_data.to_json(orient='records', force_ascii=False, date_format='iso', date_unit='s')\r\n",
                "            parsed = json.loads(json_data)\r\n",
                "            with open(f'{level1_directory}/cluster_{cluster_id_kMeans}-{item_kMeans[0]}_{item_kMeans[1]}_{item_kMeans[2]}_{item_kMeans[3]}_{item_kMeans[4]}/{date}.json', 'w', encoding='utf-8') as f:  #create the json file for the level 3 timed events\r\n",
                "                f.write(json.dumps({\"data\": parsed}, indent=4, ensure_ascii=False))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Drops all unwanted columns of the dataframe"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "source": [
                "def drop_unwanted_columns(df):\r\n",
                "    df = df.drop(columns=[\"filtered_maintext\"], errors='ignore')   #drop unwanted attributes that should not be output into the json files.\\r\\n\"\r\n",
                "    return df"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Stores clusters in folders grouped by their month and year of publish"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "source": [
                "path = 'lda_clustered_json/'    # the path of the folder where the lda clustered articles (level 1) are in\r\n",
                "\r\n",
                "amount_of_articles = 0\r\n",
                "i = 0\r\n",
                "for filename in glob.glob(os.path.join(path, '*.json')):    #iterate through every json file in path\r\n",
                "    print(\"Executing code for main cluster \", str(filename))\r\n",
                "    with open(filename, encoding='utf-8', mode='r') as currentFile:\r\n",
                "        df = pd.read_json(currentFile, orient='index')\r\n",
                "        amount_of_articles = cluster_data(df, i)\r\n",
                "    print(\"Finished multi level clustering of main cluster \" + str(i) + \". Processed \" + str(amount_of_articles) + \" articles.\")\r\n",
                "    i = i+1"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Executing code for main cluster  lda_clustered_json\\cluster_6-trump_ president_ obama.json\n"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.11",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.11 64-bit ('kcc': conda)"
        },
        "interpreter": {
            "hash": "21ab713a94b9dff8ceab4d966c04cbd434fb99e8b3685c982045bb942eef746a"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}