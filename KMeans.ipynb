{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 155,
            "source": [
                "import pandas as pd\r\n",
                "import glob\r\n",
                "import os\r\n",
                "import json\r\n",
                "from kneed import KneeLocator\r\n",
                "from sklearn.cluster import KMeans\r\n",
                "from scipy.spatial.distance import cdist\r\n",
                "from tqdm import tqdm\r\n",
                "from scipy.spatial.distance import cdist\r\n",
                "import numpy as np\r\n",
                "import matplotlib.pyplot as plt\r\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
                "from yellowbrick.cluster.elbow import kelbow_visualizer\r\n",
                "from yellowbrick.datasets.loaders import load_nfl"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 156,
            "source": [
                "max_clusters = 20   #the maximum amount of subclusters, KMeans should generate for any man cluster\r\n",
                "min_df = 0.05   #for tfidf vectorization: ignore terms that appear in less then min_df (percent) articles\r\n",
                "max_df = 0.6    #for tfidf vectorization: ignore terms that appear in more than max_df (percent) articles\r\n",
                "backup_cluster_amount = 15  #if the elbow point can not be calculated, this value will be chosen as the optimal amount of clusters\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 157,
            "source": [
                "def cluster_data(df):\r\n",
                "    processed_articles, amount_of_articles = count_articles(df)\r\n",
                "    tfidf, words = convert_to_tfidf(processed_articles)\r\n",
                "    knee, model = calculate_kMeans(tfidf, words)\r\n",
                "    cluster_words_list, df = gather_top_words(model, words, df)\r\n",
                "    df = assign_top_words(cluster_words_list, df)\r\n",
                "    df = date_month_publish(df)\r\n",
                "    df = drop_unwanted_columns(df)\r\n",
                "    create_cluster_files(df)\r\n",
                "    return amount_of_articles"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Load JSON Files into Pandas Dataframe"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 158,
            "source": [
                "def count_articles(df):\r\n",
                "    amount_of_articles = len(df[\"filtered_maintext\"])\r\n",
                "    processed_articles = df['filtered_maintext']\r\n",
                "    return processed_articles, amount_of_articles"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 159,
            "source": [
                "def convert_to_tfidf(processed_articles):\r\n",
                "    tfidfconverter = TfidfVectorizer(lowercase=True, stop_words='english', min_df=min_df , max_df=max_df)  \r\n",
                "    tfidf = tfidfconverter.fit_transform(processed_articles)\r\n",
                "    words = tfidfconverter.get_feature_names()\r\n",
                "    return tfidf, words"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 160,
            "source": [
                "def calculate_kMeans(tfidf, words):\r\n",
                "    distortions = []\r\n",
                "    all_kmeans_models = []\r\n",
                "\r\n",
                "    K = range(2,max_clusters)\r\n",
                "    X = np.matrix(tfidf.toarray())\r\n",
                "    \r\n",
                "    print(\"Generating the K-Means model...\")\r\n",
                "    model = kelbow_visualizer(KMeans(max_iter=400), tfidf ,k=max_clusters)\r\n",
                "\r\n",
                "    print(\"Found elbow for the following amount of clusters: \" + str(model.elbow_value_))\r\n",
                "    print(\"Generating model with the optimal amount of clusters...\")\r\n",
                "\r\n",
                "    kMeans = KMeans(n_clusters=model.elbow_value_, max_iter=400).fit(tfidf)\r\n",
                "    kMeans.predict(tfidf)\r\n",
                "    \r\n",
                "    return model.elbow_value_, kMeans"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 161,
            "source": [
                "def calculate_knee(K, distortions, all_kmeans_models):\r\n",
                "    #if one elbow calculation fails, try again with different orientations of the curve\r\n",
                "    try:\r\n",
                "        kneedle = KneeLocator(K, distortions, S=1.0, curve=\"convex\", direction=\"decreasing\")\r\n",
                "        print(\"The elbow/knee point of the curve has distortion: \" + str(kneedle.knee_y) + \", K: \" + kneedle.knee)\r\n",
                "        kneedle.plot_knee()\r\n",
                "        return kneedle.knee\r\n",
                "    #except:\r\n",
                "        #print(\"Could not find an elbow point. Trying again with yellowbrick.\")\r\n",
                "        #pass\r\n",
                "    #try:\r\n",
                "        #X, y = load_nfl()\r\n",
                "        #model = KElbowVisualizer(KMeans(), k=10)\r\n",
                "        #model.fit(X)\r\n",
                "        #model.show()\r\n",
                "        #return kneedle.knee\r\n",
                "    except:\r\n",
                "        print(\"Could not find an elbow point. Continuing with the backup_cluster_amount.\")\r\n",
                "        plt.plot(K, distortions, 'bx-')\r\n",
                "        plt.xlabel('Amount of clusters')\r\n",
                "        plt.ylabel('Distortion')\r\n",
                "        plt.title('The distortion per amount of clusters')\r\n",
                "        plt.show()\r\n",
                "        return backup_cluster_amount"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 162,
            "source": [
                "def gather_top_words(model, words, df):\r\n",
                "    # save the clusterIDs to the dataframe\r\n",
                "    # minus 2 because we start checking with 2 clusters\r\n",
                "    df[\"kMeans_ID\"] = model.labels_\r\n",
                "    cluster_words_list = []\r\n",
                "    common_words = model.cluster_centers_.argsort()[:,-1:-11:-1]\r\n",
                "    for num, centroid in enumerate(common_words):\r\n",
                "        cluster_words = []\r\n",
                "        for word in centroid:\r\n",
                "            cluster_words.append(words[word])\r\n",
                "        cluster_words_list.append(cluster_words)\r\n",
                "    return cluster_words_list, df"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 163,
            "source": [
                "def assign_top_words(cluster_words_list, df):\r\n",
                "    row_words = []\r\n",
                "    for index, row in df.iterrows():\r\n",
                "        cluster = row.kMeans_ID\r\n",
                "        row_words.append(\", \".join(cluster_words_list[cluster]))\r\n",
                "\r\n",
                "    df['kMeans_topic_keywords'] = \", \".join(row_words)  #join list of top words to a single string sorted by frequency\r\n",
                "    return df\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Subcluster by release date\r\n",
                "\r\n",
                "We will determine each articles release date and sort them into individual json files."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 164,
            "source": [
                "def getMonthYear(s):\r\n",
                "     return s.split('-')[0]+\"-\"+s.split('-')[1]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Generating folder structure\r\n",
                "\r\n",
                "The following code creates the desired folder hierarchy and names each cluster after the top 3 dominant words in each one. Within each cluster/folder we are subclustering all articles by their release date.\r\n",
                "The output json file has the format *year-month.json*. "
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 165,
            "source": [
                "def date_month_publish(df):\r\n",
                "    df['date_publish'] = pd.to_datetime(df['date_publish'])\r\n",
                "    df['month_year'] = df['date_publish'].apply(lambda x: getMonthYear(str(x)))\r\n",
                "    return df"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 166,
            "source": [
                "def create_cluster_files(df):\r\n",
                "\r\n",
                "    item_LDA = df.iloc[0][\"LDA_topic_keywords\"]  # the top words from the lda clustering\r\n",
                "    item_LDA = item_LDA.split(\", \") # make string a list\r\n",
                "    cluster_id_LDA = df.iloc[0][\"LDA_ID\"]\r\n",
                "    print(\"ILOC: \" + str(cluster_id_LDA))\r\n",
                "    level1_directory = f'./clustered_json/cluster_{cluster_id_LDA}-{item_LDA[0]}_{item_LDA[1]}_{item_LDA[2]}'\r\n",
                "    os.makedirs(level1_directory)   # create the directory for the LDa clustering level\r\n",
                "\r\n",
                "    for cluster_id_kMeans, data in df.groupby(df.kMeans_ID):\r\n",
                "        item_kMeans = data.kMeans_topic_keywords.tolist()[0]     # the top words from the kMeans clustering\r\n",
                "        os.makedirs(f'{level1_directory}/cluster_{cluster_id_kMeans}-{item_kMeans[0]}_{item_kMeans[1]}_{item_kMeans[2]}_{item_kMeans[3]}_{item_kMeans[4]}')     #create the directory for the level 2 kMeans clustering\r\n",
                "        for date, date_data in data.groupby(data.month_year):\r\n",
                "            json_data = date_data.to_json(orient='records', force_ascii=False, date_format='iso', date_unit='s')\r\n",
                "            parsed = json.loads(json_data)\r\n",
                "            with open(f'{level1_directory}/cluster_{cluster_id_kMeans}-{item_kMeans[0]}_{item_kMeans[1]}_{item_kMeans[2]}_{item_kMeans[3]}_{item_kMeans[4]}/{date}.json', 'w', encoding='utf-8') as f:  #create the json file for the level 3 timed events\r\n",
                "                f.write(json.dumps({\"data\": parsed}, indent=4, ensure_ascii=False))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 167,
            "source": [
                "def drop_unwanted_columns(df):\r\n",
                "    df = df.drop(columns=[\"filtered_maintext\"], errors='ignore')   #drop unwanted attributes that should not be output into the json files.\\r\\n\"\r\n",
                "    return df"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 168,
            "source": [
                "path = 'lda_clustered_json/'    # the path of the folder where the lda clustered articles (level 1) are in\r\n",
                "\r\n",
                "amount_of_articles = 0\r\n",
                "i = 0\r\n",
                "for filename in glob.glob(os.path.join(path, '*.json')):    #iterate through every json file in path\r\n",
                "    print(\"Executing code for main cluster \", str(i))\r\n",
                "    with open(filename, encoding='utf-8', mode='r') as currentFile:\r\n",
                "        df = pd.read_json(currentFile, orient='index')\r\n",
                "        amount_of_articles = cluster_data(df)\r\n",
                "    print(\"Finished multi level clustering of main cluster \" + str(i) + \". Processed \" + str(amount_of_articles) + \" articles.\")\r\n",
                "    i = i+1"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Executing code for main cluster  0\n",
                        "Generating the K-Means model...\n"
                    ]
                },
                {
                    "output_type": "error",
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[1;32m<ipython-input-168-986a4dc1d853>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcurrentFile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrentFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mamount_of_articles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Finished multi level clustering of main cluster \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\". Processed \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamount_of_articles\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" articles.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m<ipython-input-157-098c27c4fec3>\u001b[0m in \u001b[0;36mcluster_data\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprocessed_articles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamount_of_articles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_articles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_tfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_articles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mknee\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_kMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mcluster_words_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgather_top_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_kmeans_models\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mknee\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0massign_top_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster_words_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m<ipython-input-160-bdcb5dbc406d>\u001b[0m in \u001b[0;36mcalculate_kMeans\u001b[1;34m(tfidf, words)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Generating the K-Means model...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkelbow_visualizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_clusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Found elbow for the following amount of clusters: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melbow_value_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\miniconda3\\envs\\kccs\\lib\\site-packages\\yellowbrick\\cluster\\elbow.py\u001b[0m in \u001b[0;36mkelbow_visualizer\u001b[1;34m(model, X, y, ax, k, metric, timings, locate_elbow, show, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m     )\n\u001b[1;32m--> 507\u001b[1;33m     \u001b[0moz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\miniconda3\\envs\\kccs\\lib\\site-packages\\yellowbrick\\cluster\\elbow.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m             \u001b[1;31m# Set the k value and fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;31m# Append the time and score to our plottable metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\miniconda3\\envs\\kccs\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1022\u001b[0m             \u001b[1;31m# run a k-means once\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1023\u001b[1;33m             labels, inertia, centers, n_iter_ = kmeans_single(\n\u001b[0m\u001b[0;32m   1024\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenters_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\miniconda3\\envs\\kccs\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[1;34m(X, sample_weight, centers_init, max_iter, verbose, x_squared_norms, tol, n_threads)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         elkan_iter(X, sample_weight, centers, centers_new,\n\u001b[0m\u001b[0;32m    386\u001b[0m                    \u001b[0mweight_in_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenter_half_distances\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m                    \u001b[0mdistance_next_center\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper_bounds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower_bounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.4",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.4 64-bit ('kccs': conda)"
        },
        "interpreter": {
            "hash": "d7b8994cdf06398bb1d6bc43ff8b2ebd874fbdc1b32ac998f9bfa73841ea6f5e"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}