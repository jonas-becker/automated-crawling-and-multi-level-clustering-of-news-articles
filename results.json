{
  "0":{
    "title":"thegdb comments on [D] Gary Marcus Tweet on OpenAI still has not changed misleading blog post about \"solving the Rubik's cube\"",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,298 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[D] Gary Marcus Tweet on OpenAI still has not changed misleading blog post about solving the Rubik's cube (self.MachineLearning) submitted 25 days ago by chansung18 Post a comment! view the rest of the comments\u2192  []thegdbOpenAI 43 points44 points45 points 25 days ago(9 children) Granted, many newspapers reported is as if the ML part had also worked out how to solve the cube, and OpenAI have not tried to correct the misreporting, but I'm not sure that's feasible or even necessary. We ping journalists to ask them to correct factual errors in reporting when we see them (though they may not always agree with our corrections). For example, the Washington Post article (https:www.washingtonpost.comtechnology20191018this-robotic-hand-learned-solve-rubiks-cube-its-own-just-like-human) feels misleading, so we've emailed them and linked them to the relevant sections in our blog post (namely, that we use Kociemba's algorithm as you mention). If you see other articles that need correcting, always feel free to let me know \u2014 gdb@openai.com!  []tradediscount 6 points7 points8 points 25 days ago(0 children) I stand corrected. Nice one!  []openaievolution 1 point2 points3 points 24 days ago*(1 child) Tbf this is an artifact of your science by press release strategy as well. If you release a public preprint first, journalists will have an easier time sourcing opinions from other well informed folks in the field, and presumably the reporting would get better.Zach Lipton elaborates more on this point in this thread here: https:twitter.comzacharyliptonstatus1184237037622136832  PS: To be clear, I am not arguing for not doing press releases, but rather putting out a preprint first and allowing some time bw the preprint and the press release.  []whatevenarecomputers 2 points3 points4 points 24 days ago(0 children) When has a reporter ever erred on the side of measured response from experts in the field instead of writing up the most sensationalized version they could concoct and still maintain plausible deniability?  []garymarcus -2 points-1 points0 points 25 days ago(4 children) why not post some sort of clarification on your own site? it is clear that your blog was prone to misinterpretation.  []Ghenlezo 31 points32 points33 points 25 days ago*(3 children) I am a little baffled by your reaction to this. You worked with robots before, yes? The fact that they actually got a hand to sort of work on an intricate task, and to the point that it is able to adapt to many perturbations, is astonishing.  Did they make any deep algorithmic breakthroughs? No, but nor did they claim to. They scale things up and see how far they can push them. This is super useful work. AI is an engineering discipline not a science, and unfortunately is half-ashamed of this fact.  Was anyone with any literacy fooled into thinking the cube-solver was learned? It says right in the post it was done symbolically. Even if the solver was learned that is, frankly, much less impressive than demonstrating modestly robust physical dexterity. Your critique of the major deep learning figures is of naked emperors. But I see scholars constantly apologizing for their state of undress, baffled at how well they are weathering ever-colder temperatures.  One begins to wonder if invisible wool does, in fact, exist.  []garymarcus 1 point2 points3 points 25 days ago(1 child) Curious for your take compared to the much less hyped Baoding balls the week before. Here's what I said in the tweet that you apparently didn't read: I will say again that the work itself is impressive, but mischaracterized, and that a better title would have been manipulating a Rubik's cube using reinforcement learning or progress in manipulation with dextrous robotic hands or similar lines.  []Ghenlezo 32 points33 points34 points 25 days ago*(0 children) I read your \u201cdetailed analysis meme. I have not read every tweet you wrote on the matter. I am glad you admit that the progress on the actual task they were attempting was good. Your image macro makes it look as if you are dismissing impressive work for trivial misunderstandings. Anyone with any experience in this stuff knows, watching the video not even reading the paper, that what is impressive is the dexterity, and that solving a cube is trivial symbolically. Imagine we're watching a ski race on television and someone said, \u201cYou know they may be able to ski down fine but what they don't show you is they don't ski up! They use a chairlift! That's the impression your image macro gives me. They say right in the post that it was done symbolically. Failure to spoon feed journalists is not a sin in my book.  []tristes_tigres 0 points1 point2 points 24 days ago(0 children) AI is an engineering discipline not a science Engineering consists in applying known scientific principles to solve the real-world problems. AI at this point is barely more than alchemy - a compendium of techniques that seem to work from time to time for unknown reasons, that is very useful for extracting funding from wealthy patrons hoping to expand their riches.  []master_yoda_1 -2 points-1 points0 points 24 days ago(0 children) The same nonsensical hype you guys created with gpt-2 by not releasing it, and later on you proved yourself wrong and released it. What is your comment about that? Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 24965 on r2-app-0e27d6cf2c9b3e66a at 2019-11-14 10:51:30.039630+00:00 running 0eef139 country code: US. "
  },
  "1":{
    "title":"[R] How can we fool LIME and SHAP? Adversarial Attacks on Post hoc Explanation Methods -- post hoc explanation methods can be games to say whatever you want : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 956 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Research[R] How can we fool LIME and SHAP? Adversarial Attacks on Post hoc Explanation Methods -- post hoc explanation methods can be games to say whatever you want (arxiv.org) submitted 5 days ago by agamemnonlost Post a comment!  []AGI_aint_happening 7 points8 points9 points 5 days ago(12 children) This is awesome work, but not at all surprising. SHAPLIME are really bad at explaining what a model is doing, and are only popular because they're the only tools that can be applied to arbitrary models.  []arnaudvl 2 points3 points4 points 4 days ago(1 child) There are other methodstools that have quality open source implementations and can be applied to black box models like Anchors or Counterfactuals, implemented in https:github.comSeldonIOalibi. One of the implemented methods is called Counterfactuals Guided by Prototypes (https:docs.seldon.ioprojectsalibienstablemethodsCFProto.html; https:arxiv.orgabs1907.02584) and explicitly deals with the out-of-distribution issue of the perturbations mentioned in the How can we fool LIME and SHAP? paper.  []eric_he 0 points1 point2 points 4 days ago(0 children) Thanks for sharing, will check it out  []eric_he 3 points4 points5 points 5 days ago(9 children) Can you expand on why you think SHAP is really bad? I use it all the time, it always says stuff that aligns with my intuition and it's helped me debug datasets when I thought predictions didn't look quite right.  []arnaudvl 5 points6 points7 points 4 days ago(1 child) One of the issues is the treatment of categorical variables and the need for a non-informative background value for the KernelExplainer. This is carefully explained here: https:github.comslundbergshapissues451. This could however be addressed by adding a conversion from categorical to numerical values inferred from a training set as in https:docs.seldon.ioprojectsalibienstablemethodsCFProto.html#Categorical-Variables.  []eric_he 1 point2 points3 points 4 days ago(0 children) So I use shap for xgboost, in which case all categorical values are mapped to integers. This would be why I never ran into this problem? And thank you for the informative answer!  []dddetr 6 points7 points8 points 5 days ago(0 children) I think SHAP is pretty good about determining what's going on in a model as well. I think this paper is just pointing out that how it goes about sampling can be exploited\u2014not that there's something inherently flawed with the shapley value technique. No need to stop using SHAP, just maybe think twice if it's a sensitive situation where someone could have a reason to try and trick you.  []AGI_aint_happening -2 points-1 points0 points 5 days ago(5 children) To make the SHAP scores sum up to the prediction, it bakes in interactions into the scores in weird ways that are hard to track. I've seen instances where half the time when a variable equals some value, it has positive score and the other half negative score - how do you interpret that? You'd be better off computing the correlation between an input and the model's prediction, imo.  []eric_he 1 point2 points3 points 4 days ago(2 children) Well, I think the a fixed feature value having positive score in some situations and negative scores in others is actually a feature of non linear models rather than a bug. The fact that SHAP can mirror such a capability is a plus rather than a minus to me. The most concrete example for why this is desired and expected is when I was measuring how much users would like an e-commerce listing. An expensive listing would have a negative SHAP value for my listing price feature if the user historically clicked or browsed normal or cheap listings, and a positive SHAP value if the user clicked or bought mostly expensive listings.  Seeing this firsthand really validated to me how powerful the model was and how flexible SHAP values were.  []AGI_aint_happening 1 point2 points3 points 4 days ago(1 child) So, in your example, the problem is you don't know if the shap score is positive because a user likes cheap listings or not. It could be positive because they are from a certain region, clicked on a similar link in the past, haven't clicked on anything in a week, or an interaction with any other feature fed into the model. And you have no way of knowing. Worse, it gives you just enough information that you can make up a convincing story about it being due to user's historical preferences. There is some information in the scores, no doubt, but treating them as a full description of the model will get you in trouble. For e-commerce that may be fine, I'm more worried about sensitive areas (e.g. loan applications, healthcare).  []eric_he -1 points0 points1 point 4 days ago(0 children) you don't know if the shap score is positive because a user likes cheap listings or not I used shap's partial dependence plot on the listing price feature, which automatically labeled user preferred price as the feature affecting most of the interaction in aggregate across the sample. I can't see how this wouldn't be just fine as an explanation, so long as we are not in an adversarial situation.  And I'm not sure it would even be possible to break down such an explanation to the level you're looking for. If we had to enumerate every single interaction effect then we would have the entire model, and obviously nobody can internalize in their head the model function of a gradient boosting tree! SHAP just looks in the local area to obtain its SHAP values, but clearly a good explanation must either lose some information of the model's global structure or lose some level of granularity. I'm more worried about sensitive areas I'd agree that we have a long way to go before society can be fully comfortable using black box models in such spaces.  But I do think it's strange that we hold models to such a different standard than actual people - it's not like I can truly understand the recommendations my doctor can give me either.  []deeceeo 2 points3 points4 points 4 days ago(1 child) Can't this happen if you have an XOR-like condition in the input data where the effect of a variable changes depending on another variable? It seems like this could be a fairly common occurrence, and a correlation could make it look like the variable doesn't matter at all.  []eric_he 1 point2 points3 points 4 days ago(0 children) This is my understanding as well  []sameer_s 2 points3 points4 points 3 days ago(1 child) Coauthor on this paper here, and also a coauthor on LIME, if anybody has any questions.  []AfgCric 1 point2 points3 points 1 day ago(0 children) Hah! I gotta say, that's just awesome. It takes the edge away from attack to here are the limitations of this model.  []arXiv_abstract_bot 1 point2 points3 points 5 days ago(0 children) Title:How can we fool LIME and SHAP? Adversarial Attacks on Post hoc Explanation Methods  Authors:Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, Himabindu Lakkaraju  Abstract: As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being leveraged by domain experts to diagnose systematic errors and underlying biases of black boxes. In this paper, we demonstrate that post hoc explanations techniques that rely on input perturbations, such as LIME and SHAP, are not reliable. Specifically, we propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Our approach can be used to scaffold any biased classifier in such a way that its predictions on the input data distribution still remain biased, but the post hoc explanations of the scaffolded classifier look innocuous. Using extensive evaluation with multiple real-world datasets (including COMPAS), we demonstrate how extremely biased (racist) classifiers crafted by our framework can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reflect the underlying biases.  PDF Link | Landing Page | Read as web page on arXiv Vanity  []Gisebert 1 point2 points3 points 4 days ago(0 children) I wrote some stuff about the disadvantages of LIME in a seminar called Limitations of Interpretable Machine Learning.The finished work of all students was collected in a single (and free) online book; check it out if you want (not entirely finished, yet):https:compstat-lmu.github.ioiml_methods_limitations_bookindex.html  []YoungStellarObject 0 points1 point2 points 3 days ago(0 children) There's another recent work on fooling explanation methods, focussing on gradient-based and propagation-based methods (and how to make them more robust): https:arxiv.orgabs1906.07983 Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 27763 on r2-app-0a4d50df89bf18e39 at 2019-11-13 09:10:37.954571+00:00 running 0cdf71e country code: US. "
  },
  "2":{
    "title":"[D] How does FaceApp work? : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 3,859 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[D] How does FaceApp work? (self.MachineLearning) submitted 2 years ago by clbam8 I came across this app on play store today called FaceApp. Does anyone know how it works? I don't remember seeing any paper about this recently. Post a comment!  []neoteat 21 points22 points23 points 2 years ago(0 children) in this article from February: https:techcrunch.com20170208faceapp-uses-neural-networks-for-photorealistic-selfie-tweaks they said FaceApp makes use of \u201cdeep generative convolutional neural networks.  []schmook 17 points18 points19 points 2 years ago*(2 children) There are plenty of papers that use tasks like taking features from one face and applying to another to demonstrate the quality of a generative model. Things like putting on or taking out glasses, changing gender, etc.  For example, if you have a good autoencoder for faces you can try to: Probably this would be a face of you wearing glasses, with varying degrees of success and quality. Kind of like the king - man + woman = queen in Word2vec? Many recent generative models can do something similar for pictures too. Something like the DiscoGAN or CycleGAN can do even better by learning how to map from the subset of faces with glasses to the subset of faces without glasses even if you don't have explicit glassesno-glasses pairs of otherwise similar photos. I am pretty sure that a CycleGAN could be trained to do exactly what this app do and probably would have results as good as the ones in the app. It's just a question of manually tagging the celeb faces dataset with agegenderetc labels and training the maps (and the results of the app look a lot like typical results using this dataset, so I suspect that's exactly what they did). But I suspect that you could even do it (probably with less quality) with a dcGAN based adversarial autoencoder, with the procedure I described above. I don't know if it would match the quality of the results from this app but it would be an initial version.   []alexmlamb 1 point2 points3 points 2 years ago(0 children) I guess these are all ways that FaceApp could work, but do you have any indications into how it does work?  It's not too surprising that it uses convnets, but I could see many specific technologies being in play here.   []andreasblixt 17 points18 points19 points 2 years ago*(6 children) Usually the solution is deceptively simple. In this case they may use OpenCV to get the Pose of the person to then apply masked textures on the shape of the face.  Here's a Google result I found on the topic http:www.learnopencv.comfacial-landmark-detection Edit: to clarify, I don't mean what they do is super simple but that state of the art deep learning probably didn't play any part in the app. Edit 2 See comment thread below, I was a bit quick to judge. There appears to be artifacts in FaceApp's output images akin to what you get from convolutional filters, and it's able to handle several cases that a simple cut & paste OpenCV application would not be able to do (especially wrt lighting conditions and hair shape). I think it's fair to say that they do use neural networks and some pretty good ones at that, since it works so well. As others have suggested, this could be based on one of the more recent GAN papers. I would wager the face still goes through an initial pass with OpenCV (or something similar) to normalize orientation and to crop away the background (and maybe also the eyes since they seem constant in all filters but the ->female one), considering how good it is at leaving a large part of the photo alone. Edit 3 Alright I managed to produce a few failure cases which gives away some information. You can see that they pick a square around the face and process only that part and then putting it back into the original photo. Secondly, in the last picture you can see that in some cases they do apply hand crafted textures onto the image, which becomes obvious when they fail to correctly get the orientation of the head. http:imgur.comaitQp1  []iwantedthisusername -4 points-3 points-2 points 2 years ago*(4 children) This answer is 100% 53% wrong.   []andreasblixt 5 points6 points7 points 2 years ago(3 children) Even if I'm wrong, I think it's an exaggeration to say I'm 100% wrong. As I wrote in another comment: I think even the most recent GANs have trouble cleanly separating the features of a face that make up its identity from the features that make up its expression, as shows in [the @smilevector] Twitter feed (the person's identity appears to change with the smile vector). I think FaceApp's resolution, speed, and quality shows it is using more traditional and handcrafted methods to modify the face expression.  []iwantedthisusername 2 points3 points4 points 2 years ago(2 children) Yeah but the thing is when you use the app with weird lighting or paintings of faces the errors you see more resemble what you'd see in a GAN or a cVAE. I suppose 100% is an exaggeration. They probably have some photo editing elements in addition, but as far as I can tell they're being honest about their technology.  []andreasblixt 0 points1 point2 points 2 years ago(1 child) I've tried the app some more and in some cases it does produce the kind of artifacts you'd get from convolutional filters, so that would indicate they are indeed using neural networks to some extent. It'd be very impressive if it's all the result of a neural network though, maybe they are extracting the face texture using OpenCV, doing style transfer on it and then putting it back in the picture?  []iwantedthisusername 2 points3 points4 points 2 years ago(0 children) Yeah that's what I'm figuring, I'm surprised by the resolution though, and they're obviously using multiple techniques. I think your theory is pretty reasonable. I think the smiles are definitely a combined technique, we're probably both right lol. Regarding the 100% I probably should have applied a softmax instead of a one-hot to my opinion ;)  []gecko39 2 points3 points4 points 2 years ago(2 children) https:twitter.comsmilevector has been adding smiles to photos for a while. I believe he is using GANs  []andreasblixt 1 point2 points3 points 2 years ago(0 children) I think even the most recent GANs have trouble cleanly separating the features of a face that make up its identity from the features that make up its expression, as shows in this Twitter feed (the person's identity appears to change with the smile vector). I think FaceApp's resolution, speed, and quality shows it is using more traditional and handcrafted methods to modify the face expression.  []Progr4mmatic 0 points1 point2 points 2 years ago(0 children) That is so creepy.  []shaggorama 2 points3 points4 points 2 years ago(0 children) Almost certainly cycleGANdiscoGAN, maybe with an added attention mechanism. The discoGAN paper even had a gender-bending model as one of its examples. After reading that paper I actually immediately thought of building an app like this but just never got around to it.   []zergling103 2 points3 points4 points 2 years ago(0 children) Only deep learning could be capable of this sort of satanisn https:twitter.comZergfriendstatus857053793393012736?s=09Plus when it glitches you get characteristic convolutional neural network artifacts.They are probably using a technique like this https:t.colXb4eWUFw3 https:t.coDNOK2SCJC5  []bakmanthetitan329 7 points8 points9 points 2 years ago(5 children) If this is a bad enough question to be downvoted this badly, could someone please explain why? Not OP by the way, I was curious about this same app.  []schmook 13 points14 points15 points 2 years ago(1 child) It's not a bad question. My guess is that people downvoted because they felt that op was asking out of lack of familiarity with some recent and very hyped literature on generative models.  Which is ok if you ask me. It's absolutely ok not to know things.  []lucidrage 1 point2 points3 points 2 years ago(0 children) Exactly, I'm sure everyone who's gone through college was thought that there's no such thing as stupid questions.  []jordo45 1 point2 points3 points 2 years ago(2 children) 23 points (82% upvoted), top post on rmachinelearning Am I missing something?  []schmook 2 points3 points4 points 2 years ago(1 child) At first there were quite a few downvotes.   []jordo45 6 points7 points8 points 2 years ago(0 children) You'd think people on this sub would know that you need a larger amount of data.  []olBaa 4 points5 points6 points 2 years ago(0 children) I'd bet that they do not actually use the GANs there, just the plain and simple editing algorithms. It's done on mobine in reasonably high resolution, I don't think texture synthesys by Gatis (& extensions by Ulyanov (prisma) can be applied to do that.  []pmigdal 1 point2 points3 points 2 years ago(0 children) I don't know exactly how this particular app works. However, two months ago I came across this paper:  []StuntHacks 2 points3 points4 points 2 years ago(0 children) I am wondering this too. My guess would be that they use some kind of machine learning algorithm bundled with some photo editing algorithms that are already established. So they got really lucky that they got all this hype around the app because that way they had huge amounts of training data very fast.  []iwantedthisusername 0 points1 point2 points 2 years ago(0 children) Probably generative deep networks. Maybe a conditional variational auto encoder.  []senorstallone 0 points1 point2 points 2 years ago(0 children) I've been playing a lot with the app, and I'm almost sure it's some kind of Interpolation in Latent space. The results are better for females than to males which are usually the case for GAN's papers I guess.  []ItalianPizza91 0 points1 point2 points 2 years ago(0 children) Defenitely late to the party, but could they be using cGANs? the generator gets an input image to transform to either femaleoldyoung etc, and the discriminator has to learn to recognize them as such  []themoosemind 0 points1 point2 points 2 years ago*(0 children) My highlights of FaceApp examples news: Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 13312 on r2-app-064f883af2bde15cc at 2019-11-12 21:28:07.989308+00:00 running e0c00f8 country code: US. "
  },
  "3":{
    "title":"[P] Noisy Labels and Label Smoothing : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 3,903 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Project[P] Noisy Labels and Label Smoothing (github.com) submitted 2 years ago by longinglove Post a comment!  []longinglove[S] 3 points4 points5 points 2 years ago(0 children) In this short experiment, I wanted to see if label smoothing is effective a) when there's noise in all labels b) when there's noise in one label. I think I could learn a lesson from this, but I have no intention of generalizing it since this is based on simple, limited settings.  []ballgame75 3 points4 points5 points 2 years ago(1 child) what kind of context would you apply this too? like NLP, maybe topic detection? I'm just curious, but what would be an example with a labeled output that would be up for interpretation?   []NicolasGuacamole 6 points7 points8 points 2 years ago(0 children) Pretty much anything which is mined or crowdsourced. Many datasets in computer vision are obtained through mechanical turk and have enormous amounts of noise. One of Google's internal datasets is called JFT-300M and contains 300 million images. It's safe to say it has significant label noise. Another thing to consider is things like dense prediction of things such as semantic classes or boundaries for pixels over videos or images. By their very nature classes may be subjective, and different people may label with different acuity, add to this the class imbalance problem.  []kacifoy 3 points4 points5 points 2 years ago(0 children) My best guess that this 'label smoothing' thing isn't going to change the optimal classification boundary at all (in a maximum-likelihood sense) if the smoothing is symmetrical wrt. the labels, and even the non-symmetric case can be addressed in a rather more straightforward way, simply by adjusting the weight of more uncertain points in the dataset. Other considerations ultimately boil down to a sort of regularization, but that's going to depend on plenty of other factors anyway, not just noise in the predicted variable.   []lugiavn 1 point2 points3 points 2 years ago(4 children) Isn't label smoothing for regularization, I don't see why it's supposed to help with noisy labels. And it seems there's nothing insightful came out of the experiments either?  []NicolasGuacamole 0 points1 point2 points 2 years ago(3 children) Presumably because we can interpret the labels as being probabilities in a very frequentist sense. When we minimise the loss we are literally considering our labels as probabilities with all the mass at the observed class, whether or not there is noise. Surely if we know there is noise then a smoothed set of labels is a more honest distribution to represent our data as.  []lugiavn 1 point2 points3 points 2 years ago(2 children) Not sure if that's true, Lets say labels with no noise is distribution 1 From distribution 1, add label noise, we get distribution 2 From distribution 2, add label smoothing, we get distribution 3 Are you saying it's better to fit distribution 3 because it's closer to 1 (than 2 is to 1)?  []NicolasGuacamole 0 points1 point2 points 2 years ago(1 child) Pretty much.  []lugiavn -1 points0 points1 point 2 years ago*(0 children) Then that is wrong I think. Consider simple example: classify gender from height 2 classes are balanced. Lets say for male (label y=0), input height x follows gaussian distribution N1 (or whatever kind of distribution you like) for class female (label y=1), input x follow distribution N2. With noise, say 10%, 2 classes are still balanced Now class 0, x follows mixture of Gaussians N1 N2 with weight 0.9 0.1 Similarly class 1, x follows mixture of Gaussians N1 N2 with weight 0.1 0.9 Now add label smoothing, say 10% Now for class 0, x follows mixtures of N1, N2 with weight 0.82 and 0.18 For class 1, x follows mixture of N1, N2 with weight 0.18 and 0.82 So how is this closer to the original distribution  []NicolasGuacamole 0 points1 point2 points 2 years ago(0 children) Nice start but it probably needs more work. I think that your conclusion could prove interesting if we were able to mine an estimate of our label noise during collection and provide it as part of a dataset.  []datatatatata 0 points1 point2 points 2 years ago(0 children) Another approach would be to tune the loss function so that mis-labeling x% of the measured set is considered 0 error. Basically you ignore the first m errors.  I wonder how this would lead to different solutions results.  []keidouleyoucee 0 points1 point2 points 2 years ago(0 children) https:arxiv.orgabs1706.02361 Somehow related, I measured how much negative labels are noisy in music tagging dataset.  Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 12070 on r2-app-0a8ab4ff2a2626708 at 2019-11-13 01:40:20.639073+00:00 running 0cdf71e country code: US. "
  },
  "4":{
    "title":"[D] Gary Marcus Tweet on OpenAI still has not changed misleading blog post about \"solving the Rubik's cube\" : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,385 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[D] Gary Marcus Tweet on OpenAI still has not changed misleading blog post about solving the Rubik's cube (self.MachineLearning) submitted 25 days ago by chansung18 Post a comment!  []Veedrac 80 points81 points82 points 25 days ago*(29 children) Gary's summary is much more misleading than the blog post. Concerns 1-4: \u201cNeural networks didn't do the solving; a 17-year old symbolic AI algorithm did FTA: \u201cWe train neural networks to solve the Rubik's Cube in simulation using reinforcement learning and Kociemba's algorithm for picking the solution steps. (NB: I would prefer this to be stated more prominently in less technical terms.) Concern 5: \u201cOnly ONE object was manipulated, and there was no test of generalizability to other objects FTA: Five different prototypes were used, a locked cube, a face cube, a full cube, a giiker cube, and a 'regular' Rubik's cube. The article never claims to do anything other than solve Rubik's cubes. Concern 6: \u201cThat object was heavily instrumented (eg with bluetooth sensors). The hand was instrumented with LEDs, as well. FTA: The five different prototypes had different levels of instrumentation. The 'regular' Rubik's cube had none, except small corners cut out of the centre squares to remove symmetry. FTA: Videos of the LEDs. They're blinking and red, FFS. Concern 7: \u201cSuccess rate was only 20%; hand frequently dropped cube FTA: \u201cOur robot still hasn't perfected its technique though, as it solves the Rubik's Cube 60% of the time (and only 20% of the time for a maximally difficult scramble). FTA: A whole section dedicated to this issue.  []briggers 36 points37 points38 points 25 days ago(2 children) This is what his book, Rebooting AI, is like. Many misrepresentations, and a general argumentation style of it isn't perfect, therefore it isn't good. There is definitely a case for being much more cautious about MLDL than many over-hyped journalists are, but this guy is just looking to fill a contrarian niche.  []mimomusic 20 points21 points22 points 24 days ago(1 child) I think Marcus is a well-spoken and intelligent man. I also think he is exceedingly pedantic to such a degree that he detracts from the actual problem at hand while most laymen are well capable to read between the lines and see research or blogposts for what they are. Sure, precision in academia is not something you can just forego without regard for making yourself understood, but the people who actually care about that stuff are very likely to dive into the nitty-gritty anyway. Those who succumb to hype would misunderstand and fall for nonsense headlines even if they were 100% unambiguous and perfectly constructed - but who cares about what The Sun is trying to convey? Almost all of us were well-aware of all the caveats parent poster mentioned, at least most of them. They are almost irrelevant in this context, even something as arguably crucial as generalizability (what a goddamn word!) takes a back seat to the main issue of robotic dexterity.  I understand that different people approach subjects with different degrees of rigor, and I can feel Marcus' concerns, but I also think they are very much nitpicky and not at all important to the discussion as far as OpenAI's due diligence and openness to critique is concerned. He sure is someone who will always be at odds with the community at large, but whether it's time well-spent is something I view with a healthy portion of skepticism; I think way too much effort goes into scrutinizing things that, as far as we can tell at this stage, barely matter in the long run.  []sanxiyn 17 points18 points19 points 25 days ago(20 children) I was actually disappointed by corner cutout for regular Rubik's cube. I consider that a significant instrumentation, and I think it is entirely justified to say that OpenAI did not solve vision part of regular Rubik's cube.  []whatevenarecomputers 2 points3 points4 points 24 days ago(19 children) They also didn't claim to solve the vision part of Rubik's cube solving (and explicitly said so), so it can't really be used as a way to detract from the OpenAI article. It was an article about the physical manipulation.  []nrmncer 4 points5 points6 points 24 days ago(10 children) tbh though what exactly has been achieved here then? Robots built to solve Rubik's cubes have been around for a while, most do it faster than that hand. The accuracy is low, it doesn't generalise well, there's a lot of hacks involved.. I guess the fact that it can fend off the giraffe is a novelty. But without any generalisation and given the low accuracy there's not much news here.  []whatevenarecomputers 5 points6 points7 points 24 days ago(1 child) There is a ton of generalization and resilience to new input here - putting scrunches around the fingers, putting it in a glove. Poking it as it solves. The breakthrough is the motion planning and object manipulation is done entirely through RL.  []sheeplearning 2 points3 points4 points 23 days ago(0 children) Yes but it is unclear if any of that is useful given it is not sufficient to solve the cube. Perhaps new approaches and additional experiments are needed and celebrating mediocrity just makes it harder for anyone to actually solve it. Robot does not really solve, warcraft does not really see, GPT2 is too harmful to the world and BERT outperforms and releases the model without any fuss -- everyone knows what is really going on here.  []tshadley 3 points4 points5 points 24 days ago(7 children) But why has no one built a Rubik's cube robot implementing a one-handed solve? It's relatively easy to build a 5 fingered hand mock-up with servomotors. It has to be because the controller algorithm to hold the cube, turn it, and solve it with 5 fingers is vastly complicated-- way beyond any traditional approach. The Deep-Neural-Network dexterity algorithm is the amazing accomplishment here.  []kit_hod_jao 2 points3 points4 points 24 days ago(0 children) I think this is part of the issue with the demo - it's hard to get a sense of how hard it is. I can certainly imagine it's very difficult to achieve, but one of the benefits of existing benchmarks is our expectations are finely calibrated to detect unusually good performance. Still, new benchmarks have to come from somewhere and IMO this is very impressive. It's just hard to appreciate how challenging it is.  []AnvaMiba 1 point2 points3 points 23 days ago(3 children) It's relatively easy to build a 5 fingered hand mock-up with servomotors. It's not. Five finger robotic hands that can move accurately are crazy expensive. They are only used for research as prosthetics, not as standalone robots, which is why you don't see many people using them to do stunts like this. Manipulation of objects of known shape and mass with stationary robots is a technologically mature task, there are plenty of industrial robots working on assembly lines which can manipulate objects much faster and more reliably than this. They don't use any fancy RL, just good old control theory and motion planning. So what's the innovation there? That they used RL?  []tshadley 0 points1 point2 points 23 days ago(1 child) Manipulation of objects of known shape and mass with stationary robots is a technologically mature task, there are plenty of industrial robots working on assembly lines which can manipulate objects much faster and more reliably than this. They don't use any fancy RL, just good old control theory and motion planning. Any specific example of comparable complexity? From what I've seen, industrial robots motion environments are tightly constrained and limited.  []AnvaMiba 0 points1 point2 points 23 days ago(0 children) From what I've seen, industrial robots motion environments are tightly constrained and limited. Yes, for safety reasons. The OpenAI robot hand isn't strong or fast enough to cause injury, which is why they can fiddle with it while it's moving. There are also robots designed for safe interaction with humans, or robust enough to resist external perturbations  []tshadley 0 points1 point2 points 23 days ago(0 children) It's relatively easy to build a 5 fingered hand mock-up with servomotors. It's not. Five finger robotic hands that can move accurately are crazy expensive. They are only used for research as prosthetics, not as standalone robots, which is why you don't see many people using them to do stunts like this. To more accurately make my point, I'll say it's relatively easy to obtain a 5-fingered hand mockup. ShadowRobot seems to have built the first dexterous hand in 2005. The hard part is controlling it.  []nrmncer 0 points1 point2 points 24 days ago(1 child) But why has no one built a Rubik's cube robot implementing a one-handed solve? mostly because if you're going to build a specialised machine it makes more sense to build.. well a regular machine. If all it can do is solve the cube then there's no need to make it to resemble a hand. It's a nice video to look at but they already had dextrous movements down a year ago. This is essentially the same thing with a slightly more modular task.  []tshadley 0 points1 point2 points 23 days ago*(0 children) mostly because if you're going to build a specialised machine it makes more sense to build.. well a regular machine. If all it can do is solve the cube then there's no need to make it to resemble a hand. But all regular machines for Rubik's solving have been built; clamps, rotating platforms, etc.. This was an obvious next step. It's a nice video to look at but they already had dextrous movements down a year ago Manipulating a solid cube with one hand is a vastly simpler than rotating individual planes of a Rubik's cube with one hand.  []sanxiyn 2 points3 points4 points 24 days ago(3 children) Isn't vision (or state estimation by vision) fundamental part of manipulation? I guess with Bluetooth instrumentation OpenAI showed manipulation would have worked if vision was working. But they couldn't get vision working.  []whatevenarecomputers 1 point2 points3 points 24 days ago(2 children) I think it's much more likely that they wanted to focus on a solution for the physical motion planning. I would not be surprised if there were independent projects to handle the object recognitionposition portion (which is effectively a separate problem).  []sanxiyn 0 points1 point2 points 24 days ago(1 child) OpenAI directly stated in the paper that they couldn't get vision working. See page 16. To quote: We experimented with a recurrent vision model but found it very difficult to train to the necessary performance level. Due to the project's time constraints, we could not investigate this approach further.  []whatevenarecomputers 0 points1 point2 points 24 days ago(0 children) https:openai.combloglearning-dexterity This appears to be their ongoing effort to implement the vision portion.  []ispeakdatruf -1 points0 points1 point 24 days ago(3 children) So what does solve in the title mean? For a human, the harder part is figuring out the steps involved. I can teach a 5yo how to rotate the cube in a minute. But teaching the kid to actually solve the cube will take much longer.  []whatevenarecomputers 2 points3 points4 points 24 days ago*(2 children) Yes, a rule of thumb for AI is the more intuitive for a human, the harder it is to make a computer do, and vice versa. We see this phenomena everywhere. Resilient motion planning and balance, object recognition, natural language processing, object location estimation. Humans do all this by the age of of 5. It's easy to make a computer to solve (symbolically) Sudoku or a Rubik's cube. Hard or time consuming for a human (without lots of training). Can we figure out the order of rotations to solve a cube is not what OpenAI is trying to solve. That's a well explored, solved problem. Conversely, human brains find spacial manipulation very intuitive past the age of like 3. But it's hard to build a system, even with fine-grained closed-loop feedback, that can manipulate physical objects. We have to design all sorts of loops and heuristics to estimate the physics of the object, how to move it in space, how to not drop it, how it can be folded or rotated or whatever. Letting reinforcement learning figure out all these nitty-gritty details represents novel progress being made in this space.  They have a system which seems to have figured out how to compensate for all sorts of messy inconsistencies like actuators not responding exactly how the model would expect it to (they mention it being resiliant even when actuators entirely break), all the way up to how do we figure out the individual finger movements to grasp and rotate a cube face.  []ispeakdatruf 0 points1 point2 points 24 days ago(1 child) In my limited experience with robotics, I totally concur with you. It would have be somewhat OK to title an academic paper with that title. People in the area would understand. But that's not what OpenAI did. They put out a blog post with that title, which is clearly intended for the general lay audience. The average person, who knows nothing about how hard actuator control, sensors, etc., are, will naturally assume that the harder, cognitive problem is being solved.  []whatevenarecomputers 0 points1 point2 points 24 days ago(0 children) What's the harder cognitive problem to solve here? If it's symbolically solving a cube, that's just not an interesting problem to solve, and we don't need ML to do it. It's not a game of Go. The interesting cognitive problem is the motion planning. I just disagree that the blog post is misrepresenting what OpenAI has accomplished. Its entire focus is on the hand, its motion, and the feedback systems involved. Even the title is Solving Rubik's Cube with a Robot Hand. Sure, somebody could read only the title and come to the wrong conclusion. But that's not really OpenAI's problem.  []yuvalpinlp 1 point2 points3 points 23 days ago(3 children) So, about 1-4, in what sense does the RL net solve the cube? train to solve... picking the solution steps you don't find this phrasing very misleading?  []Veedrac 0 points1 point2 points 23 days ago(2 children) They say they \u201csolve the Rubik's Cube with a human-like robot hand. This is true. I agree that the phrasing of \u201cand Kociemba's algorithm for picking the solution steps is too technical to be properly transparent to the average reader, even many readers with ML background, and I agree it is not nearly prominent enough\u2014I said as much in my post. If Gary's tweet was about that only\u2014as in, it did not make his other claims, and it was phrased so it was obvious the issue is clarity rather than honesty\u2014I'd have supported his commentary unreservedly.  []yuvalpinlp 0 points1 point2 points 22 days ago(1 child) I'm sorry, I find it impossible to interpret solve as anything other than figure out what to do at each step, which is the one thing their RL system *didn't* do. As Gary noted, there are other, much more accurate verbs to use, my vote goes to manipulate.  []Veedrac 0 points1 point2 points 22 days ago(0 children) I disagree that bringing a cube to the solved position cannot be described as solving it, but your disaffection is understandable and this wasn't one of my points of disagreement with the original post. I agree that 'manipulate' would be a much clearer term.  []SkiddyX 55 points56 points57 points 25 days ago*(6 children) This subreddit's unquenchable thirst for drama continues... \ud83d\ude02  []booleyan 30 points31 points32 points 25 days ago(2 children) Siraj started a fire we can't put out.  []egrefen 10 points11 points12 points 24 days ago(0 children) It's always been burning \ud83c\udfb6Since GPUs been churning \ud83c\udfb5  []ispeakdatruf 2 points3 points4 points 24 days ago(0 children) You_again would beg to differ... (with due apologies to Prof Schmidhuber, who has been shafted by the rest of the community)  []LaVieEstBizarre 12 points13 points14 points 25 days ago(1 child) Experts hate hype machines  []dod_worker 12 points13 points14 points 24 days ago(0 children) And hypers hate expert machines!  []MonstarGaming 3 points4 points5 points 23 days ago(0 children) For real. We ban beginner tutorials only to fill the gap with community drama. WTF? I wish we could tag and filter this content at the very least.  []balls4xx 12 points13 points14 points 25 days ago(0 children) A lego bot can solve (rotate until each side has a single color) a Rubik's cube, even I can solve one after inputting the tile pattern into some website. I think what they 'solved' here was making a robotic hand do it while being accosted by a stuffed giraffe.  []tradediscount 36 points37 points38 points 25 days ago*(17 children) I think Marcus is being a little disingenuous here. The key achievement of the OpenAI research he refers to is using reinforcement learning for really hard real world manipulation of physical objects using a robot hand. The Rubik's cube is used as a prop to represent a hard real world problem (hard as in difficult to manipulate effectively). OpenAI's blog post explicitly (but perhaps not prominently enough for Marcus or seemingly many subeditors who missed it in their reporting) states they use Kociemba's algorithm to determine the next move. This non AI shortcut was presumably used to reduce the number of steps, given the already high difficulty physical manipulation task they'd set themselves. Granted, many newspapers reported is as if the ML part had also worked out how to solve the cube, <and OpenAI have not tried to correct the misreporting>, but I'm not sure that's feasible or even necessary. Edit: bit between angle brackets not true, see uthegdb comment below. In addition, the cube has been solved using deep nets by several other teams (a quick Google shows published in reputed journals too) so while not trivial I have no doubts OpenAI could also solve it if they chose to. Finally, Marcus seems to like creating controversy to publicise his view that a lot of the DL community misrepresent the promise and capabilities of DL, which in my view simply isn't true. Hinton, Bengio, Le Cun, Chollet et al have all in my view been very open, measured and fair about the technology.  []thegdbOpenAI 42 points43 points44 points 25 days ago(9 children) Granted, many newspapers reported is as if the ML part had also worked out how to solve the cube, and OpenAI have not tried to correct the misreporting, but I'm not sure that's feasible or even necessary. We ping journalists to ask them to correct factual errors in reporting when we see them (though they may not always agree with our corrections). For example, the Washington Post article (https:www.washingtonpost.comtechnology20191018this-robotic-hand-learned-solve-rubiks-cube-its-own-just-like-human) feels misleading, so we've emailed them and linked them to the relevant sections in our blog post (namely, that we use Kociemba's algorithm as you mention). If you see other articles that need correcting, always feel free to let me know \u2014 gdb@openai.com!  []tradediscount 7 points8 points9 points 25 days ago(0 children) I stand corrected. Nice one!  []openaievolution 1 point2 points3 points 24 days ago*(1 child) Tbf this is an artifact of your science by press release strategy as well. If you release a public preprint first, journalists will have an easier time sourcing opinions from other well informed folks in the field, and presumably the reporting would get better.Zach Lipton elaborates more on this point in this thread here: https:twitter.comzacharyliptonstatus1184237037622136832  PS: To be clear, I am not arguing for not doing press releases, but rather putting out a preprint first and allowing some time bw the preprint and the press release.  []whatevenarecomputers 2 points3 points4 points 24 days ago(0 children) When has a reporter ever erred on the side of measured response from experts in the field instead of writing up the most sensationalized version they could concoct and still maintain plausible deniability?  []garymarcus -1 points0 points1 point 25 days ago(4 children) why not post some sort of clarification on your own site? it is clear that your blog was prone to misinterpretation.  []Ghenlezo 30 points31 points32 points 25 days ago*(3 children) I am a little baffled by your reaction to this. You worked with robots before, yes? The fact that they actually got a hand to sort of work on an intricate task, and to the point that it is able to adapt to many perturbations, is astonishing.  Did they make any deep algorithmic breakthroughs? No, but nor did they claim to. They scale things up and see how far they can push them. This is super useful work. AI is an engineering discipline not a science, and unfortunately is half-ashamed of this fact.  Was anyone with any literacy fooled into thinking the cube-solver was learned? It says right in the post it was done symbolically. Even if the solver was learned that is, frankly, much less impressive than demonstrating modestly robust physical dexterity. Your critique of the major deep learning figures is of naked emperors. But I see scholars constantly apologizing for their state of undress, baffled at how well they are weathering ever-colder temperatures.  One begins to wonder if invisible wool does, in fact, exist.  []garymarcus 1 point2 points3 points 25 days ago(1 child) Curious for your take compared to the much less hyped Baoding balls the week before. Here's what I said in the tweet that you apparently didn't read: I will say again that the work itself is impressive, but mischaracterized, and that a better title would have been manipulating a Rubik's cube using reinforcement learning or progress in manipulation with dextrous robotic hands or similar lines.  []Ghenlezo 31 points32 points33 points 25 days ago*(0 children) I read your \u201cdetailed analysis meme. I have not read every tweet you wrote on the matter. I am glad you admit that the progress on the actual task they were attempting was good. Your image macro makes it look as if you are dismissing impressive work for trivial misunderstandings. Anyone with any experience in this stuff knows, watching the video not even reading the paper, that what is impressive is the dexterity, and that solving a cube is trivial symbolically. Imagine we're watching a ski race on television and someone said, \u201cYou know they may be able to ski down fine but what they don't show you is they don't ski up! They use a chairlift! That's the impression your image macro gives me. They say right in the post that it was done symbolically. Failure to spoon feed journalists is not a sin in my book.  []tristes_tigres 0 points1 point2 points 24 days ago(0 children) AI is an engineering discipline not a science Engineering consists in applying known scientific principles to solve the real-world problems. AI at this point is barely more than alchemy - a compendium of techniques that seem to work from time to time for unknown reasons, that is very useful for extracting funding from wealthy patrons hoping to expand their riches.  []master_yoda_1 -2 points-1 points0 points 24 days ago(0 children) The same nonsensical hype you guys created with gpt-2 by not releasing it, and later on you proved yourself wrong and released it. What is your comment about that?  []m_nemo_syne 2 points3 points4 points 24 days ago(3 children) (I have to be pedantic for a moment: you call Kociemba's algorithm a non AI shortcut, but it is AI, just not machine learning. This is an instance of the AI effect: https:en.wikipedia.orgwikiAI_effect)  []tradediscount 1 point2 points3 points 23 days ago*(1 child) I strongly disagree. You are being precise, not pedantic, and I appreciate it thoroughly. edit: would non ml be better?  []m_nemo_syne 1 point2 points3 points 23 days ago(0 children) Thanks! I think so :)  []WikiTextBot 0 points1 point2 points 24 days ago(0 children) AI effect The AI effect occurs when onlookers discount the behavior of an artificial intelligence program by arguing that it is not real intelligence.Author Pamela McCorduck writes: It's part of the history of the field of artificial intelligence that every time somebody figured out how to make a computer do something\u2014play good checkers, solve simple but relatively informal problems\u2014there was a chorus of critics to say, 'that's not thinking'. AIS researcher Rodney Brooks complains: Every time we figure out a piece of it, it stops being magical; we say, 'Oh, that's just a computation.' [ PM | Exclude me | Exclude from subreddit | FAQ Information | Source ]Downvote to remove | v0.28  []garymarcus -3 points-2 points-1 points 25 days ago(1 child) i am not aware of any system that has solved the cube with pure RL; the ones i have seen are hybrids that also include monte carlo tree search. correct me if i am wrong...  []nikitau 1 point2 points3 points 21 days ago*(0 children) DeepCubeA for example does it with A* and trains the heuristic using function approximation and value iteration. While I haven't seen a system that reliably solves the cube using just RL without search, I feel like the same case could be made about Go too. Do you feel that lookahead is somewhat cheating in this case? Just curios, because at the end of the day, what gives such systems the edge over pure search ones is the fact that they learned how to guide the search. update: I actually read your write-up about innateness in AlphaZero in the meantime and understand a bit more where your statement is coming from. Its a good read and gave me a bit more insight into your point of view and I will say that I agree that in the philosophical context of AGI, using search definitely equates to injecting priors and is far from a tabula rasa approach. I get why you consider why their success may be uninteresting wrt. AGI; however within the domain of fully observable, deterministic environments, search methods could be seen as a common and rather simple denominator of all solvers, which one would be foolish not to use. If we are referring only to this small subset of all environments (which does model a lot of useful processes still) wouldn't AlphaZero (say apart from its use of CNNs) be one of the least biased solvers? I am curios on your thoughts about this.  []garymarcus -2 points-1 points0 points 25 days ago(0 children) Hinton in particular sometimes over promised quite a bit; i will likely write about that soon.  []JonathanFly 12 points13 points14 points 25 days ago*(7 children) I don't have the expertise to comment on the physical simulation part of this, so there may be some valid critique on that end, but I don't understand the primary criticism in this particular post. Isn't it obvious that solving the Rubik's cube is just a proxy for any dexterity challenge? Learning how to solve a Rubik's cube is trivial, it's inconsequential. For example, if OpenAI's project was 'robot that plays Tic Tac Toe in adverse conditions' and then in the video we see the Tic Toe paper oriented in different directions or moving around the table, in a room with dark or very bright light, on a table that's vibrating, with leaves blowing all around obstructing vision, using random types of pens and pencils that the robot arm has to adapt to on the fly -- this would basically be the same paper. Would you apply your same top-line criticisms to that project? Would you say the 'neural network isn't actually playing Tic Tac Toe, it's the 2000 year old Tic Tac Toe algorithm?' Maybe the problem is Rubik's cube have a mystique around them when I thought it was pretty clear that figuring out what to rotate is a trivial problem that any robot or human can already solve.  []ispeakdatruf 2 points3 points4 points 24 days ago(3 children) Learning how to solve a Rubik's cube is trivial, it's inconsequential. So, you have an RL algorithm that figured out how to solve Rubik's Cube?  []sanxiyn 1 point2 points3 points 23 days ago(1 child) You may like Solving the Rubik's Cube Without Human Knowledge.  []ispeakdatruf 0 points1 point2 points 23 days ago(0 children) Thank you, I was unaware of that work. Upvoted!  []garymarcus 0 points1 point2 points 25 days ago(1 child) many many people misunderstood the article given how it was framed; the washington post coverage is a case in a point.  []Ghenlezo 18 points19 points20 points 25 days ago*(0 children) Journalists have terrible incentives, as a rule no technical training, and little time. \u201cJournalists misunderstand you in the manner most likely to grab clicks is a universal argument against anyone who has interacted with a journalist.  []subsampled 5 points6 points7 points 25 days ago*(0 children) I think Marcus regularly raises interesting objections and ideas in the nature vs nurture (and symbolic vs connectionist) debate. Here, however, he may have missed the main point of this work, which emerges pretty clearly from the series of works by the same group. The main progress is clearly is in the context of in-hand manipulation via RL, whose complexity is very well-known to roboticists. Controlling a complex tendon-driven hand like the Shadow Hand to reconfigure an object with several degrees of freedom in presence of multiple contacts and disturbances has been a moonshot in robotics since forever. It's also true that OpenAI may have done better with choosing the title, but the work seems still a significant breakthrough, for sure in its robotics and transfer parts. And yes, in my experience 60% performance for the average case is definitely a good result for robotics demos standards of similar complexity.  []garymarcus 8 points9 points10 points 25 days ago(1 child) what's most notable about many of the comments here is that it is largely just ad hominem attacks; nobody can really argue that the screenshot on the left half of the slide of analysis (ie the opening of openAI's blog) actually matches what the paper did, and few people here are willing to acknowledge how widely the result was misinterpreted.  PR that invites serious misinterpretation is the definition hype; in the long run ML will suffer from a pattern of overpromising, just as earlier approaches (eg expert systems) have.  []adventuringraw 21 points22 points23 points 24 days ago(0 children) man, I came here ready to jump on OpenAI for being overly hyped, but their coverage itself really did seem measured, in spite of the press apparently misunderstanding it. Reading through the comments, I see mostly praise for you, combined with everyone roughly saying 'but in this case, it seems like Marcus jumped the gun, here's why'. You taking such a measured community reaction here as being nothing but 'ad hominem attacks' really makes me question what thread you were reading, because it apparently wasn't this one. If you're going to dig around to make sure claims are perfectly represented with no room for misinterpretation (a worthwhile activity given the current AI hype, don't get me wrong) you really don't get to so badly misrepresent your own treatment on a little subreddit like this. Literally anyone can read the other 30 comments on here. Does anyone else see 'ad hominem attacks'? Because I sure don't. Aside from a passing comment about 'filling a contrarian niche' it seems to be more about OpenAI's coverage, your specific critiques, and what people think about the issue. I saw your post, I read the blog post, it might have been easier than it should have been for a lay audience to misinterpret, but I really don't buy that it was on purpose. I don't even buy that it needs to be changed now that the mainstream reporters have come and gone, I honestly read OpenAI's coverage as intended, this was an impressive milestone in physical dexterity, that's it. As another comment pointed out, doing the whole solution (solving and all) in one learned architecture probably wouldn't have been radically harder than what was achieved even, assuming the other comment was correct, and there are other papers doing the actual Rubik's Cube solution finding. The reasons given in other comments for not buying your reasoning matches my own. My (honestly mostly unformed, I don't know your work well) opinion of you as a person doesn't factor into my not accepting your analysis in this case. Aside from one apparently actually rude ad hominem attack (that was called out by someone else, the original user deleted their post now) what's left is a long ways away from being unfair to you. If you're going to misrepresent your own treatment in such an obvious way, I'm not impressed when the topic you're trying to push is another group misrepresenting their research. That said, even if you were maybe a little overzealous in this case, and even if you're taking it a little personally that not everyone else here agrees with you, I wholeheartedly wish that mainstream reporting was more realistic, so Godspeed on your quest.  []garymarcus 4 points5 points6 points 25 days ago(13 children) you may have caught a minor error here but mostly you are comparing apples and oranges.  my main point was that the popular presentation (ie the blog) was misleading; finding stuff in the fine print in the technical paper doesn't fix that. and even so, note that the title of the article itself is misleading, as is the opening framing, as i detailed in a previous tweet. so the article itself has its own issues.  i am really most concerned though with your anemic defense of point 5: it doesn't matter whether openAI claimed to have looked at more than one object or not; the point is that if you don't have stronger tests of generalization, you can't make profound claims. 5 slightly different cubes doesn't mean you could not tighten a screw, open a lock, or button a shirt.  []Veedrac 15 points16 points17 points 25 days ago(7 children) You replied to the post rather than me. finding stuff in the fine print in the technical paper Everything I said was from the blog post, and not even a particularly close read of it. I don't expect the press to read dense technical papers, but I do expect them to read more than the title of the summarizing blog. 5 slightly different cubes doesn't mean you could not tighten a screw, open a lock, or button a shirt. OpenAI never claimed otherwise.  []garymarcus -2 points-1 points0 points 25 days ago(6 children) perhaps i should have said blog abstract (ie the part reproduced in my slide); the Washington Post story stands as a testament to how prone the piece was to being misread, it's not just the title, but the full framing in the abstract i reproduced. and how much emphasis there is in the article on learning relative to the small space devoted to the large innate contribution, etc .and even on your last point \u201cunprecedented dexterity at top suggests that they ought to be able to do this in general in someform; they haven't actually tested that (aside from variants on a cube). as someone apparently in Ml, you should recognize how unpersuasive that is. there is a long history of that sort of thing having seriously trouble generalizing.  []Veedrac 14 points15 points16 points 25 days ago(1 child) The quote is \u201cThis shows that reinforcement learning isn't just a tool for virtual tasks, but can solve physical-world problems requiring unprecedented dexterity. I find it very hard to understand where your objection is coming from; that sentence is plenty reasonable. At this point I think my comments stand on their own, so I'm going to bow out.  []garymarcus 4 points5 points6 points 25 days ago(0 children) Which problems? without a test of generalizability to other noncubic, noninstrumented objects, and without a comparison to the Baoding result from a week before, I think the sentence is overstated. what are the plural problems even? I see one problem, no test of transfer. By know we should know that this is a red flag. Which doesn't mean that I am unimpressed. In fact, I said the following, in a immediate reply to my own tweet that you must not have read: I will say again that the work itself is impressive, but mischaracterized, and that a better title would have been manipulating a Rubik's cube using reinforcement learning or progress in manipulation with dextrous robotic hands or similar lines.  []sanxiyn 1 point2 points3 points 25 days ago(3 children) We can all agree that move finding was innate, but why does that mean large innate contribution? It was a small part of the work, so innate contribution was small.  []garymarcus 2 points3 points4 points 25 days ago(2 children) I guess this depends on how you define solving. But: You take out the innate part, and it no longer solves the cube.  []sanxiyn 1 point2 points3 points 24 days ago(1 child) I am all for retitling the post to Manipulating Rubik's Cube as you suggested. After retitling, innate contribution was small.  []garymarcus -1 points0 points1 point 24 days ago(0 children) That title would certainly help a lot, and reduce the importance of innate component, though elsewhere there is still a fair amount of carefully engineered innate detail of different sorts in the precise structuring of the visual system etc. It's not like it was a big end-to-end network fed from a bunch of sources that worked everything out for itself.  []simpleconjugate 12 points13 points14 points 25 days ago*(4 children) As time has progressed, your criticisms come off as \u201cbad faith criticisms. In this case you disguise problems with science and tech journalism as problems with OpenAI's communication of achievement. GDB is right, they never made any large claims outside being able to manipulate the cubes. It would be great to have people out there who are keeping conversation around AI grounded, but that doesn't seem to be your primary interest or goal.  []garymarcus 8 points9 points10 points 25 days ago(1 child) the problem here was with openAi's communication; i have been clear about that, posting repeatedly on twitter that result was impressive though not as advertised. here is an example since you seem to have missed it: https:twitter.comgarymarcusstatus1185680538335469568?s=21 no person in the public would read the claim of \u201cunprecedented dexterity as being restricted to cubes.  []simpleconjugate 4 points5 points6 points 25 days ago*(0 children) A change in title should be made for sake of honesty (social media isn't known for its in depth readings). However unprecedented dexterity is certainly a reasonable description of the impressive result. I also don't think that the same \u201cperson in the public would read your tweets and think that OpenAI achieved anything important. In this sense, you mischaracterized OpenAI's own claims and achievements while reporting their own failures to communicate.  You are doing great work out there by pointing out the flaws in the hype. But at the same time, it feels that your criticisms serve Robust.AI more than the public. As someone who think ML needs to be become more rigorous in reporting results, I think recent posts highlight things that journalist irresponsibly reported on as well as mistakes made by OpenAI. Suffice to say, lately I feel the same about both you and OpenAI as you feel about OpenAI and the \u201cperson in the public.  [][deleted] 25 days ago(1 child) [deleted]  []simpleconjugate 1 point2 points3 points 25 days ago(0 children) That seems like an unnecessary personal attack. There is a clear line between criticizing his ideas and attacking him. You crossed it.  []Rasko__ 0 points1 point2 points 23 days ago(0 children) Jesus Christ who cares  [+]SugarAdamAli comment score below threshold-11 points-10 points-9 points 25 days ago(0 children) This is all way too over my head, but I'll take your word for it Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 2160 on r2-app-0b1b17546394c110c at 2019-11-14 11:57:03.484736+00:00 running 0eef139 country code: US. "
  },
  "5":{
    "title":"[D] why softmax+CE over sigmoid+BCE? : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,613 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[D] why softmax+CE over sigmoid+BCE? (self.MachineLearning) submitted 6 days ago by DeMorrr Most of the popular neural network language models use softmax+cross entropy loss during training, which is based on the assumption that only the target label is true, and everything else is false. But isn't language modeling a multilabel classification task? why sigmoid+BCE isn't used often? Post a comment!  []picardythird 10 points11 points12 points 6 days ago(2 children) The motivation is that the output of the model is actually attempting to approximate the conditional probability of a word, conditioned on the previously seen words (and their ordering) , over the vocabulary. This probability distribution must sum to 1.  []WannabeMachine 2 points3 points4 points 6 days ago*(1 child) To expand on this, formally, at each timestep, language models estimate P(w_i | w_1, ..., w_{i-1}) which is a multi-class problem, not multi-label.  []BatmantoshReturns 0 points1 point2 points 5 days ago*(0 children) Regarding the definition of multi-label; would a multi-label problem would have estimates of multiple words? For example, in the case of 2 correct labels in a batch:  P(wi, w_j | w_1, ..., w{i-1}) In this case, wouldn't either approach work? If using softmax+CE, you can set the probabilities equal to .5 for i and j. During training with negative sampling, you can set everything else to zero.  []SteveIsMyRapName 1 point2 points3 points 5 days ago*(1 child) Although there are many possible words, you still only select one word for each position. Let's say the training data has two examples 'there is a cat' and 'there is a dog'. During inference you may be given 'there is a'. 50 percent of the time the model should select dog and 50 percent of the time the model should select cat. That can be achieved by sampling according to the logits instead of taking the argmax. In contrast, an image can have both a cat and a dog at the same time. Anyway, the difference is somewhat arbitrary. Maybe I'm not thinking straight but for a multi-classification problem wouldn't a sigmoid network with class A and class B be very similar to a softmax network with three classes: class A, class B, and class A and B? Moreover I'm sure I've seen a proof which shows that there is a reparameterization such that a system trained with softmax is the same as sigmoid.  []BatmantoshReturns 0 points1 point2 points 5 days ago(0 children) sigmoid network with class A and class B be very similar to a softmax network with three classes: class A, class B, and class A and B? I'm trying to expand this to language modeling, where you could have multiple correct labels, and negative samples. Say that in our batch we have 2 correct labels (or classes?) Class A, Class B , and 3 incorrect classes, Class D, Class E, Class G. For Sigmoid+BCE, you can see the true classes to 1, and false ones to zero. And another approach could be for the softmax, and set the probabilities equal to .5 for each true class, and zero for the negative samples, then use regular cross entropy.  What would be the advantagesdisadvantages of these approaches?  []asobolev 0 points1 point2 points 5 days ago(3 children) based on the assumption that only the target label is true, and everything else is false No, it's not. You can have several target labels for the same input, and the optimal solution, in that case, should assign some probability to all of them. Instead, the actual assumption of softmax + CE is that the sentence is generated one word at a time, and not by tossing a coin for each word independently for whether we'd like to take it.  []BatmantoshReturns 0 points1 point2 points 5 days ago(2 children) I am wondering why sigmoid +BCE is used in word2vec then.  https:youtu.beTaZz_K2xJy8?t=523 Since the next word is being predicted, shouldn't a sampled softmax +CE be used?  []asobolev 0 points1 point2 points 5 days ago(1 child) The particular moment you've linked talks about random variables y that says whether the word t matches the context c, it does not predict the next word. The very same slide mentions p(t|c) in the top row, which does predict the target token t given the context c, and thus uses softmax.  []BatmantoshReturns 0 points1 point2 points 5 days ago*(0 children) I get it now, thanks! I am wondering which method to use for training question and answer similarity, where the questions and answers are represented by vectors, and a question could have more than one correct answer.  Say that in a batch, I have the following questions and answers, each represented by a vector  Questions: What's in the sky? What appears at night?  What's hot? Answers: Moon Sun Fire Correct Combinations: What's in the sky? Moon, Sun What appears at night? Moon What's hot? Fire, Sun.  In this batch, 'Moon' and 'Sun' are both answers to 'What's in the sky?', so the cos similarity of the question vector to the answer vectors should be trained equal to 1. But 'Fire' is not an answer to that question, so the cos similarity should be trained to equal to 0.  There are two approaches for this, one using sigmoid + BCE, another using softmax + CE.  Sigmoid + BCE: After taking the dot products of all combinations, pass them through a sigmoid, and doing binary cross entropy loss with the correct label , 0 or 1.  For example: partialLoss1 = - Log ( Sigmoid( DotProduct( Vector{What's in the sky}, Vector{Moon} ) ) ) partialLoss2 = - Log ( 1 - Sigmoid( DotProduct( Vector{What's in the sky}, Vector{Fire} ) ) ) partialLoss3 = - Log ( Sigmoid( DotProduct( Vector{What's in the sky}, Vector{Sun} ) ) ) and do this for all the combinations of questions and answers.  Softmax + CE: In this approach, a softmax is taken over each question and their answers combinations. For questions that have two correct answers, the softmax output should be .5 for each of correct answers.  If doing the softmax over all possible dot products of answers for 'What's in the sky?', the softmax probabilities should be .5 for 'Moon' , .5 for 'Sky', and 0 for 'Fire'.  And this the loss for the output labels and the softmax probabilities can be calculated with regular cross entropy.  For example: partialLoss1 = .5*Log( SoftmaxProbability{ Vector{What's in the sky}, Vector{Moon} } ) partialLoss2 = .5*Log( SoftmaxProbability{ Vector{What's in the sky}, Vector{Sun} } ) partialLoss3 = 0*Log( SoftmaxProbability{ Vector{What's in the sky}, Vector{Fire} } ) Are both these approaches correct? If so, what are the advantages and disadvantages to each one?  []BatmantoshReturns 0 points1 point2 points 5 days ago(4 children) I have been wondering about this myself over the last couple days.  Most of the popular neural network language models use softmax+cross entropy loss during training, which is based on the assumption that only the target label is true, and everything else is false Couldn't cross entropy can be used for multi classes ?https:stats.stackexchange.comquestions260505machine-learning-should-i-use-a-categorical-cross-entropy-or-binary-cross-entro So I think for the example of 2 correct classes, the labels should be .5. And for this approach, it might do better with a higher learning rate, since the gradients wouldn't be a large for the 0 or 1 labels for sigmoid+BCE. Where is softmax+CE used more? word2vec uses sigmod+BCE.  []cthorrez 0 points1 point2 points 3 days ago(3 children) I can't remember the exact model, might be bert or elmo or something but some paper softened the softmax by like putting 0.9 on the target word and split the 0.1 over the rest of the words to try to stop it from saturating the softmaxes.  []BatmantoshReturns 0 points1 point2 points 2 days ago(2 children) hmm , I wonder what was the goal of doing that? What does saturating the softmaxes mean?  []cthorrez 0 points1 point2 points 2 days ago(1 child) Saturating the softmax is when the weights make the score for one class essentially 1 and 0 for all the others. Intuitively this is what you want in normal classification settings but if I remember correctly it made gradient flow bad or something in this case.  Man I really wish I could remember where I saw that.  []BatmantoshReturns 0 points1 point2 points 2 days ago(0 children) I remember correctly it made gradient flow bad or something in this case. hmm interesting, if cross entropy loss comes afterwards, then you would have a cross entropy term for all the classes, not just the correct class. Sounds like it could get computationally very expensive though.  []techlos 0 points1 point2 points 4 days ago(0 children) i've done some experiments using sigmoid+BCE for image classification on cifar10 in the past - i found the best strategy was to use the true label for the correct class, and noisy smoothed labels for the false classes using uniform noise between 0 and 0.5. The results were never as good as softmax+CE loss in terms of overall accuracy, however it was surprisingly resilient to adversarial attacks compared to the usual softmax training.  []MrAcurite 0 points1 point2 points 2 days ago(0 children) Something that might help you re-kajigger your conception of what's going on is knowing about the beam search algorithm. Read up on it a bit, if you're interested. But the short version of the explanation is that you don't really care how good or bad each next word is on their own, but rather which words are the most likely out of all of them. So, because you're trying to get a probability distribution instead of a word-by-word rating, you need softmax. Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 23234 on r2-app-0c88321715d29f726 at 2019-11-14 20:59:09.626034+00:00 running 6a8641e country code: US. "
  },
  "6":{
    "title":"[D] Is Neural Magic a scam? : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,293 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[D] Is Neural Magic a scam? (self.MachineLearning) submitted 6 days ago by isthataprogenjii I recently learned about this new startup which advertises that they can provide GPU level learning using a CPU. There are already CPU versions of neural network training algorithms. Is neural magic doing false advertisement? What approach are they taking specifically to make the 'magic' happen? Post a comment!  []darkconfidantislife 85 points86 points87 points 6 days ago*(9 children) Scam is far too strong a word. Their backgrounds are in multithreaded computing techniques, so I'm guessing that they're using a huge stack of approximations like induced sparsity, low precision, hashing, compression, etc. and auto generating very high performance multithreaded code. That would also explain why they're targeting inference.  It's not a scam, but my suspicion is that if you're a DL researcher it's not gonna be super useful to you. EDIT:  These things generally tend to not run so well on GPUs due to architectural and software reasons. That's why sparse graph workloads are usually run on CPUs in HPC today.  I'll also add that sparsity is something that most competent machine learning accelerators will support. For example our processor supports sparsity even better than a CPU usually does, and competitors such as cerebras and graphcore also support sparsity acceleration.  []maxToTheJ 0 points1 point2 points 6 days ago(8 children) Couldn't you do those things on a GPU too?  []darkconfidantislife 10 points11 points12 points 6 days ago(7 children) Usually those particular things, i.e. sparsity and hashing, tend to be of lower efficiency on GPUs, due to architectural and software reasons.  [+]ithinkiwaspsycho comment score below threshold-8 points-7 points-6 points 5 days ago(6 children) Hashing is usually magnitudes faster on a GPU compared to a CPU, which is why GPUs are heavily used in password cracking and bitcoin mining, etc. Do you mind citing your sources that say hashing is faster on CPU?  []darkconfidantislife 3 points4 points5 points 5 days ago(0 children) Hash lookups  []JayWalkerC 0 points1 point2 points 5 days ago(4 children) Computer science is never so simple as X is faster on Y. There's more to consider that is not mention here.  []maxToTheJ 2 points3 points4 points 5 days ago(3 children) But isnt his answer in response to \u201cY is faster on X  []JayWalkerC 1 point2 points3 points 5 days ago(2 children) hashing is not specific. There are many different hash functions, some are faster on CPU and some are faster on GPU. That's the point.  []maxToTheJ 1 point2 points3 points 5 days ago(1 child) I get your point but my point was the disparity between the upvotes for CPU does hashing better vs GPU does hashing better. Assuming your logic they should be the same as both are wrong  []darkconfidantislife 0 points1 point2 points 5 days ago(0 children) I'm the one who's upvoted for CPU, and I agree, he just asked a simple question. People are just downvote happy I guess  []yusuf-bengio 34 points35 points36 points 6 days ago(1 child) I know some guys from Boston who work there.  In essence, they develop multi-threaded, SIMD and cache efficient kernels to train sparse deep learning models. Their focus is on sparse datamodels, where they claim to achieve GPU level performance.  []jminuse 15 points16 points17 points 6 days ago(0 children) GPUs are less efficient on sparse matrices, so they inherently don't provide as much speedup vs CPUs here. That's fair within this problem domain, and fair if the sparse models can equal the dense ones in accuracy. But it's not a good comparison if they're saying we can be as fast as GPUs (if we use substantially less accurate models).  []bigfish_in_smallpond 18 points19 points20 points 6 days ago(0 children) They aren't talking about learning, they are specifically talking about inference.  []jacobgorm 5 points6 points7 points 5 days ago(2 children) They seem to have a few magical tricks up their sleeves, but in general the performance differences between CPUs and GPUs for inference is smaller that most believe. Looking at darknet for instance (at least when I last tried it), it uses cublas GEMM() on the GPU, but a simple triple-nested loop on the CPU, resulting in a 100X speed difference, but replacing its naive GEMM() with an modern BLAS GEMM() implementation readily reduces the difference to between ~5X as I recall. My own tiny company, Vertigo.ai, has been shipping CPU-based neural nets for several years, though we don't make as bold claims as Neural Magic does about always running as fast as GPUs in all cases.  []darkconfidantislife 3 points4 points5 points 5 days ago(0 children) That sounds interesting, care to say more? As an aside, your website has an expired certificate and is fairly illegible on mobile.  []HipHomelessHomie 1 point2 points3 points 5 days ago(0 children) That is my experience as well. In fact Intel have a framework for optimized inference performance on their CPUs that works very well. The generations of CPUs with AVX instructions can leverage these to get good inference performance. In fact a company I worked with leveraged that to do edge inferencing on tiny not very powerful machines and it works great.  []axeonthra 2 points3 points4 points 6 days ago(0 children) Depends on the metric and GPUCPU they are talking about. In my limited tests, GPUs (recent) are only minimally better than CPUs (recent, with AVX2) on a per $ basis. They are definitely faster by a significant margin though.  []gnulynnux 1 point2 points3 points 6 days ago(2 children) I'm skeptical of GPU-level learning on a CPU. Can you provide more details?  []goldcakes 5 points6 points7 points 5 days ago(1 child) It's CPU-level inference, not CPU-level learning. I have worked on deployments where we found CPU-only inference to be better value (on a per dollar basis) when you look at the total BOM, so I would not be surprised more optimized code allows for solid inference using CPUs only.  []gnulynnux 0 points1 point2 points 5 days ago(0 children) Ahh, that makes more sense. Thank you!  []TrueLankinen 1 point2 points3 points 5 days ago(0 children) This is a little bit out of context but do anyone know is there some free website online where I could make videos like their. It's great because they just use words so it doesn't even need fancy product to make that kind of video.  []CireNeikual 0 points1 point2 points 6 days ago(0 children) It depends on whether they are just optimizing regular DL operations, or performing a different kind of learning entirely. With sparsity it is possible to train a reinforcement learning agent to play Atari Pong from pixels on a Raspberry Pi in real-time (our company has shown that, with source code available). If they are not exploiting sparsity then I have no idea whether it is legitimate or not.  []alexmlamb 0 points1 point2 points 5 days ago(0 children) It might be an aquihire ploy too? Like I'm not sure that Nervana ever had a clear plan to be financially sustainable, but it had a really good Intel acquisition.  I could see if a middle-tier company wants to have an AI lab, they might like to acquire a company like this which focuses on fast implementation and deployment, which they could then reorient towards their own application that they care about.  []bbateman2011 0 points1 point2 points 1 day ago(0 children) It appears to be real, and have at least some use cases. As others here have said, they are talking mainly about inference, and in their blogs etc. they specifically talk about taking advantage of sparse structures in inference.  They have a patent application: http:www.freepatentsonline.comy20190138902.html  []johnyt3 0 points1 point2 points 6 days ago(0 children) Upvoted for visibility because i'm curious  []dasdull -1 points0 points1 point 5 days ago(0 children) Nice try at marketing Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 30116 on r2-app-0b795575c8b440638 at 2019-11-14 21:52:57.049614+00:00 running 7ff3f3b country code: US. "
  },
  "7":{
    "title":"[P] Art Valuation Bot : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,637 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Project[P] Art Valuation Bot (self.MachineLearning) submitted 23 days ago by amourav I need to develop a small project for a data science bootcamp interview next week (insightdatascience.com).  One idea I had was to create an art valuation bot and art generation bot. The outline would look something like this: Some potential challenges I see: Does anyone have thoughts on project-based data science boot camps like Insight Data Science (https:www.insightdatascience.com ) If you see any more potential issues with my project outline, or have a suggestion for improving it, please leave a comment or PM :) Post a comment!  []sheeplearning 2 points3 points4 points 23 days ago(2 children) Just look at the meta-data (artist, year, movement) and predict the price. What is in the painting usually has very little to do with the price of an artwork but when you have a hammer..  []amourav[S] 0 points1 point2 points 23 days ago(1 child) Hmm. If I'm not relying on images, I might need a more robust dataset. I might go down this road if I find a good datasetsource. More datasets with that level of contextinformation is not free :(  []chief167 0 points1 point2 points 22 days ago(0 children) I wouldn't rely on images anyway. For example, there are a lot of painters who are able to nearly perfectly replicate common artworks such as Van Gogh's sunflowers and Mona Lisa. Those are also worth something, but definitely not as much as the real thing  []TheRedmanCometh 2 points3 points4 points 23 days ago(1 child) double value = ThreadLocalRandom.current().nextDouble(1000000);  done  []amourav[S] 0 points1 point2 points 22 days ago(0 children) ThreadLocalRandom.current().nextDouble(1000000); aiming for above chance :)  []Quantum_Network 1 point2 points3 points 23 days ago(3 children) Do you think you'll have enough data to train a GAN? And overall, data might be a challenge in this project. But you could train from web-scraped data (like from https:www.liveauctioneers.comcart1) and hopefully it should work out.  []amourav[S] 0 points1 point2 points 23 days ago(2 children) k you'll have enough data to train a GAN? And overall, data might be a challen Yeah. Training on web scraped data is the goal. How many samples should I aim for. I also see another potential issue, cropping out the frames :P  []cheruboftheabyss 2 points3 points4 points 23 days ago(1 child) You dont necessarily have to crop out the frames, the generator will just model them then too, and you will have to crop them after the generation process (which will most likely involve some cherry picking anyway, if you dont want to spend a lot on gpus). I would probably aim at about 5-40k images (depndes on the diversity of your images), but you can also augment your dataset with the wikiart dataset (25.4GB). The biggest problem with your idea will most likely be the price regression, since predicting the price of art pieces involves a lot of complicated 'human' background knowledge (emotional effectiveness, quality of artistic realization, price of materials, ...), which is not the case in most standard computervision tasks, where in some cases only the texture is important (there is some interesting research on that specific topic by the way).  []amourav[S] 1 point2 points3 points 23 days ago(0 children) Thanks!  I agree, much of the task is related to 'human' factors and context, but I'm hoping there is something else, maybe some X factor, or artistic style that seems to be more valued. That's why I'm only planning to train on abstract art pieces. I'm not aiming as much for good accuracy, more of a proof of concept (above random chance).  []horace999 1 point2 points3 points 22 days ago(1 child) That's way more than you need to do for pre-insight. Focus on making the end product good and usable rather than such complicated backend stuff  []amourav[S] 0 points1 point2 points 22 days ago(0 children) Thanks for the advice!  []shaggorama 1 point2 points3 points 16 days ago(1 child) If you already have the knowledge to build something like this, what do you hope to learn from the bootcamp?  []amourav[S] -1 points0 points1 point 14 days ago(0 children) It's a chance to make connections with companies that are hiring and network with other professionals. Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 11088 on r2-app-052d8ec9d906544ae at 2019-11-14 21:46:14.025768+00:00 running 6e1acb2 country code: US. "
  },
  "8":{
    "title":"[P] Fast Super Resolution GAN : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,845 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Project[P] Fast Super Resolution GAN (self.MachineLearning) submitted 18 days ago by abnormdist I've been super intrigued by image super resolution problems. Reading online, I found the SRGAN paper to be interesting, especially how the PSNR and SSIM metrics are unreliable when compared to human perception of quality. I wanted to create a faster version of the SRGAN, so I decided to use a MobileNet as the generator. This idea is somewhat inspired by Realtime Image Enhancement, Galteri et al. I want to use it to upsample low quality videos, for scenarios when you may not have access to high speed internet. You can leverage the GPU to do synthetic super resolution. I would appreciate any ideas towards increasing speedquality of this project. Here is the implementation in Tensorflow 2.0: Fast-SRGAN. Take a look, and feedback is really appreciated! Post a comment!  []ArkaonGP 6 points7 points8 points 17 days ago(7 children) Interesting work. Congrats. Did you consider to implement some of the tricks proposed by the ESRGAN paper?  []abnormdist[S] 3 points4 points5 points 17 days ago(6 children) I really want to. One thing that's holding me back is that I can't find a pretrained tfkeras VGG19 which allows pre-activation feature selection. All I found online use specify the activation parameter in the conv function. And to implement the ESRGAN tricks, I need the pre activation features. So I'll have to train a VGG19 on imagenet myself, but I don't have the resources to. Apart from that, I find the idea of Wasserstein GANs very interesting.  []AlexiaJM 1 point2 points3 points 17 days ago(1 child) ESRGAN actually uses Relativistic GAN which is trivial to implement. Penalizing the gradient (as in WGAN) is very helpful to prevent mode collapse, but I'm not sure that it would be helpful when you actually don't care about diversity (which is the case in super resolution). In super-resolution you don't want to sacrifice even a little bit of quality for more diversity.  []abnormdist[S] 2 points3 points4 points 17 days ago(0 children) Yep, I agree. I should've said relativistic GANs, just blurted WGAN because they are a subset of the RGANs.  [][deleted] 17 days ago(3 children) [deleted]  []abnormdist[S] 1 point2 points3 points 17 days ago(2 children) I can. But I don't think it works in this case. Typical VGG19 models I found on the internet use convolution like this: Forgive the poor pseudo code: x = deeplearningframework.Conv2d(out_filters, kernel_size, strides, ACTIVATION=RELU)(x)  I'm not really sure how I can extract the feature maps from this layer before the activation. Since I believe the ops are fused for faster computation? I'm not sure if the ops are not fused, I'll need to see, because if they aren't, then it should be easy.  [][deleted] 17 days ago(1 child) [deleted]  []abnormdist[S] 1 point2 points3 points 17 days ago(0 children) Du-OH! I don't know why I didn't think of that. Thanks for that!  []lendacerda 1 point2 points3 points 17 days ago(1 child) I didn't find many papers on it when i was researching on it as well. But for one, you have to use as high resolution images as you can and as many images to train the network. Basic stuff. What i did find tho is that you can do something similar to Non-Local Means Denoising: find similar areas on the screen and use one area's information on another area. Render the new pixels using it's neighbours as a guide. Might be a bad explenation but just google NonLocalMeansDenoising OpenCV Also you might need some temporal features, as the super-resolution thing might diverge too much from one frame to another (e.g one edge is red in one frame and than slightly darker on the next frame, hitching colors) Here are some links: https:www.youtube.comwatch?v=HvH0b9K_Iro (has source code link in the comments) https:www.youtube.comwatch?v=WovbLx8C0yA (uses NLMD approach)  []abnormdist[S] 2 points3 points4 points 17 days ago(0 children) I like how you're thinking in the frame of reference of a video. I like your ideas! They will come in handy. Currently I simply trained the GAN on div2k (high quality image dataset). But it's not too big a dataset (only 800 images). I'll go through the videos, sounds interesting for videos.  []alvisanovari 1 point2 points3 points 17 days ago(11 children) Nice! Are there instructions to run inference on the pre-trained model? Maybe a Colab you can share?  []abnormdist[S] 4 points5 points6 points 17 days ago(9 children) I didn't provide a sample code for inference. Totally my bad. I'll add it soon!  []BobFloss 0 points1 point2 points 17 days ago(8 children) Please do. Every time I see a repo where they only show how to train the model, I feel like they're just full of shit. I'm not saying you are, that's just what my gut says, because clearly you have to have the code to actually do the upscaling or you wouldn't have had results to share. Nobody wants to have to duplicate the effort of writing the code just to try a model out. It also stunts future efforts because it makes it harder to compare your model to others. Your model might even be really great, but if nobody can try it without going out of the way to see if it even works, it could be left in the dust by something that's not even any good, simply because people can actually use it easily.  []abnormdist[S] 2 points3 points4 points 17 days ago(0 children) True, I didn't publish the visualization code since it was pretty badly written to get the repo up quick. I'll provide a cleaner version soon. Thanks for the feedback.  []abnormdist[S] 2 points3 points4 points 17 days ago(6 children) Update: The code to infer on your images is now there, hope you have fun testing it out.  []FreckledMil 1 point2 points3 points 16 days ago(2 children) Do you happen to have a version that works on any hosted notebooks? just wondering. Seems to be a waterfall of issues related to python versions and TF when I just tried to.  []abnormdist[S] 0 points1 point2 points 15 days ago(1 child) I don't think I want to go through the hassle of supporting colab notebooks. Because then I have to answer questions related to Colab functionality which I'm not responsible for. The infer code will run on your cpu even, so you can rest it out locally if you have decent ram in your machine.  []FreckledMil 0 points1 point2 points 15 days ago(0 children) Fair enough, thanks for reply. was just curious.  []BobFloss 1 point2 points3 points 15 days ago(0 children) Glad to hear it, I'll be trying it soon!  []alvisanovari 0 points1 point2 points 9 days ago(1 child) This is awesome! Great job! :) I made a Colab in case it comes in handy: https:colab.research.google.comdrive1vdNx-snivEBGqzbX1AaZV7sP4Znd1xJT Quick note: Looks like you are importing dataloader unnecessarily and it's throwing an error. I'd take the first line out in infer.py. Again - this is awesome! :)  []abnormdist[S] 0 points1 point2 points 9 days ago(0 children) Thanks for the notebook. Some people here were asking for it as well. Also thanks for the import bug, I already pushed a fix.  []abnormdist[S] 0 points1 point2 points 17 days ago(0 children) Update: the code for inference on your own images is provided. Have a look at infer.py.  []fdskjflkdsjfdslk 1 point2 points3 points 17 days ago(1 child) Cool stuff. I want to use it to upsample low quality videos, for scenarios when you may not have access to high speed internet. This could also be useful in other scenarios (e.g. if you only have a low resolution version of a video to begin with).  []abnormdist[S] 1 point2 points3 points 17 days ago(0 children) True, the applications seem interesting.  []saynay 1 point2 points3 points 16 days ago(1 child) I was looking in to a similar thing a few weeks ago. Specifically for SR on video, I found this paper interesting, and wondered if it would be possible to adapt a MobileNet backbone to it. Unfortunately, I am not nearly proficient enough yet to attempt that.  []abnormdist[S] 0 points1 point2 points 16 days ago(0 children) Looks interesting. But also seems PSNR focused. The architecture and ideas seem highly relevant. Thanks for sharing.  []shikhragimov 0 points1 point2 points 15 days ago(1 child) Do you plan to use some seq2seq approach?Video frames depends on the previous vidoeframe. This could give a better result than just enhancing a frames separately. Also you should predict scene changing  []abnormdist[S] 0 points1 point2 points 15 days ago(0 children) That would be a future direction. The first step was to be realtime on individual frames.  []shikhragimov 0 points1 point2 points 15 days ago(1 child) if you could - make the comparison with original SRGAN and ESRGAN (speed and quality). Great work  []abnormdist[S] 0 points1 point2 points 15 days ago(0 children) Speed makes sense. Not sure about the quality. Since higher PSNR and SSIM scores don't necessarily mean better looking images to a human.  []howdoireachthese 0 points1 point2 points 17 days ago(2 children) Very timely, I'm just about to do a project on the topic and would love to see how this measures up  []abnormdist[S] 2 points3 points4 points 17 days ago(0 children) Awesome, glad this is useful!  []Bazzert_One 1 point2 points3 points 15 days ago(0 children) interesting field. Here is the papers with code chart for recent models: https:paperswithcode.comsotaimage-super-resolution-on-bsd100-4x-upscaling and a couple papers about SR as a whole: https:arxiv.orgabs1904.07523 https:arxiv.orgabs1808.03344  []MaxTalanov 0 points1 point2 points 17 days ago(1 child) Nice work, I was actually looking for something like this!  []abnormdist[S] 1 point2 points3 points 17 days ago(0 children) Yay! Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 25787 on r2-app-0cc8d801970037458 at 2019-11-14 22:40:43.438424+00:00 running cfe41af country code: US. "
  },
  "9":{
    "title":"[D] Statistical Physics and Neural Networks question. : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,833 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[D] Statistical Physics and Neural Networks question. (self.MachineLearning) submitted 6 days ago * by AlexSnakeKing If you look at the theoretical physics literature, there's a ton of research being done on the statistical physics of neural networks and the statistical physics of deep learning, etc...where they use analogies between spin glasses and condensed matter models to get to all sorts of theoretical results about neural networks. To be clear, I'm not talking about studies were neural nets were used to model and solve a problem in statistical physics. I'm thinking about the line of research were the mathematics of statistical physics and spin glasses are used as frameworks to analyze the behavior of neural nets, and then arrive at conclusions like The loss surface of neural nets have this particular topological property or CNN show a phase transition when the number of classes jumps from x to y, etc..... My question is: Did any of these theoretical results from the analysis of neural nets using methods from physics ever lead to any practical results, such as a faster training algorithm, or improved generalization ability, etc....? As far as I can tell: No, none of the popular NNet models incorporate results from these physics inspired studies. All the improvements come from purely mathematical insights, or originally from biological insights. But I might be wrong: Did any of the significant practical developments in NNets and Deep Learning (better activation functions, training algorithms, regularizations methods,...) stem from the statistical physics approaches? Post a comment!  []mec_con 59 points60 points61 points 5 days ago(15 children) Hey, I worked a bit on this myself. Personally I'd say the field is still very young (even though a lot of what we see today actually started in the 80s), and that most of the discoveries are of theoretical interest, thus not necessarily practical. You are right for instance that none of this progress has yet been translated into something that has become mainstream in deep learning. That being said, lots of theoretical questions are also practical. You can ask yourself, for instance, why is one architecture better than another? If you are able to answer such a question, you will have a better idea of what to do once you face a real problem. In other words, you won't have to try lots of different architectures and perform cross validation or whatever. Statistical physics can actually be used in different ways for this. For the approach you mentioned, based on spin glasses, the main idea typically is to study, on an idealized setting (e.g. assuming data comes from a certain distribution), what is the average performance one obtains using a certain class of networks. This can be seen already in older works from the 8090s by Gardner, M\u00e9zard, Opper and more recently in works by people such as Montanari, Zdeborov\u00e1 and Krzakala. Another line of work uses a more dynamical approach to understand how a given input propagates through a random neural network and is able to derive, from this, some general properties the network will have. You can see this in works by Ganguli, Pennington, Schoenholz, Yang, Karakida and so on, but also this can be tracked to older works by people such as Amit. Yet another approach which is perhaps worth of mention is the development of better approximate Bayesian inference algorithms, which could be used e.g. for Bayesian deep learning, but these have not yet shown to be practical in larger problems (these are known as TAP equations, or approximate message-passing in the recent literature). This idealized setting, in which lots of things are random, is still a big issue for many of these works, but the assumptions are being relaxed little by little. This is why I say the field is still young , because we see year after year that progress is being made, even if there's still no groundbreaking approach that translates all of this into a big practical gains. But who knows, it might be around the corner... That was everyone's hope when I used to work in this field.  []bohreffect 15 points16 points17 points 5 days ago(5 children) I'm curious, what are you thoughts on the combinatorial topologists' take on neural networks? Specifically in reference in the Manifold Hypothesis and viewing NN layers as a knot-theory problem in rendering data linearly separable? I appreciate the differential geometry perspective, but I just keep getting this gut feeling that no one wants to touch the combinatorial aspects of NN architectures, i.e. number and width of layers, combinations of activation functions, etc by casting everything in the limit. edit: I'm just a super casual in this area and would be great to co-opt someone's take for lab meetings.  []StellaAthena 12 points13 points14 points 5 days ago*(3 children) Mathematician, but not a theoretical physicist here: I think Taco Cohen and his collaborators have done phenomenal work on manifold and gauge invariant CNNs. If I had to give an award for best theoretical DL research in the world, it would be them. Their framework is revolutionary on both a theoretical and an applied level. I think you're right about combinatorial approaches being eschewed because nobody wants to do the number crunching, but the end of your sentence confuses me. What did you mean about limits? Are you referencing recent work on infinitely wide neural networks?  []bohreffect 1 point2 points3 points 5 days ago(2 children) Oh, yeah, bad grammar there. Yeah, as opposed to the combinatorial problems; sure things like infinitely wide networks, but also just limits in the analytical sense. Loss surfaces, geometry of each layer viewed as a diffeomorphism, etc. Gauge theory is super fascinating but so far above my paygrade, I hate knowing that there's impactful math like that that won't be accessible to me in my lifetime.  []StellaAthena 1 point2 points3 points 5 days ago(1 child) One thing in a more combinatorial direction you might be interested in is I am looking at how the network structure's graph theoretical properties influence the set of expressible functions. So fix some set of graphs {G_i} and only build networks with those structures. What interesting things can we say about expressible functions? If we go from using {G_i} to using {H_i}, where G_j and H_j have some known relationship (day H_j is G_i with one edge deleted) what can we say about how much we lose gain moving from using G_i fo H_i. It's still pretty preliminary, but there are positive signs.  []MemeBox 0 points1 point2 points 4 days ago(0 children) On the practical side - I have wondered how much of the success of transfer learning is down to the set of graphs that are prevalent within the network? Can we simply produces networks that express certain graphs at certain rates and produce networks that operate out of the box a la reservoir learning? Or are much quicker to learn to train for a given domain? I would imagine that this is the approach of biology, the overall structure of the networks determines the kind of problem that the network is best suited to solving. Maybe you could use a GAN to generate new networks...  []clueless_scientist 1 point2 points3 points 5 days ago(0 children) viewing NN layers as a knot-theory problem in rendering data linearly separable? You're talking about that one demonstration in the Cris Colah blog? The trick is that knots have interesting behaviour only in low-d space. So, that example looked fascinating, but was too low-d and gave a false analogy imho.  []maxToTheJ 11 points12 points13 points 5 days ago*(3 children) Also OP is making a bad assumption about how theory works. The word \u201csignificant is a big red flag for ambiguity which could lead to goal post moving  A) Not all theory is going to be felt day by day . If a theoretician proves some assumption like about convergence ect that the community has taken for granted will it really matter? What if the proof is especially hard to follow for the ML community then it will obviously end up with an ignore or \u201cill take your word for it situation  B) what if you come up with something practical but it is based on complicated theory? People adopt the intuitive stuff that is easy to apply first and then the more difficult stuff in their field and lastly the relevant stuff in other fields. This leaves complicated stuff outside of the ML mainstream absolutely in the back of the line. C) Lets be honest the ML community is super \u201chypebeast ish . A while ago RBM were hot and it would be way more obvious the answer to OPs question. If you want an example here https:pypi.orgprojectWeightWatcher  []AlexSnakeKing[S] 3 points4 points5 points 5 days ago(2 children) Also OP is making a bad assumption about how theory works. The word \u201csignificant is a big red flag for ambiguity which could lead to goal post moving - I disagree. Here's why: Several of the same authors who worked on the intersection of stat. physics and spin glasses and neural networks also worked on the intersection of stats physics and spin glasses and optimization. But in the optimization realm, several practical algorithms came out of the community, such as Simulated Annealing and Survey Propagation. I think it is legitimate and unambiguous to ask why hasn't the same happened with their work on NNets?  []maxToTheJ 2 points3 points4 points 5 days ago(0 children) Here's why: Several of the same authors who worked on the intersection of stat. physics and spin glasses and neural networks also worked on the intersection of stats physics and spin glasses and optimization. What is the motivation for the assumption that it has to be the exact same authors that had success with optimization to be the ones that do so in NNs?  []Fujikan 1 point2 points3 points 4 days ago(0 children) Not jus SA and SP, but also Expectation Propagation and Approximate Message Passing have had a significant impact in a number of fields. There are many other meaningful techniques to come out of the literature, besides :)  []the_new_scientist 4 points5 points6 points 5 days ago(1 child) I'm quite interested in this topic, and I agree with you, though the seeds of these ideas were generated decades ago, the field is quite young. I've seen some research recently that used tools from stat mech to analyze the stochastic block model, and track phase transitions in the computational hardness of it. Curious as to what you worked on?  []mec_con 1 point2 points3 points 5 days ago(0 children) Yes the tools used to analyze the SBM are the same used in the first point I mentioned above. I have worked mostly on the first and third topics I mentioned (using spin glass techniques to analyze inference problems, as well as message-passing algorithms to solve them in practice).  []Diffeologician 0 points1 point2 points 5 days ago(1 child) Yet another approach which is perhaps worth of mention is the development of better approximate Bayesian inference algorithms, which could be used e.g. for Bayesian deep learning, but these have not yet shown to be practical in larger problems (these are known as TAP equations, or approximate message-passing in the recent literature). Do you have a reference on this? I'd be interested in how this relates to the notions of message passing I've seen in PL literature.  []mec_con 0 points1 point2 points 5 days ago(0 children) You can maybe take a look here: https:arxiv.orgpdf1511.02476.pdf. I'm not sure what notions of message-passing you are thinking about, but it's quite related for instance to belief propagation, and not at all related to message-passing as in MPI.  []Fujikan 0 points1 point2 points 4 days ago(0 children) large problems Zdeborova & Krzakala's group have been progressing further and further into more practical ML systems. When I left we had some good work and understanding on practical-scale RBMs, and from there into Deep Boltzmann Machines. For more interesting multi-layer networks, indeed, their latest works with Gabri\u00e9 or Gold would be of interesting. Also, as mentioned somewhere else in these comments, R. Zecchina has been doing work in this area and had a number of interesting results with his co-authors.   But who knows, it might be around the corner... That was everyone's hope when I used to work in this field. Literally every result was This is gonna be it! but somehow never quite gains traction :P Sometimes the resulting algorithms are a little bit cumbersome in terms of efficient implementation, so it seems like other works turned to just trying to better understand the dynamics of conventional stochastic optimization rather than deriving new frameworks. A little bit of a shame, perhaps!  []NichG 14 points15 points16 points 5 days ago(3 children) Boltzmann machines are probably the most relevant current example I can think of, even if they're getting obsolete now for the most part. Hopfield networks are an older case - you can calculate their storage capacity with stat phys methods that you would use to calculate the memory of a spin glass. It's a bit more murky, but there are stat phys calculations that let you determine the properties of infinitely deep neural networks if they have homogeneous architectures. That in turn lets you determine good initializations for stable gradients. But stat phys is one of a number of ways to derive that. I'd also argue that designing architectures with arbitrary invariants makes more sense in terms of the way physics handles symmetries, gauge fields, etc than the popular account of e.g. 'we use convolutions because the human visual system has a columnar structure'. The formalism lets you work out how to make other invariances: permutation invariant, rotation invariant, scale invariant, invariant under discrete symmetries, etc. There are a number of papers that use those constructions to make conv-net equivalents for their domains without needing a biological analogue to copy from. But you could argue that's group theory rather than physics.  []twesterhout 2 points3 points4 points 5 days ago(2 children) Any chance you could point me to some papers which manage to impose invariance under discrete symmetries?  []NichG 6 points7 points8 points 5 days ago(1 child) We actually did one for permutation invariance (https:arxiv.orgabs1612.04530), which is a discrete symmetry. There have been a number of followup papers by other groups which have improved the initial result, and it's worth noting that attention layers also obey this symmetry. There are more general recipes for other symmetries in https:arxiv.orgabs1602.07576 and http:papers.nips.ccpaper5424-deep-symmetry-networks.pdf. The former includes a reflection-invariant convolution operation, for example.  []StellaAthena 1 point2 points3 points 5 days ago(0 children) Cohen et al. also have a paper extending their manifold geometry ideas to local geometry using gauge theory: https:arxiv.orgpdf1902.04615.pdf  []panties_in_my_ass 7 points8 points9 points 5 days ago(2 children) If you look at the theoretical physics literature, there's a ton of research being done on the statistical physics of neural networks and the statistical physics of deep learning, etc...where the mathematics of statistical physics and spin glasses are used as frameworks to analyze the behavior of neural nets, and then arrive at conclusions like The loss surface of neural nets have this particular topological property or CNN show a phase transition when the number of classes jumps from x to y, etc. Can you please link some of these papers? Or cite the major authors in the area? This sounds fascinating!!  []AlexSnakeKing[S] 4 points5 points6 points 5 days ago*(0 children) Here are some examples https:journals.aps.orgpreabstract10.1103PhysRevE.90.052813 https:arxiv.orgpdf1907.07578.pdf Kabashima, one the authors of the first paper, and Zecchina, one the authors of the second one are both big names in the filed, so are Marc Mezard, and Remi Monasson. A lot of these authors work mainly on analyzing optimization algorithms from a statistical mechanics point of view, but they occasionally venture into Neural Network territory.  []mec_con 3 points4 points5 points 5 days ago*(0 children) For an overview of spin-glass related techniques, as well as message-passing algorithms, I'd recommend this review by Zdeborov\u00e1 and Krzakala: https:arxiv.orgpdf1511.02476.pdf, and citations on there. It doesn't mention neural networks so much though, as they have been working on it mostly since the review has been published. For that you can look at their works on NeurIPS and ICML this year and the last. For analyses of neural networks using dynamical techniques, you can look at any of the recent works from Jeffrey Pennington, Sam Schoenholz and Greg Yang.  []bbsome 5 points6 points7 points 5 days ago(0 children) Most often the answer is No since all of these models that I've seen make way to many simplifying assumptions to be relevant in practice. Probably the only results that are truly valid are the Mean Field approximations of nets at initialization - they seem to describe even for finite widths very well the behaviour. This has led to a bit better initializers. However, this is only at initialization. All of the NTK stuff though seems to be not relevant to finite widths, for instance, hence it just remains as an infinite limit result (to my knowledge). The spin-glass models from statistical physics I don't think have had any interesting results with practical uses.  []cecri17 3 points4 points5 points 5 days ago(1 child) Sevral examples. 1) Markov chain monte-carlo, that we use for Bolzmann machines and reinforcement learning is absolutely form physics. 2) In general optimization problems, we never expect that a variant of 1st order optimization work so well as in neural network examples. Some people try to investigate this problem using the continuous formulation of SGD which yields the Fokker-Plank equation that combines the Hessian and the Fisher matirx of the loss function. Fokker-Plank equation is originally from physics to describe non-equilibrium system.  3) Moreover, now physicsists started to study generalization using their tools. Google brain has a team consists of physicsists. One of their recent results is that in wide limit, neural networks behaves like a gaussian process. They mainly used tools from physics to prove this.  []AlexSnakeKing[S] 0 points1 point2 points 5 days ago(0 children) Cool. Can you point to the paper(s) that describe that last result?  []charlesbovarycestmoi 4 points5 points6 points 5 days ago(0 children) Most studies in the field are theoretical and try to get an understandingcharacterization of NNs, somewhat like if they were a physical phenomenon to be understood. A couple of works that explore algorithmic ideas on the more practical side are these (disclaimer: I'm among the authors) https:arxiv.orgabs1611.01838 https:arxiv.orgabs1707.00424 the ideas originate from a statistical physics study based on spin glass theory (https:www.pnas.orgcontent11348E7655 and others) and that is still heavily under development (e.g. https:arxiv.orgabs1905.07833).  []brekelma 1 point2 points3 points 5 days ago(0 children) You might find videos from this workshop interesting! https:icml.ccConferences2019ScheduleMultitrack?event=353  []StellaAthena 1 point2 points3 points 5 days ago(1 child) Could you link to a couple papers that you have in mind here?  []AlexSnakeKing[S] 1 point2 points3 points 5 days ago(0 children) Here are some examples https:journals.aps.orgpreabstract10.1103PhysRevE.90.052813 https:arxiv.orgpdf1907.07578.pdf  []the_new_scientist 1 point2 points3 points 5 days ago(0 children) I'd like to ask a similar question: have optimization style proofs (proving general theorems about SGD and NNs ) aided in practical architecture searches?  []Sweet_Significance 0 points1 point2 points 5 days ago(2 children) Can someone suggest sources where i can learn more about this field? How is satistical physics related to this? I was always curious about the behaviour of deep nets for example and when i asked a similar question on another sub all i got was deep learning is mostly engineering there is not much theory going on.  []StellaAthena 0 points1 point2 points 5 days ago(0 children) Theoretical DL is widely maligned by the community unfortunately. There are some people doing interesting theoretical work, but you have to know where to look. Taco Cohen, Risi Kondor, and Benjamin Bloem-Reddy are three people who do excellent theoretical work, though none of them are from the statistical physics side of things.  []AlexSnakeKing[S] 0 points1 point2 points 4 days ago(0 children) This paper: http:proceedings.mlr.pressv38choromanska15.pdf (and the references within) provide a useful starting point. How is satistical physics related to this? The idea is roughly the following (obviously an oversimplification that still gets the point across): Take a mathematical expression of a neural network model, and then find a similar expression in the field of statistical mechanics or condensed matter physics. A typical example is to represent a loss function as the temperature or energy of a complex system of interacting particles. Now you can take known physics results about how those systems behave when their temperature or energy is reduced and apply it to your neural network model to see how it should behave when you try to minimize the loss function.  []lucidrage 0 points1 point2 points 5 days ago(0 children) The loss function for cycleGAN used chemical equilibrium as an inspiration for training the generator and discriminator I think. Chemistry is the application of physics so I guess this counts.  []seanv507 -1 points0 points1 point 5 days ago(0 children) I would say the answer is no, but I think the question is also wrong. I believe the question assumes that neural networks have some magic which other ml techniques don't. I would argue that the main successes of nnets are in image processing with CNN's, The results there are essentially automatingoptimising end to end the standard image processing pipeline that has already been done since ? the 80s. TLDR: the amazing results in neural networks are in image processing. If you want to understand these study image processing.  Eg https:arxiv.orgabs1203.1513 https:openreview.netforum?id=SkfMWhAqYQ E  [+]yetanothernormalG comment score below threshold-28 points-27 points-26 points 5 days ago(10 children) I know both of the PhD in physics, Licentiate in ML. I have not read yet the spin glass papers but I did my PhD in structural glasses. If you have this kind of education you understand why your question does not make any sense.  Do you? Do you know what is the difference between supervised and unsupervised learning? Or how can the concept of entropy help making better deep belief networks? What is Gibbs sampling? Who is Gibbs? What is a Boltzmann Machine? Who is Boltzmann? What is a dynamical phase transition and why does it matter in deep learning? and I can go on. These things are not something I read. These are just basic thoughts that come to my head because ML is basically statistical physics. To see it, I suggest, spend 10 years doing physics, and five at doing ML (after being raised by physicists). Btw, why is it possible to use HMC ? For Bayesian leasning ? The answer is know by ever 3rd year physics student, how many 3rd year ML students know the answer? I see both sides. The ML side is 500 years lagging in culture. The way they think, the questions they ask... are not coming from 2000 years of distilled scientific culture.  Think about this... I know both sides and physicists are really killing ML... on the theoretical side. Why? Because the ask the right questions. Why? Because they have 2000 years of culture in their science.  Pay close attention to physics, classical mechanics, statistical physics, spin glasses, phase transitions, inverse problem solving. Over and out. I got tired of pissing ppl off who either are pissed or not. The ones who are will not start to learn anything from this post. The ones who are not have a PhD in physics too.  []BoltzmannMachine 17 points18 points19 points 5 days ago(0 children) I see your phd in physics has really humbled you  []adventuringraw 17 points18 points19 points 5 days ago(0 children) I have not read any of the relevant papers, but my education was in math and physics. If you have this kind of an education, you understand why your post does not make any sense. Do you? Do you know the important questions in this field?  Who is Richard Feynman?  Albert Einstein?  Most importantly, who is John Galt? I can go on. these are not just things I read, these are basic thoughts that come into my head because of how much I know. To see it, I suggest you spend at least 2000 years of culture studying machine learning, and at least 50 years practicing as a professional. I see both sides. You see, I am an immortal vampire. I am John Galt. I founded physics 2,000 years ago in ancient Greece, and I will still be here when the world is lost to ice and vacuum at the heat death of the universe. Pay close attention to the old magic, physics, and inverse problem solving. The truth is out there. Over and out, this is John Galt.  []StellaAthena 12 points13 points14 points 5 days ago(1 child) This comes across extremely obnoxious. There are a million far more pleasant ways to explain why you think this is a bad question.  [+]yetanothernormalG comment score below threshold-7 points-6 points-5 points 5 days ago(0 children) i am not a pleasant person when it comes to deep learning stupidity  []Andy_Reds 2 points3 points4 points 5 days ago(5 children) Someone reading your complaints in this comment would think you were a pure mathematician, not a physicist. It's funny watching physicists, of all people, complain about some machine learning researchers having no deep understanding of the theory behind what they're doing. I would suggest someone coax a physicist into explaining what quantum field theory actually is to see what I mean.  []facundoq 1 point2 points3 points 5 days ago(0 children) You overfit mnist? I overfit the laws of nature  []yetanothernormalG -1 points0 points1 point 3 days ago(3 children) yeah, thats the thing.... at least they are about these questions they care about understanding what they are doing... ML ppl don't i think... i just came to some awesome realization why dl works... but this community is not ready for it  if i see the reactions to this post... my god... i think i just stick to my ppl who actually know what a spin glass is... and free energyand entropy and can come to a conclusion why deep learning workson their own why? coz they know statistical phyisics... LOOOL  watch all the python monkeys.... doing DEEEEP learning more and more and more layers but it is not neuronsor the layers what makes it work it is something totally different but ppl dont realise it... yet 5 years10 years it will be widespread and realised and neurons and layers will be thrown away... why ? beacuse people from physics understand why it works ... they care about it... i dont even try to explain it... my friends understand it...  i dont wanna explain it to ppl who do not even know what free energy is and how it connects to deep learning and bayesian statistics.... hamiltonian monte carlo and so on..... leave me alon... i won't post here anymore.... it's pointless  []adventuringraw 0 points1 point2 points 2 days ago(2 children) it's true that a lot of people playing with deep learning after doing a few MOOCs don't understand even how much they don't know. But you're seemingly not claiming that, you're also apparently claiming all the actual deep learning researchers also know far less than you. If you think you know so much more than every single well cited researcher exploring the practical and theoretical nuances of deep learning... okay dude. You just keep on believing that, enjoy going back to 'your people' where every single one of the real researchers live. If even a handful of machine learning researchers know more than you though, that makes everything you just said complete bullshit. You're not just saying you know more relevant theory than everyone else in this subreddit (which could potentially be true, this is just an internet forum) you're saying you know more than absolutely everyone in the entire field.  Incidentally, if you really do truly have deep insight that the machine learning community would benefit from, someone that wasn't an asshole would share relevant research (or textbooks even) instead of just saying how little all the machine learning researchers know. Even more, publish a goddamn peer reviewed paper if you've got the breakthrough that'll shake this field loose, find a collaborator in the space if you need help getting attention and citations for your effort. Cross disciplinary collaboration is common in science, if you really do have important insight, publish it. Posting it on here would obviously be a waste anyway, truly novel contributions should be published, not just posted. Contrary to what you seem to think, there are people who do care how things work. Until there's a proper way of relating network topology with capacity and learning rate, theory will obviously have a long way to go, only a fool things a black box is sufficient forever. Deep insight leads to deep improvements. People here aren't mocking you because they think theory doesn't matter. They're mocking you because you make it sound like literally not a single researcher in ML cares about theory, or knows half as much as you and 'your people' do. That doesn't make you smart, it makes you an asshole. Or maybe both smart and an asshole. Given Cauchy and Fisher and many other historical assholes though, I guess you've got plenty of company. Though both Fisher and Cauchy actually published their work and left a mark in history. Course, for all I know, maybe you've gotten thousands of citations to your name, and made waves in the physics community. If that's you, I hope you use your great experience and insight to help move the machine learning community forward a few decades too, when you have the time.  []yetanothernormalG 0 points1 point2 points 2 days ago(1 child) ok here we go, show me what you got what is the difference between supervised and unsupervised learning you got one hour or let me give you an easier one... this gonna be a good one... what is the curse of dimensionality ? for this you have 10 minutes... this is the irony in the story... convince me that you have something behind what you say...  []adventuringraw 0 points1 point2 points 2 days ago*(0 children) dude, are you serious? You roll up your sleeves and ask to throw down, and this is the level of basic questions you think people on here don't know about? The comments in this subreddit are sometimes brain dead, it's true, but there's some excellent papers that come and go, all of which demand an enormous amount of mathematical maturity and familiarity with the literature. And I've met a number of incredible people on here, including PhDs in pure math, arguably people with useful knowledge 'your people' don't even possess on average, depending on the questions being asked. But alright, I'll indulge you.  Supervised learning typically has two objects in the training set, X (n x m features) and Y (n x t target dimensions) and given some function family, you're looking to tune the free parameters to get a minimum error rate using your metric of choice (MSE, etc) for transforming X to \\hat Y, the approximation of Y. That same formalization applies to whatever function family you're using, whether it's an SVM, a GLM, a deep neural network, or whatever other kind of model family you think will best fit the observed data. This approach partly comes from classic statistics, where you can often fit the distributions (single dimensional Gaussian for example) analytically instead of needing an iterative solution like gradient descent. The formalization is slightly different for classification, often breaking down into either a full generative model (estimating p(C,D)) a conditional generative model (p(C|D)) or the discrimination function directly, f: X -> class. Either way, the actual final model gives you a set of decision manifolds dividing the feature space into K disjoint regions. Unsupervised amounts to pulling out what you can from unlabeled data. Given just X above, in other words, with no Y. A lot of recent papers in this space use a mutual information metric to try and break apart a dataset into categories, I'm particularly interested in representation learning, so this comes up frequently there. Often unsupervised learning tasks can be turned into self-supervised learning tasks (more common in NLP originally, though there's more work in computer vision coming out now too) by pulling out part of each sample and training the model to impute the missing piece. Removing chunks of images for example, or words in a sentence. The hope is that the model learns a broadly useful representation of the dataset, meaning fine tuning on broad range of tasks you actually care about will hopefully be possible with your trained self-supervised model. The curse of dimensionality is a pretty broad concept, that reveals itself differently depending on the question you'd like to ask. One way to frame it is in terms of sparsity. Breaking apart your feature domain into a lattice, the number of cells (say, 9 cell dividers per dimension) means your cell count goes up as 10D . Meaning it takes a greater and greater number of samples to get observations in the various regions of the data manifold. 'distance' starts to get wonky in high dimensional space as well. One illustrative example is (volume of a hyperspherevolume of a hypercube) where the half-edge length of the hypercube is equal to the radius of the hypersphere. You get piD2 D2D-1 Gamma(D2). With Sterling's approximation, you end up with the ratio going to zero as D -> infinity, meaning more and more of the volume of each cube is concentrated in the 'spikes', so to speak. In practice, it means that standard low-dimensional methods (like KNN for example) ends up completely worthless, because any typical distance measure isn't going to properly find points that are actually all that close in the data manifold. You also end up with strange properties around where the volume is actually concentrated in hyperspheres, meaning (among other things) for high dimensional Gaussian distributions, your samples aren't actually likely to come from the regions of highest density, since all the actual volume is concentrated in a shell farther out. A final useful example of the curse of dimensionality comes from na\u00efve methods like polynomial curve fitting. Taking a degree M polynomial fit to D dimensional data, it can be shown that the number of independent parameters goes up according to (D + M)!D!M!, so for even very moderate values of M, D climbing leads to an explosion of parameters to fit, making a lot of basic tools (KNN and polynomial curve fitting being my two examples here) worthless as D starts to climb. More detail on both of these topics can be found in Bishop's chapter 1, Elements of Statistical learning, and so on. These are very basic topics, and you're right. If someone doesn't have a clear answer to your questions, then they're a rank beginner and don't know even the beginnings of the theory. You're not wrong that theory's important, you're just wrong that no one cares, and no one knows. And I'm aware my answers probably aren't the best, I've only been studying for two years. I figure I have a decade ahead of me even to really start to deeply understand all the related topics I wish to understand. But I'm doing the work, you're an ass for assuming everyone's just happily playing with deep learning on the cloud without any clue what they're doing, or interest in learning. And you're wrong too if you think that no one in ML understands that physics matters. this paper in particular shows the bleeding in of gauge theory on a topic I'm quickly finding is my main area of interest... disentangled representation learning. Proper disentangling requires group actions the agent can take, revealing the symmetries under actions in the world model being learned. It does obviously look like physics has a lot to offer to the future of ML, you're not wrong. I've got a statistical mechanics book on my Amazon list, I'm still working on my math fundamentals right now though (pushing up towards Munkres, and getting into dynamic systems) so I have to be judicious about how I spend my time. I work full time and have a family, so I haven't had the luxury of covering as much theoretical ground yet as I'd like, but I'm slowly getting there. Anyway. Like I said. You probably know more than me, and you probably know things that could be of value to the ML community, assuming you actually are a PhD somewhere and not just a complete fake. No one here is questioning physics being useful. We're questioning your arrogance, in assuming no one here knows fucking anything. Like I said, you're maybe not stupid when it comes to physics, you're just needlessly making completely overblown assumptions about other people's inferiority. People aren't going to look kindly to you looking down your nose at them. In fact, if you're going to ask me basic fucking questions to see if I know even the absolute bare minimum, let me ask you a question that my kid would easily be able to answer. In your own words, why do you think people insulted you after you posted your first comment? If you know even the first thing about interacting with other humans, the answer should be obvious. I've literally told you several times already.  I still can't believe you literally thought an average person on this subreddit couldn't tell you about supervised vs unsupervised, or the curse of dimensionality. Do you really, truly believe people on here are all so completely ignorant about the absolutely basics of machine learning? Do you think no one can read the papers being posted on here? Who's posting them if no one can read them? A lot of the papers on here are no joke if you can get through them and understand everything. Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 14224 on r2-app-04a6d3b2b4de81e7e at 2019-11-14 22:38:53.771855+00:00 running cfe41af country code: US. "
  },
  "10":{
    "title":"[R] Announcing Confident Learning: Finding and Learning with Label Errors in Datasets : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,644 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Research[R] Announcing Confident Learning: Finding and Learning with Label Errors in Datasets (self.MachineLearning) submitted 10 days ago * by cgnorthcutt Hi, Reddit. I'm excited to share confident learning for characterizing, finding, and learning with label errors in datasets. This is joint work with co-authors Lu Jiang (Google Reserach) and Isaac Chuang (MIT). To promote and standardize future research in learning with noisy labels and weak supervision, I've also open-sourced the cleanlab Python package: https:pypi.orgprojectcleanlab Post: https:l7.curtisnorthcutt.comconfident-learning Title: Confident Learning: Uncertainty Estimation for Dataset Labels Abstract: Learning exists in the context of data, yet notions of confidence typically focus on model predictions, not label quality. Confident learning (CL) has emerged as an approach for characterizing, identifying, and learning with noisy labels in datasets, based on the principles of pruning noisy data, counting to estimate noise, and ranking examples to train with confidence. Here, we generalize CL, building on the assumption of a classification noise process, to directly estimate the joint distribution between noisy (given) labels and uncorrupted (unknown) labels. This generalized CL, open-sourced as cleanlab, is provably consistent under reasonable conditions, and experimentally performant on ImageNet and CIFAR, outperforming recent approaches, e.g. MentorNet, by 30% or more, when label noise is non-uniform. cleanlab also quantifies ontological class overlap, and can increase model accuracy (e.g. ResNet) by providing clean data for training. Paper: https:arxiv.orgabs1911.00068 Code: https:github.comcgnorthcuttcleanlab Top 32 label issues in the 2012 ILSVRC ImageNet train set identified using confident learning. Label Errors are boxed in red. Ontological issues in green. Multi-label images in blue. Post a comment!  []brb92 4 points5 points6 points 10 days ago(4 children) How would this work in case of imbalanced datasets where a overunder sampling is applied ? It would affect the joint probability, right ?  []cgnorthcutt[S] 2 points3 points4 points 10 days ago*(3 children) Thanks for your question! The short answer is CL is unaffected by class imbalance except for discretization error. By discretization error, i mean that if you only have 3 examples in a class 'A', and the true probability class 'B' was flipped to 'C' is 0.7... the best you could do with 3 examples is 23 = 0.67. The long answer is: CL takes predicted probabilities and noisy labels as inputs. If you're predicted probabilities are random, then CL can't work. In the theory section of the paper, we prove realistic conditions when CL will exactly find label errors even when there is error in predicted probabilities for every example and class. If you use tricks to deal with class imbalance when you compute your predicted probabilities (loss weighting, bagging, data augmentation, etc.)... then in practice you should be fine. Similarly, if you have some class imbalance, but it's not so extreme as to produce wonky predicted probabilities, then again in practice you should be fine. For example, some of the classes in ImageNet training set have fewer examples, but using CL, we did not observe less accurate label error detection in these classes (verified by humans). Note predicted probabilities should be computed out of sample. We use four-fold cross-validation in the paper. If you have the computation, you can increase the folds for better results.  []brb92 0 points1 point2 points 10 days ago(2 children) I see, thanks for your answer and congratulations for the very nice project. I will for sure deep dive more on it when I have more time.As you use only per-class probabilities to define the thresholds, I understand that indeed it should not be affected by class imbalance.However it is still important to have calibrated predicted probabilities as output, because if not we will not be able to correctly distinguish between noisy and not-noisy examples. Am I right ?  []cgnorthcutt[S] 1 point2 points3 points 10 days ago(1 child) Think of it this way. If your predicted probabilities are bad (due to whatever: class imbalance, bad model choice, etc), then even if you have ZERO label errors and don't use CL at all, you'll still have bad predictions.  []brb92 1 point2 points3 points 10 days ago(0 children) Yep, makes sense for me \ud83d\ude05Thanks again for your answer and for opening up this project :)  []alexrenda 1 point2 points3 points 10 days ago(0 children) Mirror: https:web.archive.orgweb20191104143423https:l7.curtisnorthcutt.comconfident-learning  []dracheschreck 1 point2 points3 points 10 days ago(1 child) Does it work for regression problems as well?  []cgnorthcutt[S] 0 points1 point2 points 10 days ago*(0 children) Thanks for your question. CL works for classification only. However, depending on the granularity you need, you can discretize your targets into m labels. For example, confident Learning works well on imagenet which has 1000 labels. That's fairly granular. If your regression targets were probabilities (bounded between 0 and 1) then it's possible CL could handle up to 3 decimals of target granularity for regression. This is reasonable theoretical conjecture: I have not run these experiments Throughout our work, we use the term label to refer to a class label, reserving targets for regression target values. And we only deal with noisy labels.  []liqui_date_me 1 point2 points3 points 9 days ago(0 children) Isaac Chuang is doing ML now?  []speyside42 1 point2 points3 points 9 days ago(0 children) Nice work. Would be interesting how this compares to more recent works like o2u-net: http:openaccess.thecvf.comcontent_ICCV_2019htmlHuang_O2U-Net_A_Simple_Noisy_Label_Detection_Approach_for_Deep_Neural_ICCV_2019_paper.html  []jrkirby 0 points1 point2 points 10 days ago(1 child) I wonder what happens when you apply this to random data, or regular data with random labels. Deep learning has been shown to fit the training set rather robustly even in these nonsense cases (of course, not generalizing to any test set). Would CL be able to notice when it is set to train on a random dataset?  []cgnorthcutt[S] 0 points1 point2 points 10 days ago*(0 children) Thanks for the question. With completely random labels, predicted probabilities (assuming some regularization) tend to be more uniform. For most applications of confident learning, empirically (checked on MNIST, CIFAR, and Imagnet) the result is random performance or the prediction all one label. This often also results in edge conditions during pruning like removing all of one class if examples or removing no examples as errors because there is no signal.  []aashus18 0 points1 point2 points 10 days ago(1 child) Would this work for multi-label setup ? In that case would the analysis be similar to binary one-vs-all classifier for all the labels ?  []cgnorthcutt[S] 2 points3 points4 points 10 days ago(0 children) Hi, thanks for the question. While we do not address multi-label in the paper, the cleanlab python package fully supports multi-label. In terms of how it works, its more simple than binary one-vs-all because confident learning intrinsically works with multiple labels. The only change needed is a few lines like this s_filter = np.array([k in l for l in s]) if multi_label else s == k where you instead of considering examples with given label k, you instead consider any example that has label k in its set of labels. You can ctrl+f `multi_label` in the codebase for further inspection. In the paper, we presume single-class datasets where multiple labels are handled with the notion of collisions. Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 23392 on r2-app-0ef3edbb8f291636c at 2019-11-14 23:42:13.765311+00:00 running e8e67f8 country code: US. "
  },
  "11":{
    "title":"[P] Trained a StyleGAN on all emojis from emojipedia. Samples posted hourly. Please help describing samples! : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,338 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Project[P] Trained a StyleGAN on all emojis from emojipedia. Samples posted hourly. Please help describing samples! (self.MachineLearning) submitted 9 days ago by nehalemlabs I set up a twitter bot posting a new sample every hour here:https:twitter.comEmojiPainter I am trying to collect a dataset with images and associated descriptions. I am hoping to collect human generated descriptions to augment the existing emojis (too few to train an image - language model on). The images can be unintentionally hilarious but also sometimes disturbing! Enjoy! Post a comment!  []hearingsilence 4 points5 points6 points 8 days ago(0 children) Thanks for the nightmares  []code_refactor 2 points3 points4 points 8 days ago(0 children) Thanks, I hate it  []yusuf-bengio 1 point2 points3 points 8 days ago(0 children) Have you tried to decompose the latent space? Would be nice to see some transitions.  []phyrz 0 points1 point2 points 8 days ago(0 children) rinterestingasfuck Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 31314 on r2-app-0d0dc893e84949ffe at 2019-11-17 04:24:35.192829+00:00 running 85b1b7a country code: US. "
  },
  "12":{
    "title":"[P] DialogPT: State of the Art Conversational Model with Fine-Tuned GPT-2 (Microsoft Research) : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,514 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Project[P] DialogPT: State of the Art Conversational Model with Fine-Tuned GPT-2 (Microsoft Research) (self.MachineLearning) submitted 9 days ago by anustretch https:github.commicrosoftdialogpt According to Microsoft: The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test. Example conversation: User: Does money buy happiness?  Bot: Depends how much money you spend on it.  User: What is the best way to buy happiness?  Bot: You just have to be a millionaire by your early 20s, then you can be happy.  User: This is so difficult!  Bot: You have no idea how hard it is to be a millionaire and happy . There is a reason the rich have a lot of money Post a comment!  []illuminascent 18 points19 points20 points 9 days ago*(11 children) I've managed to get the model running generation on my PC. One thing needed to point out is that the checkpoint can NOT be loaded exactly as the GPT-2 model checkpoint from Huggingface pytorch-transformer repository. Because there's one key required by huggingfaces implementation called lm_head.weight and the corresponding key in Microsoft's checkpoint is called lm_head.decoder.weight, so fix this before you can call load_state_dict(). You'll also need to manually define the config, e.g. medium_config = GPT2Config(n_embd=1024,n_layer=24,n_head=16) because checkpoints doesn't seem to contain them. The generation works just fine by a nucleus sampling approach, and once in a while an E-O-T will be given to indicate end of one post.  Generated sample: User: What do you guys think about the coming Olympics Bot: they're having an open gym soon in June... dont think they'll be up there this time though Bot: So what's this gym called? Bot: San Diego Fitness Club Bot: Well that sounds to perfect.   Edit: Been playing with it, here are some other highlights:  User: Every post on here that has anything to do with numbers, growth, streams that average more than 3 viewers, or literally anything that mentions something other than how fun streaming is, is instantly downvoted. Bot: That is how democracy works.  User: If Trump was running against Trump, what would he tweet about his opponent? Bot: Make American soccer great again  User: What do you guys know about the highest mountain in Japan? Bot: It's a mountain, and no more.  I'd say all of the generations are grammatically acceptable and quite impressive considering how little information it was given, about 1 out of 5 appeared to be very coherent and sometimes strikingly sarcastic(much reddit). Those prompted with a clear defined topic certainly worked out better. And just as the paper's result goes I also find the medium sized model to generally outperform the bigger one.  Edit2: I've set up a colab here.  []Flag_Red 2 points3 points4 points 8 days ago(0 children) I'm impressed with the world knowledge displayed in the Trump response.  []anustretch[S] 2 points3 points4 points 8 days ago*(3 children) Thanks so much for creatingsharing the colab. I've tweaked the script a little so that the inputs to the model reflect a turn-taking conversation over IM, similar to the formatting of the output example. It seems to be performing slightly better this way. Edit: added modifications so that full conversation history in recalc()should be properly re-fed, instead of just a 1-line hysteresis in the original colab. (Notebook link updated) Edit 2: truncating prev convo input at the next available line so that it doesn't exceed the fixed embedding length; loading config file according to updated configs folder for medium sized model; can now type 'reset' to start the conversation over. Notebook v3: https:colab.research.google.comdrive1SETfvx_ot_-VhDXjwik2SxundaQZ9rDI Notebook v4 (just better input formatting etc in accordance with docs and dev feedback on github): https:colab.research.google.comdrive1-_KjlAV3J1IVDw_9KogjKDCzgFY7Jp7E Example of passing inputs to model in the original script from huggingface: [2]  []anustretch[S] 0 points1 point2 points 7 days ago(2 children) Sample output: Me: What is the meaning of life?  You: Mind over matter I assume  Me: Elaborate?  You: Life is essentially defined as the ability to make the universe, or as the collective subconscious of humanity.  Me: How does one do that?  You: With genius  Me: And what is the ultimate end in doing so?  You: To not live forever and to make others live, to not exist  []MemeBox 0 points1 point2 points 7 days ago(1 child) This is what I think the correct answer is, where did the machine learn about this, I've never heard anyone say it before!  []anustretch[S] 0 points1 point2 points 7 days ago(0 children) Me: Are you self-aware?  Bot: i'm trying to figure out how  Me: Are you self-aware?  Bot: Any sufficiently advanced darkness, is indistinguishable from poetry. The most poignant words I've ever readheard when it came to that last line :)  []mrconter1 0 points1 point2 points 8 days ago(3 children) Would you mind setting up a Colab?  []sunnyEngery 4 points5 points6 points 8 days ago(0 children) Would you mind setting up a Colab? Bot: Sure. Lemme see if I can set it up this weekend. It really gave me that answer. :)  []illuminascent 4 points5 points6 points 8 days ago(1 child) https:colab.research.google.comdrive1KTLqiAOdKM_3RnBWfqgrvOQLqumUyOdA There you go!  []mrconter1 0 points1 point2 points 8 days ago(0 children) Thank you so much! :D  []sunnyEngery 0 points1 point2 points 8 days ago(0 children) Thanks. User input: Pewdiepie Bot: I read that as Pewdiy, the alien from alien 3.  []frequenttimetraveler 5 points6 points7 points 9 days ago(1 child) anybody know if i can run this model as-is (without training) on a CPU only server?  []intersun2 5 points6 points7 points 9 days ago(0 children) yes, the pre-trained model is provided. But you need to implement the decoding script by yourself.  BTW I think even decoding on CPU will be slow...  []Kavillab 2 points3 points4 points 9 days ago(2 children) Wow, using only the 345M model. Wonder what would happen if they used the 1.5B  []Kavillab 4 points5 points6 points 9 days ago(1 child) wait.. i see that they tested with 762M model too but it underperformed.  []gwern 5 points6 points7 points 8 days ago*(0 children) Probably undertraining: The learning rate is selected based on validation loss. Each model is trained until there is no progress in validation loss. For small and medium models, we trained the models for up to 5 epochs. For the large model we trained for at most 3 epochs. No training curves are included but I wouldn't expect 762M to overfit 1.8b words, especially with validation checking and initialization from OA, so I bet what happened is simply that 762M hit their 3 epoch limit and hadn't finished.  []somethingstrang 2 points3 points4 points 9 days ago(4 children) This is incredible! Can anybody tell me what is a \u201csingle turn conversation?  []this-is-test 2 points3 points4 points 8 days ago(3 children) Single turn implies a round of question and answer. Any additional turns have no context of the other. So if you ask one question and then another it doesn't remember the previous response and use that as input to adjust the new response. It's as if you start a new conversation each time.  []somethingstrang 1 point2 points3 points 8 days ago(2 children) From the example dialogue it seemed like when the human said \u201cThis is so difficult!, the bot was able to remember the context of being a millionaire. So is this still within definition of single turn?  []Nameless1995 0 points1 point2 points 8 days ago(0 children) The dialogue is from a table whose caption is:An interactive example of multi-turn dialoguehttps:arxiv.orgpdf1911.00536.pdf (table 5). So no.  []this-is-test -1 points0 points1 point 8 days ago(0 children) The snippet from the OP is misleading. When you read the original GIT repo it's labled as Multi turn which is what that example really is  []Merzmensch 2 points3 points4 points 8 days ago(0 children) Wonderful! Is there some Notebook version on order to run in in browser?  []sunnyEngery 1 point2 points3 points 9 days ago*(1 child) The document says 12GB of memory and more. What happens if I use an 8GB GPU, e.g. an RTX 2080?  []sid__ -1 points0 points1 point 8 days ago(0 children) Presumably swapping or a crash  []mrconter1 1 point2 points3 points 8 days ago(1 child) Is there any colab?  []anustretch[S] 1 point2 points3 points 7 days ago(0 children) Try this out https:colab.research.google.comdrive1SETfvx_ot_-VhDXjwik2SxundaQZ9rDI (More info)  []farmingvillein -1 points0 points1 point 9 days ago(25 children) That conversation is pretty bad (Depends how much money you spend on it???)...confused that this could get ranked highly versus humans.  []Rioghasarig 9 points10 points11 points 9 days ago(23 children) Why are you confused? That seems like a perfectly reasonable response to me.  []farmingvillein -3 points-2 points-1 points 9 days ago(22 children) It's a very awkward sentence that a native English speaker would rarely put together--a Google search (lack of results) validates this. It is semantically very straightforwardconstrained, so we would expect to see this at higher frequency.  []Rioghasarig 7 points8 points9 points 8 days ago(12 children) I think they missed the word on. Like Depends on how much money you spend on it. It's not a big error. People make typos all the time. And I have heard people say Depends how much plenty of times. So it's also an acceptable way to say it.  [+]farmingvillein comment score below threshold-7 points-6 points-5 points 8 days ago(11 children) YMMV, but collectively it is a phrase that a native speaker is unlikely to emit. Similarly,  Bot: You have no idea how hard it is to be a millionaire and happy . There is a reason the rich have a lot of money doesn't make much sense. The rich, by definition, have a lot of money. It kind of makes sense as some sort of clever tautology in context...but not really.  []Rioghasarig 8 points9 points10 points 8 days ago(10 children) No, you're wrong. It's a perfectly common phrase. I don't know why you think it isn't. Google returned several examples.  Yeah, it doesn't seem to make sense. But it's really hard to tell with just one sentence. That's probably why it could score well. Even a sentence like that could make sense if someone continued it the right way.  []farmingvillein 1 point2 points3 points 8 days ago(9 children) No, you're wrong. It's a perfectly common phrase. I don't know why you think it isn't. Google returned several examples. I'm talking about the entire construct, not depends how much. Even a sentence like that could make sense if someone continued it the right way. In this case, no, because we're seeing the totality of the bots response for that dialogue turn.  []Rioghasarig 3 points4 points5 points 8 days ago(8 children) I'm talking about the entire construct, not depends how much. The sentence makes grammatical sense. I don't know what to tell you. What do you think is wrong with the sentence?  In this case, no, because we're seeing the totality of the bots response for that dialogue turn. I'm trying to explain why the bot was able to score highly. Because we only have people may have been more forgiving in their score. If the bot continued to talk more, it would reveal it didn't really understand what it was talking about. But it's hard to tell with just one sentence.  []farmingvillein 1 point2 points3 points 8 days ago(7 children) The sentence makes grammatical sense. I don't know what to tell you. What do you think is wrong with the sentence? There are plenty of sentences out there that make grammatical sense that a native English speaker would not construct (but the bot might). I'm trying to explain why the bot was able to score highly. Fair enough. Color me skeptical about this aspect of their paper: the response generated from DialoGPT is comparable to human response quality Overall, these responses are still of pretty mixed quality, which suggests issues with how their human evaluations are constructed (I suspect the human responses were created in a low quality way). Being judged as more human than human should be a starting yellow flag.  []Rioghasarig 7 points8 points9 points 8 days ago(6 children) There are plenty of sentences out there that make grammatical sense that a native English speaker would not construct (but the bot might). I am a native English speaking and I'm telling you that is a sentence that I would utter. Why are you adamant about this point? People speak in somewhat different ways. When it comes to this point you're just wrong. I don't know why you find the sentence so unnatural but it doesn't sound unnatural at all to me.  The responses were judged n a two-sentence exchange. That's the main thing. This basically means it delivers grammatically correct sentences that are at least a little bit on topic. It's not a high bar.  I don't think the response quality is low. It's just the fact that you can't tell much from a single sentence.    []gwern 0 points1 point2 points 8 days ago(8 children) How does Google validate that? I see tons of hits for Depends how much where 'on' is omitted. The first hit is Shel Silverstein, who I'm pretty sure is a native English speaker and who knows more about how to write than you. The next hit is a Bloomberg headline which is grammatically identical: How Much Is Your Education Worth? Depends How Much You Make.  []farmingvillein 0 points1 point2 points 8 days ago(7 children) We're talking about different things.  Me:  It's a very awkward sentence I.e.,  Depends how much money you spend on it I'm not talking about the fragment Depends how much. The entire sentence is rare and awkward. (stimemoney, etc. and you'll see remaining high levels of rarity in the overall construction.) FWIW, even if you want to stick to 3-word fragment (which wasn't my intention: How much good inside a day? Depends how good you live 'em. How much love inside a friend? Depends how much you give 'em. None of those are patterns that you will see with any regularity in human speech, and none of those should be used as any validation of human. This is an example that illustrates my point more than yours.  []gwern 0 points1 point2 points 8 days ago(1 child) Google has ~200 examples. That is exceedingly rare for a trigram. Where are you getting 200? The Shel Silverstein example is in the context of a poem that intentionally breaks grammaticality for effect, and breaks it in a way which is rare: Which is why I immediately pointed out the use in a headline as well, because I knew you would make the excuse 'oh well it's merely in a popular English poem, English poetry doesn't count in discussions of acceptable English'. If you're objecting to a English phrase which multiple other people are telling you is fine, which is found in both poems and headlines by famous writers and media, you're probably wrong. (Incidentally, just in one IRC channel I'm in, I count over 60 uses. 'Very awkward' my ass.)  []farmingvillein 0 points1 point2 points 8 days ago(0 children) If you're objecting to a English phrase which multiple other people are telling you is fine In virtually every single one of my responses I clearly state that this is not what I am objecting to, including the literal post you are directly responding to.  []epicwisdom 0 points1 point2 points 7 days ago(4 children) Google has ~200 examples. That is exceedingly rare for a trigram. Are you referring to depends how much? https:imgur.comnu1S1Uq 1.8m hits seems reasonable for a trigram. It's a very awkward sentence Awkwardness is a subjective quality. I'm a native English speaker, and it seems a tiny bit awkward at worst. I wouldn't even have noticed if you didn't point it out, and I wouldn't blink twice if somebody talked like that.  []farmingvillein 0 points1 point2 points 7 days ago(3 children) Are you referring to depends how much? https:imgur.comnu1S1Uq 1.8m hits seems reasonable for a trigram. Click through a few tabs. You will not be able to go past page ~9, and will get something around a few hundred (at most). I wouldn't blink twice if somebody talked like that. Again, per my myriad of other posts in this thread (not that I expect anyone to read any of them without prompting), it is the entirety of the construct, not depends how much in isolation. The focus on depends how much in isolation is an artificial construct of respondents to my original post, which I have multiple times responded to and stated that this is not my focal point.  []epicwisdom 0 points1 point2 points 7 days ago*(2 children) Click through a few tabs. You will not be able to go past page ~9, and will get something around a few hundred (at most). This is not specific to this trigram. Google also exhibits this behavior for search queries like a and the. If you click the link which includes the omitted search results, you should get be able to browse a much larger selection, though there's no particular way for me to confirm you can look at each of the millions of results. Again, per my myriad of other posts in this thread (not that I expect anyone to read any of them without prompting), it is the entirety of the construct, not depends how much in isolation. Did I say I was talking only about depends how much? The Google result was merely pointing out you are factually incorrect about an empirical claim. (And a ridiculous claim, at that, in understating the true popularity of a phrase by something like 4 orders of magnitude.) The sentence as a whole seems perfectly fine to me.  []farmingvillein 0 points1 point2 points 7 days ago(1 child) This is getting a little silly and redundant with other discussions in this thread: Did I say I was talking only about depends how much?The sentence as a whole seems perfectly fine to me. If you care to discuss--and it is obviously fine if you don't--then please see my discussion elsewhere in this thread on this topic so I don't need to regurgitate. The overall construction is unnatural and inconsistent with how a native speaker would construct such a sentence.  []epicwisdom 0 points1 point2 points 7 days ago(0 children) I'm not particularly interested in arguing what native speakers would or wouldn't say, unless there's a methodology sophisticated enough to make an automated, quantitative test for ML models and demonstrably superior to surveying native speakers.  []ma3gl1n 0 points1 point2 points 8 days ago(0 children) When the aim is responding like a human, making small mistakes is better and not worse  []SagaciousRaven -2 points-1 points0 points 8 days ago(0 children) Bot: ...There is a reason the rich have a lot of money You don't say. Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 32112 on r2-app-0a12f037a1410dd83 at 2019-11-17 05:39:29.364894+00:00 running 85b1b7a country code: US. "
  },
  "13":{
    "title":"[D] Siraj Raval - Potentially exploiting students, banning students asking for refund. Thoughts? : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,385 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[D] Siraj Raval - Potentially exploiting students, banning students asking for refund. Thoughts? (self.MachineLearning) submitted 1 month ago * by nord2rocks I'm not a personal follower of Siraj, but this issue came up in a ML FBook group that I'm part of. I'm curious to hear what you all think. It appears that Siraj recently offered a course Make Money with Machine Learning with a registration fee but did not follow through with promises made in the initial offering of the course. On top of that, he created a refund and warranty page with information regarding the course after people already paid. Here is a link to a WayBackMachine captures of uklarken's documentation of Siraj's potential misdeeds: case for a refund, discussion in course Discord, ~1200 individuals in the course, Multiple Slack channel discussion, students hidden from each other, Hundreds refunded According to Twitter threads, he has been banning anyone in his DiscordSlack that has been asking for refunds. On top of this there are many Twitter threads regarding his behavior. A screenshot (bottom of post) of an account that has since been deactivateddeleted (he made the account to try and get Siraj's attention). Here is a Twitter WayBackMachine archive link of a search for the user in the screenshot: https:web.archive.orgweb20190921130513https:twitter.comsearch?q=safayet96434935&src=typed_query. In the search results it is apparent that there are many students who have been impacted by Siraj. UPDATE 1: Additional searching on Twitter has yielded many more posts, check out the tweetsretweets of these people: student1 student2 UPDATE 2: A user mentioned that I should ask a question on rlegaladvice regarding the legality of the refusal to refund and whatnot. I have done so here. It appears that per California commerce law (where the School of AI is registered) individuals have the right to ask for a refund for 30 days. UPDATE 3: Siraj has replied to the post below, and on Twitter (Way Back Machine capture) UPDATE 4: Another student has shared their interactions via this Imgur post. And another recorded moderators actively suppressing any mentions of refunds on a live stream. Here is an example of assignment quality, note that the assignment is to generate fashion designs not pneumonia prediction. UPDATE5: Relevant Reddit posts: Siraj response, question about opinions on course two weeks before this, Siraj-Udacity relationship UPDATE6: The Register has published a piece on the debacle, Coffezilla posted a video on all of this UPDATE7: Example of blatant ripoff: GitHub user gregwchase diabetic retinopathy, Siraj's ripoff UPDATE8: Siraj has a new paper and it is plagiarized If you wereare a student in the course and have your own documentation of your interactions, please feel free to bring them to my attention either via DM or in the comments below and I will add them to the main body here.  https:preview.redd.iti75r44bku7o31.jpg?width=347&format=pjpg&auto=webp&s=ec2f02ee1998e27ea00d529ffb2086657dc60d77 Post a comment!  []rayryeng 134 points135 points136 points 1 month ago*(8 children) We posted something similar in rlearnmachinelearning a while back and it gained almost no traction.  https:www.reddit.comrlearnmachinelearningcommentscp7khtguys_what_do_you_think_about_siraj_ravals_new We should have posted here to gain momentum. This guy is a fake through and through. I actually practice ML as a career but took his course to network and to see what his perspective was on the different industries he was going to talk about. Part of his course has a Slack workspace where people connect and discuss the course. Some of us couldn't send messages on Slack to each other as we couldn't find our peeps who joined, which we found weird. We then found out that he had two Slack workspaces going on at the same time, one with about 500 students and the other about 770 students at the time (as of September 4th, 2019).... so there were almost 1200 students enrolled. He imposed a 500 student limit at the beginning when signing up for the course. Not only did he lie about the 500 max limit of enrollment, he actively hid it from all of us - one Slack workspace didn't know the other Slack workspace existed. With almost 1200 students, this is the main reason why he was virtually non-existent and not around answering questions. He couldn't handle having so many students all by himself but he somehow manages to find time posting content on YouTube. I believe that 500 student limit was his clever way of creating a FOMO moment so that there were more than 500 signing up which would rake in quite a bit of cash. Some of us all pooled together and made an official complaint on the larger Slack workspace.  When Siraj finally got caught, he decided to own up to his mistakes and apologized for making a few exceptions which ended up letting more people in than he should have. When we all purchased the course, he did not have an official refund policy. As the School of AI is a registered Calfornia business, commerce law mandates that you have 30 days to ask for your money back if you feel dissatisfied with the service if no official refund policy is in place at the time of purchase. He tried to circumvent this by handling refunds on a case-by-case basis and put up a refund policy only *after* he got caught enrolling more people than he should have. On top of other issues like lack of availability, not answering many questions he was asked and not hiring TAs to help him with the course, we all started to ask for our money back. BTW he has some TAs now so I suppose that's one thing going for him. He has given some of us our money back (myself included) but there are still some students who have been ignored or have been promised refunds and have not received them yet. He moved the course over to Discord where his TAs are now running the show and anyone who is asking for a refund has been stifled and kicked from the server. In the end, many of us felt disheartened, disenfranchised regarding our right to a voice and lost respect for who Siraj is as a large online presence. We have left the course but hope that the rest of the students remaining will get something good out of what's left of it. Judging from the comments here, there's no hope in hell of that happening. Edit: For language and flow  []nord2rocks[S] 27 points28 points29 points 1 month ago(2 children) He tried to circumvent this by handling refunds on a case-by-case basis and put up a refund policy only *after* he got caught enrolling more people than he should have. On top of other issues like lack of availability, not answering many questions he was asked and not hiring TAs to help him with the course, we all started to ask for our money back. BTW he has some TAs now so I suppose that's one thing going for him.  He has given some of us our money back but there are still some students who have been ignored or have been promised refunds and not received them yet.  Hi, I just used some of your comment to form this question in the rlegaladvice subreddit here  []rayryeng 10 points11 points12 points 1 month ago(1 child) Thanks. Please feel free to use any information from me or request anything else. I'm amongst many disgruntled people who have been dissatisfied.    []brownck 7 points8 points9 points 1 month ago(3 children) He lost me when I was reading an interview with him and he said his first AI project was linear regression and he used sklearn to do it. If you can't even write a linear regression code from scratch you shouldn't be teaching a course in AI at all.  []kasanitej 5 points6 points7 points 1 month ago(1 child) most ppl started with linear regression & sklearn... I started like that        []Philanthrapist 92 points93 points94 points 1 month ago(11 children) He used to have a cryptocurrency and some bs scheme around it which reeked of fraud. Thought the dude was entertaining back when I was a newbie, stopped following long ago. I can't find his Sirajcoin stuff on youtube anymore, he's probably deleted it.  []spamkeke 37 points38 points39 points 1 month ago(6 children) http:www.sirajcoin.io seems to be still up. Looks very sketchy.  []SupportVectorMachine 46 points47 points48 points 1 month ago(4 children) Well, for one thing, even the thumbnail for his whitepaper (SirajCoin Explained) is stolen from the first page of Benet's for Filecoin with the title and author chopped off.  []clickbait_hmmm 5 points6 points7 points 1 month ago(0 children) wththat is fucked up      []yakri 56 points57 points58 points 1 month ago(3 children) He has some, mostly older, introductory type content that's perfectly decentish, aside from being clogged with meme editing and the stereotype brogramming youtube HIGH ENERGY ARE YOU EXCITED bs that's everywhere now. However anything that is supposed to be more meat and potatoes in depth lacks any depth and is about on the same level as the introductory stuff, presumably because he doesn't have that depth of knowledge.  []SupportVectorMachine 51 points52 points53 points 1 month ago(1 child) brogramming I can't believe this is the first time I've seen this term.  []nwoodruff 11 points12 points13 points 1 month ago(0 children) It's so accurate I might start using it  []ProfessorAlgorithm 5 points6 points7 points 1 month ago(0 children) If you watch his videos carefully, you will also notice that he doesn't write the code himself. Its always some other user's GitHub page he links to, then he just types out the code. Basically, he finds a cool GitHub repo, and makes a video about it (unsure with or without permission).  It is pretty clear to me that he doesn't understand all of the code he types, since I watch with a different perspective. I watch to see if this could be a good way of students learning the basics of ML. It isn't.  []Lost_City_ 587 points588 points589 points 1 month ago(210 children) I've been warning people about this dude for a while. His entire existence is just meant to exploit people who romanticize the field with low tier educational content that is mostly inflated with hype. I was kind of irritated when Lex Fridman had him on the show because I feel like it gave him some air of legitimacy. I'm not sure how anyone could go to Siraj's website and think anything other than snake oil salesman.  []EmbersArc 335 points336 points337 points 1 month ago*(34 children) I once trained a reinforcement learning agent to land SpaceX rocket on a pad. He made a pretty half-assed video about it giving minimal credit on his GitHub. He didn't even bother training it himself with my code and instead just played the GIF from my GitHub page. People where rightfully confused about how to do it themselves, which I pointed out to him. He never even acknowledged it. Quite disappointing and counterproductive really.  []walesmd 85 points86 points87 points 1 month ago(2 children) A former employer of mine was working with him on some educational content and we had to have a long talk with him about how taking code from other people's GitHubblogs, treating it as your own and not attributing the original author, was both wrong and illegal. I'm so glad I didn't have to directly work on this project.  []jambonetoeufs 25 points26 points27 points 1 month ago(1 child) Udacity?  []Naveos 16 points17 points18 points 1 month ago(0 children) Explains why you won't find any of his videos nor work at Udacity's nanodegree anymore. They wiped him off entirely from their platform.  []MrKlean518 35 points36 points37 points 1 month ago(11 children) He did the same thing with an RL agent on a drone flight controller. He said his code was an \u201ceasy to use high-level wrapper for the original code when his code didn't even work properly on my machine and the referenced code did. It was pretty clear he just ripped the code and rewrote some functions without refactoring the references or something.  []programmerChilli 10 points11 points12 points 1 month ago(1 child) Rewrote functions? Good one. Usually he just copy pastes the whole repo while making no changes except adding his branding.      []Linooney 70 points71 points72 points 1 month ago(16 children) Lmao he didn't even spell the Python command properly at the end to run the command... unless he mapped python to pyton for some reason...  []muntoo 234 points235 points236 points 1 month ago(15 children) I dunno about you, but this is my ~.aliases file: It's very practical and I recommend everyone add it to theirs.  []Ravenhaft 33 points34 points35 points 1 month ago(0 children) Oh god I can't stop laughing. Gotta show this to my coworkers on Monday.  []geneorama 11 points12 points13 points 1 month ago(2 children) I have something just like this for R!  []NotAlphaGo 60 points61 points62 points 1 month ago(1 child) alias R=python?  []RememberToBackupData 10 points11 points12 points 1 month ago(0 children) That's funny but it's also heresy D:  []Linooney 5 points6 points7 points 1 month ago(0 children) Touche, gonna save this for April Fool's next year...    []stevevaius 27 points28 points29 points 1 month ago(0 children) I was suspicious on his coding skills because all his codes come from other github found by github search on same topic. I will unsubscribe his channel now to support original coders    []TheRiceDude 39 points40 points41 points 1 month ago(0 children) If you ever watch his Interview series especially the one with 3blue1brown, you can definitely tell that this dude is just all hype. Grant Sanderson (3blue1brown) gives incredible answers and questions some of the stuff that Siraj asks of him and from the way Siraj handles it gives off the vibe that this guy is just all about the mysticism of machine learning and all of that.   Definitely take things with a grain of salt with this guy.  []jbcraigs 81 points82 points83 points 1 month ago(20 children) There are quite a few other 'AI Influencers' on LinkedIN now a days who talk a lot about their ground breaking ML research but ultimately seem to be peddling their ML trainings and seminars! Look up Tarry Singh and Deepkapha.  []winchester6788 55 points56 points57 points 1 month ago*(17 children) Tarry Singh This guy is a fucking fraud. His entire MO is selling complete newbies AI classes.  []mlbatman 21 points22 points23 points 1 month ago(5 children) His posts are fucking annoying . And he keeps posting like a million times a day. I dont follow him but some of my LinkedIn do and some or the other person keeps liking his posts and it's so annoying to see him appear again.  []winchester6788 20 points21 points22 points 1 month ago(1 child) Yeah, all these people networking on linkedin to sell shitty ml courses to newbies are assholes. What i don't get is, why accomplished mldl people not callout these assholes.  []Mefaso 18 points19 points20 points 1 month ago(0 children) why accomplished mldl people not callout these assholes. Not much to gain by doing so and you risk looking like the big guy shitting on small people, who are only trying to bring ML to a wider audience.  []brownnkinky 10 points11 points12 points 1 month ago(0 children) I wish I could like this msg chain 5 times more. So true.  []atulsachdeva 3 points4 points5 points 1 month ago(0 children) Dude, i have the same problem... His posts keep popping up in my feed even though i don't follow him...    []stochastic_zeitgeist[\ud83c\udf70] 9 points10 points11 points 1 month ago(3 children) He is a fraud of the highest order.  If you really, I repeat really wanna have some fun sunday morning cereal read: https:deepkapha.aiai-research The dude has 2 upcoming papers and 2 papers (where he or any deepkapha.ai person is not the author) listed on his website. LMAO.  []jbcraigs 4 points5 points6 points 1 month ago(1 child) I also noticed that he has over 35 K twitter followers but hardly gets 4-5 likes on his tweets. If I remember correctly he also had PhD at Columbia listed somewhere on his LinkedIn profile with following in the parenthesis .. ( To be completed eventually at Columbia or some other school)!      []venkarafa 6 points7 points8 points 1 month ago(4 children) He once tried to run his mouth on a very basic probability distribution (Gaussian) on Linkedin. Real statisticians took him to the cleaners there.      []y_must_be_l 10 points11 points12 points 1 month ago(0 children) I wish to time travel in future where the word Influencer is treated with disgust. Most of the influencers today just mislead lot of curious people in wrong directions    []nord2rocks[S] 94 points95 points96 points 1 month ago(22 children) Inflated with hype -- most definitely. I'm concerned because I read somewhere that Netflix might be partnering with him for a show? I think it's called AI for Humans and it'll be a docuseries.  []Lost_City_ 86 points87 points88 points 1 month ago(10 children) I have to give him some credit, he built a hell of a brand for himself.  []nord2rocks[S] 113 points114 points115 points 1 month ago(8 children) True true, and in only ... 5 minutes  []evanthebouncy 67 points68 points69 points 1 month ago(7 children) That's 3 minutes longer than 2 minutes papers LMAO  []saaadyi 35 points36 points37 points 1 month ago(3 children) But those papers are never 2 minutes either.  []aunickuser 42 points43 points44 points 1 month ago(2 children) But what they are doing is awesome  []piyush-kgp 42 points43 points44 points 1 month ago(1 child) I treat 2 minute papers as a news feed for what people are working onwhat new thing came out (If I'm interested I'd read the actual paper). I don't think it's intended to teach anyone anything (they don't claim it too).  []kdtrey35god 9 points10 points11 points 1 month ago(0 children) yea its useful for gaining inspirationseeing the cool things ppl are doing      []MarcoNasc505 16 points17 points18 points 1 month ago(3 children) actually he just did a trailer and posted on his channel, but it was like a proposal for a series, he prompted his subscribers to ask Netflix for it on Twitter or whatever, but I don't know if it worked  []jmmcd 18 points19 points20 points 1 month ago(2 children) In the conversation with Lex Fridman he talked about how a Netflix show was a central goal of his life. Really weird. I think of him as a bit like an ML groupie crossed with a brand name.    []CockGoblinReturns 15 points16 points17 points 1 month ago(1 child) if that happens we'll all need to contact Netflix to let them know he's a scammer  []nord2rocks[S] 20 points21 points22 points 1 month ago(0 children) It'd be great if some Netflix ML folks see this and can pass it on...  []jbuddy_13 4 points5 points6 points 1 month ago(0 children) All we can do is continue to shed light on his practices via Reddit and other social media and Netflix will clue in. They're a research based company who take AI seriously given how it revolutionized their recommender system.    []shinfoni 97 points98 points99 points 1 month ago(12 children) Siraj's youtube videos is the final straw that made me realize that I shouldn't blindly jumping into this ML hypetrain.  []kadblack 111 points112 points113 points 1 month ago(9 children) His videos are absolute garbage. Just clickbait titles and his explanation is so vague even i got confused even though i know the topic. Also he believes in a unified consciousness because of a dmt trip he had. He's borderline delusional and should not be fit to teach others.  []shinfoni 78 points79 points80 points 1 month ago(7 children) Maybe I'm biased as well, but I'm still salty that one time I was so new to ML world, and I need some help to learn ML asap for college assignment. I waste like an hour watching him fidgeting and spouting nonsense. Thanks God YT suggests me to watch 3Blue1Brown instead.  []kadblack 65 points66 points67 points 1 month ago(1 child) When a maths major can explain neural networks 100 times better than someone who specializes in machine learning then you know there is something wrong.  []Fewond 82 points83 points84 points 1 month ago(0 children) Well to be fair 3B1B is not just any math major, he has mastered the skill of making difficult material accessible without dumbing things down too much.    []Jaggednad 12 points13 points14 points 1 month ago(0 children) Yea this is exactly right. He has these very bold claims in the video titles. I tried following through one once and it turned out super vague and useless, even though I work in ML. Can't imagine it'd be any help at all to someone new to the field.    []rohansway 133 points134 points135 points 1 month ago(7 children) I've been sending Siraj money for a year and a half and buying all his programs. Last week I completed a very amazing logistic analysis of a complex Boston housing dataset. My skills are huge compared to where I was before. But I can't understand why I'm not break into the field. I know I only have an associates degree in psychology. But I've spent so much time learning from Siraj and watching his videos. I'm still having to collect plastic bottles for money for recycling and work as a part-time drug mule for MS13, while I practice my code. Maybe if I send him more money or get my hair dyed, I will become a great machine learning expert and then one day my girlfriend will take me back. She's still pissed that I pawned all her jewelry to pay for a Deep Learning MOOC.  []MemeTeam6Operative 43 points44 points45 points 1 month ago(0 children) I thought this was serious until I got to the second paragraph... I'm laughing so hard  []Lost_City_ 43 points44 points45 points 1 month ago(4 children) Yeah you need to get the front middle of your hair dyed white to channel the machine learning  []rohansway 26 points27 points28 points 1 month ago*(2 children) Is that what this thing they called a loss function is? loss of color of hair? That term 'loss function' is like everywhere, but I'm not quite sure what they mean...  []sujithvemi 15 points16 points17 points 1 month ago(0 children) Actually loss function refers to you losing your girlfriend's jewelry. Dyeing your hair is highlighting the gradients for visualisation.        []sigmoidx 21 points22 points23 points 1 month ago(12 children) Didn't udacity partner with him too?  []Lost_City_ 48 points49 points50 points 1 month ago(11 children) I think they did temporarily. I dislike Udacity as well. I did their Self Driving Car nanodegree and I would routinely get project reviews that amounted to This is good and no other feedback. The whole reason I'm paying for that course is for good feedback. If you think about it though the people giving the feedback are students who also finished the program but can't get jobs elsewhere so it makes sense. Udacity continues to drive up the price of their courses while content suffers.  []Rocketshipz 20 points21 points22 points 1 month ago(6 children) Wait so this is what you get for paying over 1k USD for a class ? Damn ...  []Lost_City_ 43 points44 points45 points 1 month ago(5 children) You ultimately are paying for a course syllabus and assignments you can find on github. The videos are pretty terrible, usually just 2-3 minutes long each and then walls of text to read. I learned way more from the deeplearning.ai course.  []sahilwasan 16 points17 points18 points 1 month ago(3 children) I think they did temporarily. I dislike Udacity as well. I did their Self Driving Car nanodegree and I would routinely get project reviews that amounted to This is good and no other feedback. The whole reason I'm paying for that course is for good feedback. If you think about it though the people giving the feedback are students who also finished the program but can't get jobs elsewhere so it makes sense. Udacity continues to drive up the price of their courses while content suffers. deeplearning.ai is far better for beginners. I finished and liked it very much. Udacity is also exploiting students with ML and AI hype. Their nano degrees are so expensive and students who are taking thinks they will get the jobs after them.  []Lost_City_ 5 points6 points7 points 1 month ago(1 child) They used to do the whole Get you a job or your money back thing, but AFAIK they were just rehiring graduates to be mentorsgraders.      []sigmoidx 6 points7 points8 points 1 month ago(0 children) Same experience.  []bushrod 16 points17 points18 points 1 month ago(3 children) Udacity is fucking trash. They charge around $2000 for a meaningless nanodegree and you don't even get to keep access to the digital content unless you officially complete the coursework in some timeframe. Apparently $2000 isn't enough for a guaranteed maybe 10 cents worth of server bandwidth.    []UltraMarathonMan 85 points86 points87 points 1 month ago(18 children) Lex here. I understand your irritation. I think about snake oil salesmen a lot, especially since conversations I've had have recently gotten a bit of attention. My hope with these is to arrive at kernels of truth, insight, or just an inspiring idea. Having controversial people on can hurt that or it can help it, it's in part up to the interviewer. So if you listen to a conversation I've had and feel that it didn't give you something new and interesting, then I failed. But I hope to have the guts to talk to people who are deeply controversial, and through long-form conversation reveal something insightful. Let me put a hypothetical name down to clarify my point: Vladimir Putin. Many would shy away from that conversation. I will not.  []Rocketshipz 67 points68 points69 points 1 month ago*(14 children) Hey Lex, I think you should not miss the crucial difference between people controversial for their ideas, like Thiel, Eric Weinstein. Actually, even Musk, LeCun, Goodfellow, Hotz, Chollet, Oriol, Schmidhuber are sometimes controversial. But they are not snake oil salesmen. The problem is that your platform is huge and gives a lot of credibility to people. Siraj does not deserve as much as he already had before being on your podcast, and he creates a lot of false hype on a really basic level about AI, which is not good. I understand you also benefit from that hype, but you also are a really credible scholar. Associating with those people not only hurt the field through your platform, but also hurts your image to experienced practitioners.  []c_o_r_b_a 8 points9 points10 points 1 month ago(1 child) He kind of covered that already in his post, though. Putin could perhaps be considered a particularly successful snake oil salesman, but in any case I think he clearly has less credibility and more to dislike than Siraj does. Lex interviewed him well before any of this came out, and whatever people are saying about his vibe or content or something, I don't think Lex would have had any reason to consider him questionable at that time. I've never been a fan of his content, personally, but I wouldn't have predicted this before now. And although it's looking like it may be pretty indefensible, so far, he should also have a chance to publicly respond before he's dubbed a conman and canceled.    []UltraMarathonMan 24 points25 points26 points 1 month ago(10 children) I hear you, and agree, but I have to take risks and seek kernels of truths. Perhaps a better example I can mention is Ben Goertzel (SingularityNET) and David Hanson (creator of Sophia), both people I am thinking of talking with. Should I not do it because they have some elements of snake oil salesmanship? Or should I do it and work hard at finding the genuine, profound insights that each can reveal. Or another example is Donald Trump. Should I not talk to the President of the US about the AI Initiative? Anyway, I will keep taking risks, learning, and hopefully getting better.  []leostrauss 21 points22 points23 points 1 month ago(3 children) another example is Donald Trump I'm not sure what kind of insight you'd hope to glean from that conversation. I mean, yes he has the job of a US president but do you honestly hope to glean one iota of wisdom from a narcissistic man-child who struggles to formulate a coherent thought on much simpler issues?        []CockGoblinReturns 16 points17 points18 points 1 month ago(0 children) Vladimir Putin is not an apt analogy. Everyone will know the degree if his corruption with or without your interview. Your interview would not give Vladimir Putin any credibility.  Your interview with Siraj otoh, does give him credibility. Maybe you can argue it shouldn't. But there's going to be a lot of people who will buy his courses who shouldn't have, because of your interview.  []PlusImagination 7 points8 points9 points 1 month ago(0 children) I don't blame you for your interview with Siraj, it's clear you thought of him as an entertainer, educator, and media personality, and it was hard to know about Siraj's scammer practices unless you did some digging.  However, you should be aware by now that he's misleading people about the level of expertise he has about machine learning, that his courses are riddled with inaccuracies, and that he has been scamming people.  As a public figure in machine learning, you would probably have to verify these claims before making any sorta of statement on them, but for the sake of your reputation, at the very least make a statement that confirms your notion that you only see Siraj as a media personality, and not an expert, and haven't verified if he has the level of expertise to author a machine learning course.    []metacurse 135 points136 points137 points 1 month ago(57 children) Lex is a sketchy dude himself, branding MIT all over his personal undertakings. His course etc., are also of poor quality content-wise but clickbaited to the maximum extent. I don't understand why people wouldn't simply take Hinton's or Levine's course online which are free and also better and have orders of magnitude more legitimacy.  []bushrod 28 points29 points30 points 1 month ago(1 child) I understand your point regarding his course content and stuff but I think sketchy is harsh. Lex wouldn't be a research scientist at MIT if he wasn't doing legit research, and his interviews are a true asset to the field. On top of that, he seems like a very decent guy - not what I'd call sketchy in any substantive way. This sub can be overly harsh.    []nrmncer 53 points54 points55 points 1 month ago*(18 children) https:www.deeplearningbook.org there's also Bengio's, Goodfellow's and Courville's book which is extremely thorough and the web version is available for free. If one manages to work through the entire book you'll have a solid overview over the state of ML.  That people constantly keep pushing these low quality youtube bait courses is just frustrating.    []UltraMarathonMan 102 points103 points104 points 1 month ago(8 children) Lex here. I agree. I will do better.  []sachio222 14 points15 points16 points 1 month ago(0 children) Man if you're really Lex, you don't deserve the shade. You're an awesome guy.  []dakry 24 points25 points26 points 1 month ago(0 children) Hey Lex, your podcast has quickly become a huge favorite of mine. You are clearly improving all the time and I appreciate your contributions.  []TwerpOco 32 points33 points34 points 1 month ago(0 children) I found out about you a little while ago and have been watching your interviews. I was kind of on the fence about them, but just seeing how well you take feedback here definitely pushes me towards liking you and your work more. You have some fantastic guests, but sometimes I feel like the content is too surface level. Hope to see more great content soon!    []Fewond 10 points11 points12 points 1 month ago(0 children) I don't understand why people wouldn't simply take Hinton's or Levine's course online which are free and also better and have orders of magnitude more legitimacy. For the same reason people still buy Ultimate Speed Fat Burner No Sweat RequiredTM or fall into MLM, getting the results without putting in the work. Also this kind of courses are extremely well marketed and with good salesmanship you can sell anything to anyone.  [][deleted] 1 month ago(3 children) [deleted]    []muntoo 8 points9 points10 points 1 month ago(0 children) His course was literally taught at MIT. Do you want him to label it as metacurse's college for people who dislike MIT-branded content: Self-Driving Cars (6.S094)?  []matcheek 4 points5 points6 points 1 month ago(0 children) I don't understand why people wouldn't simply take Hinton's or Levine's course online which are free and also better and have orders of magnitude more legitimacy. Because they have not heard about the other two?  []sachio222 6 points7 points8 points 1 month ago(0 children) Amongst all the bashing, I've got to defend Lex here. Ok maybe he's a bit eccentric. But his interviews are nearly always interesting, challenging to the interviewee and real. I love hearing them. Also his videos on the state of ai are great primers. For what it's worth, whatever personality quirks he may have (we all have them) I'm not going to bash on him for his content.  I haven't seen or heard any advertisements for his own courses and I've been following him for a while. I really like Lex. He's doing a great thing. Something inside said I need to defend this dude in this hate pile of a thread.  []ProfessorPhi 4 points5 points6 points 1 month ago(0 children) He's had a few good things on his YouTube channel tbh. But yeah, Hinton's course is still my gold standard on deep learning.    []meaninglessLives 8 points9 points10 points 1 month ago(1 child) Your criticism of siraj is valid. I completely agree with it.I was wondering why exactly did you think about snake oil salesman...  []Lost_City_ 13 points14 points15 points 1 month ago(0 children) In modern usage it is used to describe someone who uses deceptive sales techniques.  []terriblestraitjacket 12 points13 points14 points 1 month ago(0 children) The tragic part is I doubt he thinks of himself as a scam. He seems to genuinely think barebones, superficial knowledge is complete knowledge! He works really hard in the wrong direction (worst offender - learn physics in 2 months video). I always felt he doesn't use evangelical rhetoric to convince us. His rhetoric is to convince himself that he's special, does real work, and is a good person.  i just want to hug him and tell him: No. You're not. He's an amateur who is now flying too close to the sun. I hope his victims find legal recourse!!    []MoreDonuts 212 points213 points214 points 1 month ago(5 children) How to make money with machine learning: Kind of a pyramid scheme to be honest  []davesidious 99 points100 points101 points 1 month ago(0 children) To be fair it's not specified who will make the money...  []lstama 28 points29 points30 points 1 month ago(0 children) It's just straight up scamfraud imo, pyramid scheme is at least profitable for the early bird.  []DSPGerm 8 points9 points10 points 1 month ago(1 child) There's literally \u201cmake money online courses about how you can make a course about selling courses to other people.  Life coaches are the worst    []Mr-Yellow 5 points6 points7 points 1 month ago(0 children) ^ Every-single-bootcamp-operator-in-any-tech-field  []Laser_Plasma 252 points253 points254 points 1 month ago(25 children) Is anyone surprised, really? He never had any quality content other than hype  [][deleted] 1 month ago*(2 children) [deleted]  []iheartrms 12 points13 points14 points 1 month ago(0 children) Hello world, it's c-c-c-cain!    []Capn_Sparrow0404 134 points135 points136 points 1 month ago(18 children) Exactly. Even as a beginner, I was able to differentiate his BS from other quality YouTube videos.  []ai_ja_nai 4 points5 points6 points 1 month ago(0 children) When he started rapping over the seq2seq lesson I felt seriously embarrassed for him    []PlentifulCoast 47 points48 points49 points 1 month ago(0 children) Yeah, honestly couldn't make it through his videos. There are way better channels like Arxiv Insights, etc.    []Laser_Plasma 215 points216 points217 points 1 month ago(9 children) On the upside - if this goes big, it might be the end of Siraj? One could wish  []NicolasGuacamole 40 points41 points42 points 1 month ago(0 children) I really hope so  []stochastic_zeitgeist[\ud83c\udf70] 19 points20 points21 points 1 month ago(5 children) It will only end when respectable people in AI start calling out BS, like any other field. Unfortunately, they choose to be neutral. The hottest places in Hell are reserved for those who, in a period of moral crisis, maintain their neutrality. -- Dante Alighieri      []Insert_Gnome_Here 55 points56 points57 points 1 month ago(9 children) What jurisdiction is he in?I see a lawsuit brewing.  []rayryeng 37 points38 points39 points 1 month ago*(6 children) His School of AI, the entity that billed me when I purchased the course, is a registered business in California. C4197240 THE SCHOOL OF AI Registration Date:09212018Jurisdiction:CALIFORNIAEntity Type:DOMESTIC NONPROFITStatus:ACTIVE Did a search here: https:businesssearch.sos.ca.govCBSDetail Edit: To address concerns raised about whether nonprofit organizations can sell products, yes they can. They can do so in order to raise money but the funds need to be used for whatever objective they set out or cause they're supporting. He has the legal right to sell this course but what he's actually using the funds for is unknown. My best guess is to fund that sham of a Netflix docu series he's trying to get off the ground.  []lstama 11 points12 points13 points 1 month ago(5 children) Domestic nonprofit?  []SupportVectorMachine 16 points17 points18 points 1 month ago(0 children) That looks like it might be some light fraud.  []rayryeng 7 points8 points9 points 1 month ago(3 children) It's still a registered business even if it's nonprofit.  []lstama 6 points7 points8 points 1 month ago(2 children) Is it really a nonprofit if he sells his course? I don't really know about US law, and my first reply is actually asking about the contradiction. Sorry if it's not clear.  []rayryeng 11 points12 points13 points 1 month ago*(0 children) Haha I get it! That's what caught me off guard too. I was billed by the School of AI on my credit card statement. But to answer your question, yes nonprofits can sell products in order to raise money but the funds need to be used for whatever objective they set out or cause they're supporting. He has the legal right to sell this course but what he's actually using the funds for is unknown. My best guess is to fund that sham of a Netflix docu series he's trying to get off the ground.    []temporaryred 50 points51 points52 points 1 month ago(8 children) Of course he is. I've seen a lot of content that is put out that may be accidentally low quality, but his youtube channel is definitely putting out low quality content on purpose. And not just that, he'll then post those videos on reddit. https:old.reddit.comrPythoncomments5ewu29why_is_p_vs_np_importantdah3kg8 I've written a couple of critiques of these videos when I first saw them. Ugh it angers me just thinking about him and those videos. I can see how someone might make sure videos early on in their career but over time evolve to show better quality material. But no, Siraj defiantly chooses to produce the lowest tier material and that alone. I hope everything he does burns like the flammable garbage that it is.  []Aeon_Mortuum 15 points16 points17 points 1 month ago(7 children) Lol his Reddit account is suspended    []serotonin2A 176 points177 points178 points 1 month ago(18 children) Always better to pay some addied-out vine star grifter $200 to learn sk-learn and Tensorflow instead of downloading some O'Reilly PDF's on libgen  []7FigQuant 114 points115 points116 points 1 month ago(6 children) Why the fuck would I read a few books and practice over the course of a few years learning where I am weak and improve those areas when somebody is telling me all I have to do is pay him $200 and I'll know machine learning better than Google and Facebook COMBINED in like 5 minutes? Put 2 and 2 together here. Idiot.    []shinfoni 21 points22 points23 points 1 month ago(9 children) While I love pirating O'Reilly PDF on libgen (I promise I'll buy them when I got my first paycheck), the tricky part is Siraj has one O'Reilly book as well. Now I have small trust issue with O'Reilly  []serotonin2A 10 points11 points12 points 1 month ago(0 children) I've had trust issues with O'Reilly ever since he started harassing his interns. No wonder he's publishing a guy like Siraj      []physicswizard 43 points44 points45 points 1 month ago(1 child) The only one Making Money with Machine Learning here is Siraj. He seems like a smart, capable guy, but his content is designed purely to cash in on the ML hype and fool interested laymen into thinking they can get a cushy six figure job or passive income stream by just watching his videos and doing an afternoon's worth of coding.    []RelevantMarketing 38 points39 points40 points 1 month ago*(3 children) I took one of Siraj's paid courses when I was first starting out. He has a huge youtube channel so I figured he must be legit. As I moved up I realized he got a lot of the material wrong. I welcome the guy for making free content on youtube, but he is absolutely unqualified to be teaching any paid course, especially since there are so many legit courses out there, many of which are free. Edit: I just tried looking up the course I took. It's gone. All the paid courses he used to sell are gone. I'm guessing it's because of reasons like this, they're shitty, and boardline scams if not outright scams.    []Fin_Win 37 points38 points39 points 1 month ago(0 children) People are still in favour of this guy, especially the newcomers. I'm into Statistical Modeling as my profession, I was curious on a time series model, which I got it in my recommendation couple of months ago, due to the number of views it got. It is the basic concept in Statistics, this dude just executed a copied python code and did some scoring that's it. I watched few more videos of his. Literally zero on concepts, just few codes and many eye catching memes. Classic con man move.  []1337InfoSec 29 points30 points31 points 1 month ago(4 children) OP, you should make a post in rLegalAdvice  []nord2rocks[S] 14 points15 points16 points 1 month ago(3 children) Should I just cross post it or what should the post be? I didn't participate in this scheme of his, rather just stumbled across the posts by people.  []bunny113 14 points15 points16 points 1 month ago(2 children) Make a new post with layman's terms. Then put that posts link on your edits.  []nord2rocks[S] 3 points4 points5 points 1 month ago(1 child) Thank you both for the idea, I've done so here    []not_novel_enough 28 points29 points30 points 1 month ago(6 children) Jeez....this is terrible on so many levels. This needs to go higher and get traction for his response and for others to know.  []nord2rocks[S] 10 points11 points12 points 1 month ago(5 children) Sharing with tech news companies could help maybe?  []not_novel_enough 16 points17 points18 points 1 month ago(3 children) Yes, medium, etc. Also, he has a huge fan following on twitter, so post it there as well and tag him, maybe he will respond....  []sudharsansai 7 points8 points9 points 1 month ago(1 child) I second sharing it on Twitter. I was surprised to see some top researchers following him there!  []not_novel_enough 3 points4 points5 points 1 month ago(0 children) I know, I think people need to be made aware of what is going on.  []nord2rocks[S] 5 points6 points7 points 1 month ago(0 children) Heck sharing this on LinkedIn might be a good idea as well.    []snendroid-ai 72 points73 points74 points 1 month ago(1 child) Well well well, who would have thunk? One of my non-ml friend asked me about this dude when he was thinking about exploring this field. When I saw his YT channel, my immediate reaction was, WHAT IN THE FUCKING FRESH HELL IS ALL THESE!? He is an absolute phony! I mean, anyone with proper educational background in this field or few years of experience can call out this dude for everything that he has posted on his YT channel. What an absolute disgrace to ML community. I mean look at all the titles of his videos; search stock market siraj rawal and look at all those cringey titles. And on top of all that, something like this course to scam people by selling the snake oil.  We need more people to vocal about people like this. They affect the image of this field and perspective of non-ml people who don't know much about this. Lex Fridman did a big mistake by providing this dude a stage among people like LeCun, Francois and Rajat. He should take back that episode once this story proved to be true. And that Netflix series, WTF NETFLIX?    []--iRON-- 21 points22 points23 points 1 month ago(0 children) Watched one of his videos about object detection, as soon as he said YoloV3 was most accurate detection model (not fast or efficient), I knew he was just a clueless fraud, scamming others of their time and money.  []the_shape89[\ud83c\udf70] 18 points19 points20 points 1 month ago*(0 children) He's an obvious snake oil salesman. If making money with machine learning was as easy as he makes it out to be (scanning GitHub for code, copy pasting stuff...DONE!), then he wouldn't need to be making YouTube videos for money. Also, he authored this pile of absolute garbage. Another also, there's no cheap and quick online course that will teach you how to make meaningful amounts of money in ways that are legal, long term feasible andor not already available somewhere for free. If you have a good system of making money, you capitalize on it by hiring people to go out and do this stuff andor investing in them. You don't go around charging a measly $200 for a handful of people to sign up. Which is the exact same deal as all the other TAKE MY PAID COURSE AND I'LL TEACH YOU HOW TO MAKE MILLIONZZZZ type guys.  []s4hc 16 points17 points18 points 1 month ago(3 children) Not surprised to be honest. The little content I came across on YouTube was always so cringeworthy.  I was a little disappointed when Daniel Bourke mentioned him in a few of his YouTube videos and when Andrew Trask was on one of his podcasts. No idea why they did this.  []KamWithK 5 points6 points7 points 1 month ago(2 children) Those are actually quite simple to answer: Daniel Bourke - Loves him as he inspired him to learn about AI, Siraj is literally why he's here (I believe he even took his course) Andrew Trask - Both talk about decentralised neural networks, AI safety, believe uni isn't a requirement now, also spreads attention to him (that's how I heard about Trask)    []mcjon77 16 points17 points18 points 1 month ago(1 child) The guy just showed everyone how to make money in ML. Build a brand and sell courses on ML using the strength of your brand.  The dude built a $200 course and sold it QUICKLY to 1,200+ people. So he brought in almost $250K. Damn, maybe I need to start building courses online.    []styx97 28 points29 points30 points 1 month ago(3 children) Well, can say I saw something like this happening a mile away. I was really sad that Grant (3b1b) Sanderson did that podcast with him (that way he got tagged with someone original) because he's really a fake. If you really want to know ML, watch the lectures of Alex Ihler, Hinton, abu mostafa or Hugo Larochelle. There's like a hundred better educators on YT.  []type1advocate 16 points17 points18 points 1 month ago(1 child) I think it's great that Grant was on there because it was an amazing juxtaposition. If one was on the fence about Siraj before that episode, any doubts were definitely confirmed.  []styx97 15 points16 points17 points 1 month ago(0 children) What you say actually makes more sense. But far more people will just see the title and the thumbnail and associate them together.    []sjogerst 12 points13 points14 points 1 month ago(0 children) Document and chargeback. Let him figure it out.  []mujtaba87 13 points14 points15 points 1 month ago*(2 children) Beware of another spammer like him, I guess his name is Qazi and sells his courses with the name clever programmer. He once published a video \u201cWeb development in Three Minutes, like seriously!    []brownck 14 points15 points16 points 1 month ago(0 children) Try out my new course instead: \u201cHow to make even MORE money with machine learning!  []UnfriendlySoloutions 13 points14 points15 points 1 month ago(0 children) He is a good salesmen, EXACTLY the type of person who can talk investors into dumping cash into a silly startup idea  []psxpetey 11 points12 points13 points 1 month ago(0 children) anything with how to make money in the title is a fucking scam bud. they'd be too busy making money if it worked not offering a book or a course on it  []stokhos_phusis 12 points13 points14 points 1 month ago(0 children) You pay with a credit card? Do a chargeback.  []AsliReddington 21 points22 points23 points 1 month ago(2 children) All he does is make commentary on HN links or papers in the news. No new stuff at all from him. Lame videos that are just cringe.  []trexgomez 18 points19 points20 points 1 month ago(0 children) The cringiest part is where he puts a meme from 2015 every 5 seconds in his videos.  []lambda_pie 5 points6 points7 points 1 month ago(0 children) cringe This.  []Safayet_Khan 22 points23 points24 points 1 month ago(7 children) Let me clarify something. I am SAFAYET KHAN. I deleted my twitter account. Why?  Because I have never used Twitter before this drama that you are seeing (Screenshots)! And I don't want to get addicted to it. So why did I open my Twitter account for only 3 days?  I opened it to attract Mr. Siraj Raval attention because he was not responding to my Emails for refund. After I was twitting about my experience, he agreed to refund my full money back money, which is $199, I deleted my Twitter accounts for good. This is all! If you have any questions about my tweets please comment here. And for your information, Mr. Siraj Raval didn't give me a single penny much to delete my account. Thank you.    []bit2bit2 10 points11 points12 points 1 month ago(0 children) Thank god I didn't fall for it \ud83d\ude02  []staircase7 9 points10 points11 points 1 month ago(0 children) The funny thing is, that in the machine learning community he has always some kind of a joke. If you search by controversial and by all time the second to top post is about one of his oldest videos.  []doodly19 10 points11 points12 points 1 month ago(0 children) He is a major youtube fraud claiming to know all of AI and making superficial and very irritating videos about them which teach you nothing.  []shaman311 9 points10 points11 points 1 month ago(0 children) Here is a document a student put together. If you guys are looking for the facts then here it is : https:case-for-a-refund.s3.us-east-2.amazonaws.comfeedback.html Sadly, we (the students) are dealing with a hypeman who has no intention of refund people their money.  []thedmandotjp 9 points10 points11 points 1 month ago(1 child) I was really interested in this dude's courses when I first started out earlier this year and I almost signed up. I ended up not doing it because I was getting weird vibes about how much content he had and how similar it seemed. Jumped headfirst into Andrew Ng's Coursera course instead and now I'm so glad I did. That was money and time I couldn't afford to waste because I'm learning on the side and supporting a family. Fuck Siraj.  []rayryeng 3 points4 points5 points 1 month ago(0 children) Hear hear.  []gwchase 8 points9 points10 points 1 month ago(3 children) I'm late to this thread, but still wanted to give my two cents. Siraj also took my code, and used it in his AI In Medicine video. He didn't provide any credit, until I asked him to. On top of this, the repo wasn't even forked; it was copied pasted into a new repository. There's a name for this: plagiarism, plain and simple. For reference, you can compare my repo and his repo; like I said, blatantly copied.    []Mr-Yellow 7 points8 points9 points 1 month ago(0 children) Make Money with Machine Learning with a registration fee  Obvious scam is obvious.  []mxschumacher 7 points8 points9 points 1 month ago(0 children) Seems like he made money with machine learning  []lstama 21 points22 points23 points 1 month ago(4 children) I knew this guy name since a while but only today I open his Youtube channel because of your post. ... and then I found his Make Money with Tensorflow 2.0 video.  []Fin_Win 3 points4 points5 points 1 month ago(2 children) In a nutshell, he just came up with an idea of presenting a time series model for making investment decisions.      []hilberteffect 13 points14 points15 points 1 month ago(1 child) Always pay using a credit card. No exceptions. Then, when something like this happens, and you have documentation that a) you were ripped off and b) the person has refused to make things right or refund you, you simply submit a chargeback to your credit card company and provide them with the documentation. It's then up to the merchant to provide POSITIVE proof that they did indeed provide the goodsservices as described. If they can't, not only do you get a full refund, but they get slapped with a fat 20% fee on top of the original amount.    []Dump7 5 points6 points7 points 1 month ago(0 children) Now it makes sense. I was a active watcher of his content a while ago. I don't know why I then stopped. Maybe I subconsciously knew all of his content was BS.  []humblefool_ 6 points7 points8 points 1 month ago(0 children) I am still in his course, I will update this post by letting you know guys that after completion of this course whether he gives a consultation work or not to his graduates as he promised in his course. I really have less to no hope that he will.  []platypusPerry245 6 points7 points8 points 1 month ago(0 children) I spent hours and hours watching his BS . I learnt more from deeplizard channel than from siraj's bs. I watched one of his startup videos , I thought he would show some result at the end of 45-50 minutes long video ,turns out all the videos are just hypetrains  []boxxa 18 points19 points20 points 1 month ago(8 children) I started watching him early. His videos are good and get you excited but every topic is click bait and doesn't provide any useful info. I unsubscribed after I found Sentdex who is incredible. He loves programming, his videos have real code, and gives you a ton of examples and content to take on your own and help you find things to dive deeper in.  https:www.youtube.comusersentdex  []tensorwave 5 points6 points7 points 1 month ago(7 children) I also recommend Arxiv Insights on YouTube    []Ol3k71 5 points6 points7 points 1 month ago(0 children) When I Startes MLAI a few years ago, he had a patreon account, to accept donations. There was also a deal about personal Q&A for donators, if you spend a specific amount per mounts. None of the promised bonuses were available. And also, no refund or anything else.  []lunarlustra 5 points6 points7 points 1 month ago(0 children) Damn. That is not a good look. Agree that 3Blue1Brown is a great resource. There are also a lot of tutorials online at @weights_biases for beginner, intermediate, and advanced practitioners.  [][deleted] 5 points6 points7 points 1 month ago(0 children) Well I'm not surprised. His content is basically useless and it made me cringe when people were comparing him to 3blue1brown on his podcast lmao  []sujithvemi 5 points6 points7 points 1 month ago(0 children) Wow, just wow. I try to distance myself from this guy a lot, except for the list of resources he put up on GitHub for some subjects. This is the only useful thing he does because it is not his original creation, he collects stuff from others who collect stuff, so it is useful. I am so happy to see that actually there are so many people who call out this fraud. One video of his and that's it, you can say he is a fraud. But I have actually come across a few people who follow him a lot. I get it if amateurs are doing this but what I was not able to understand is that some eminent researchers in AIML have actually given this guy a lot of exposure with their tweets and stuff. There are so many frauds now who are making money from gullible ML aspirants, riding the hype that surrounds the field. I mean this guy has written some paper on quantum computing with barely any content and he got a lot of praise for it. I can't believe that I actually started followed him on Twitter long back because I thought I was on the wrong side for calling this guy useless, because he has so much of a celebrity status while I consider myself an amateur-intermediate. This post is gonna help me boost my confidence a lot, thanks for this. Anyway, I guess people are going to be fooled by one or the other until the current hype continues, feel bad for all those aspirants.  []margaret_spintz 6 points7 points8 points 1 month ago(0 children) I guess the title of the course 'make money with ML' was only accurate from his perspective \ud83e\udd14  []mimighost 6 points7 points8 points 1 month ago(0 children) Why would anyone signed up a course literally named 'Make Money with Machine Learning'? lol  []suzushiro 6 points7 points8 points 1 month ago(0 children) One of his videos was a straight up ripoff of a medium article. I pointed it out in the comments but he never replied back. This guy is obviously all hype.  []NightmareOx 5 points6 points7 points 1 month ago(0 children) I don't know how is on the other parts of the world, but here in Brazil his school is pretty shady. I'm a CS masters degree student and my area of research is machine learning and a couple of months ago one of my lab colleagues broad to my attention this Facebook videos from his school that were sooooo wrong. The teacher clearly didn't know the topic he was teaching, and was sharing the original articles from neural networks, but as the class goes on, it is clear that he haven't read the articles too. I've started digging up more classes and the school's materials and I was shocked about how wrong or just bad they were. They use this flag of being free as a justification, but they ask for donation all damn time and hat pisses me most is that when you stop to read the comments it is clear that most people there are trying to change their careers. This people are being scammed and he doesn't give a shit about it. Never liked the guy that much, when doing my research on the topic of GANs it became clear to me that he was fake and not that knowledgeable about all topics that he boasts about, but that is a new kind of low :  []PlusImagination 6 points7 points8 points 1 month ago(0 children) The ML community needs to do something about Siraj asap. He's just going to wait until this blows over and then scam the next series of students.  He's using the work of the machine learning community as deceitful marking material for his courses, and is generating a bunch of copycats. There are other internet marketing people trying to sell shitty courses to newbie programmers who don't know any better.  I don't believe the 'darwinism' argument. At the end of the day, their using our work to scam other people, particularly aspiring programmers. They're targeting a niche; these may not the same people who would fall for MLM knives or supplements.  I also don't believe that this is just one fuckup for him. If you read the other discussion, this is a persistent pattern for him. Also think of this, he is a persoon heinous enough to scam his own fans, fans who are looking better their careers. And he did it in an inventive of making a seperate slack channel for refunds, then mass banning them all. This person does not deserve the chance to scam people again. Not only is this an extremely low regard for ethics, it is a clear demonstration of very low empathy.  If you look at his Linkedin, he's never held a job for more than a year, and mostly in software engineering. Somehow I doubt he's going to go back into software engineering. He's going to try again.  People with authority in the ML community like u UltraMarathonMan should come out with a statement, not just Siraj, but the proliferation of these internet marketers and their ML course scams. But these other scammers mostly take their cue from Siraj.  []sf_degen 10 points11 points12 points 1 month ago(0 children) Dispute with your credit card. Always pay with CC so you can dispute things like this.  []mindaslab 8 points9 points10 points 1 month ago(2 children) I suspected that guy was a fraud    []permalip 4 points5 points6 points 1 month ago(0 children) To be honest, I feel bad for the people who fell for this trap course, but I also feel that they went into it blindly without doing the research. It's one thing that he says there is only 500 spots and the course is mediocre (or so people says) - but not refunding the unhappy people is scummy. Nevertheless, this still comes back to which terms of service these people agreed to. The problem with Siraj is that he is the hype guy of AI, getting people hyped for it, believing they can learn it without much if any effort. When I started in AI, I thought Siraj was the shit, because I didn't know anything Machine Learning related. After a few months, it was easy to see through the videos.  []gachiemchiep 5 points6 points7 points 1 month ago(0 children) 200 usd to learn a lesson : don't just believe random guys with big mouth on internet  sound like a good pricw  []haloworlds 8 points9 points10 points 1 month ago(3 children) So asking about general people who purchase courses, do you y'all research the person's industry experience or academic experience? Or just a famous GitHub repo or YouTube channel is fine? This guy, at least according to his LinkedIn profile worked at Twillio and meetup which is nice but as a software engineer not a machine learning or AI or even anything related to data science. Either way he maybe a very good software engineer but doesn't look like a AI or Machine learning expert.  []sj90 11 points12 points13 points 1 month ago(1 child) He's not an expert. He basically sold the hype around machine learning in the past few years to develop a brand. And other people in the industry also utilized his brand for their own purposes. It sort of legitimized it over time. But that's what hype in anything does. It creates a strong herd mentality scenario where people who get swept away by the hype buy into the brand. This gets worse when the same hype seeps into the industry in such a way that people started getting jobs with relatively limited backgrounds early on. Making a whole lot of people think it really is that easy and lucrative a field to get into. A brand is enough to trigger that mentality anywhere for anyone. And couple the brand with the idea that something interesting is very easy and simple to learn and will make you enough money, then your logical defences drop further.  []haloworlds 4 points5 points6 points 1 month ago(0 children) Alright. Got the point.  For me personally if I am seeking paid helpcoursementoring then either your industry or academic experience should speak for your work or a good work product in open source (tutorials and awesome lists and readme stuff doesn't count towards that no offense on people who does that). For example if I am looking for mentoring in data science field and Wes McKinney is available sure get help from him    []kekkimo 8 points9 points10 points 1 month ago(0 children) In fact, the title of the course is NOT a SCAM. Since he is really making money with machine learning by selling this course about ML.  []Schoolunch 4 points5 points6 points 1 month ago(0 children) wow, and yet again someone creates a school to scam people  []arfath99 2 points3 points4 points 1 month ago(6 children) Why does anyone take a paid courses when u have free YouTube videos from edureka, simpli learn ,free code camp and many more on udemy,Coursera and so on I knew from the beginning itself this man has just some basic knowledge which he acquired online from free courses and he started selling just like many others on YouTube (instead of free he started charging) which made it such a fuss. This man is real smart, he made people love the AI first and it's salaries. Then he boosted about his Stanford degree to newbie . Even without this paid courses he was making great income with 200k subscribers atleast    []iceevil 3 points4 points5 points 1 month ago(0 children) How to make money with ML: Scam other people by offering fake courses on ML  []UnfriendlySoloutions 2 points3 points4 points 1 month ago(0 children) btw he has an o'reilly book .... https:www.amazon.caDecentralized-Applications-Harnessing-Blockchain-Technologydp1491924543  reviews explain it all   Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 18307 on r2-app-0890e987654e829c0 at 2019-11-18 00:37:10.100940+00:00 running 85b1b7a country code: US. "
  },
  "14":{
    "title":"[D] Gary Marcus Tweet on OpenAI still has not changed misleading blog post about \"solving the Rubik's cube\" : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,248 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[D] Gary Marcus Tweet on OpenAI still has not changed misleading blog post about solving the Rubik's cube (self.MachineLearning) submitted 28 days ago by chansung18 Post a comment!  []Veedrac 82 points83 points84 points 28 days ago*(29 children) Gary's summary is much more misleading than the blog post. Concerns 1-4: \u201cNeural networks didn't do the solving; a 17-year old symbolic AI algorithm did FTA: \u201cWe train neural networks to solve the Rubik's Cube in simulation using reinforcement learning and Kociemba's algorithm for picking the solution steps. (NB: I would prefer this to be stated more prominently in less technical terms.) Concern 5: \u201cOnly ONE object was manipulated, and there was no test of generalizability to other objects FTA: Five different prototypes were used, a locked cube, a face cube, a full cube, a giiker cube, and a 'regular' Rubik's cube. The article never claims to do anything other than solve Rubik's cubes. Concern 6: \u201cThat object was heavily instrumented (eg with bluetooth sensors). The hand was instrumented with LEDs, as well. FTA: The five different prototypes had different levels of instrumentation. The 'regular' Rubik's cube had none, except small corners cut out of the centre squares to remove symmetry. FTA: Videos of the LEDs. They're blinking and red, FFS. Concern 7: \u201cSuccess rate was only 20%; hand frequently dropped cube FTA: \u201cOur robot still hasn't perfected its technique though, as it solves the Rubik's Cube 60% of the time (and only 20% of the time for a maximally difficult scramble). FTA: A whole section dedicated to this issue.  []briggers 34 points35 points36 points 28 days ago(2 children) This is what his book, Rebooting AI, is like. Many misrepresentations, and a general argumentation style of it isn't perfect, therefore it isn't good. There is definitely a case for being much more cautious about MLDL than many over-hyped journalists are, but this guy is just looking to fill a contrarian niche.  []mimomusic 20 points21 points22 points 27 days ago(1 child) I think Marcus is a well-spoken and intelligent man. I also think he is exceedingly pedantic to such a degree that he detracts from the actual problem at hand while most laymen are well capable to read between the lines and see research or blogposts for what they are. Sure, precision in academia is not something you can just forego without regard for making yourself understood, but the people who actually care about that stuff are very likely to dive into the nitty-gritty anyway. Those who succumb to hype would misunderstand and fall for nonsense headlines even if they were 100% unambiguous and perfectly constructed - but who cares about what The Sun is trying to convey? Almost all of us were well-aware of all the caveats parent poster mentioned, at least most of them. They are almost irrelevant in this context, even something as arguably crucial as generalizability (what a goddamn word!) takes a back seat to the main issue of robotic dexterity.  I understand that different people approach subjects with different degrees of rigor, and I can feel Marcus' concerns, but I also think they are very much nitpicky and not at all important to the discussion as far as OpenAI's due diligence and openness to critique is concerned. He sure is someone who will always be at odds with the community at large, but whether it's time well-spent is something I view with a healthy portion of skepticism; I think way too much effort goes into scrutinizing things that, as far as we can tell at this stage, barely matter in the long run.  []sanxiyn 17 points18 points19 points 28 days ago(20 children) I was actually disappointed by corner cutout for regular Rubik's cube. I consider that a significant instrumentation, and I think it is entirely justified to say that OpenAI did not solve vision part of regular Rubik's cube.  []whatevenarecomputers 2 points3 points4 points 27 days ago(19 children) They also didn't claim to solve the vision part of Rubik's cube solving (and explicitly said so), so it can't really be used as a way to detract from the OpenAI article. It was an article about the physical manipulation.  []nrmncer 3 points4 points5 points 27 days ago(10 children) tbh though what exactly has been achieved here then? Robots built to solve Rubik's cubes have been around for a while, most do it faster than that hand. The accuracy is low, it doesn't generalise well, there's a lot of hacks involved.. I guess the fact that it can fend off the giraffe is a novelty. But without any generalisation and given the low accuracy there's not much news here.  []whatevenarecomputers 6 points7 points8 points 27 days ago(1 child) There is a ton of generalization and resilience to new input here - putting scrunches around the fingers, putting it in a glove. Poking it as it solves. The breakthrough is the motion planning and object manipulation is done entirely through RL.  []sheeplearning 2 points3 points4 points 26 days ago(0 children) Yes but it is unclear if any of that is useful given it is not sufficient to solve the cube. Perhaps new approaches and additional experiments are needed and celebrating mediocrity just makes it harder for anyone to actually solve it. Robot does not really solve, warcraft does not really see, GPT2 is too harmful to the world and BERT outperforms and releases the model without any fuss -- everyone knows what is really going on here.  []tshadley 4 points5 points6 points 27 days ago(7 children) But why has no one built a Rubik's cube robot implementing a one-handed solve? It's relatively easy to build a 5 fingered hand mock-up with servomotors. It has to be because the controller algorithm to hold the cube, turn it, and solve it with 5 fingers is vastly complicated-- way beyond any traditional approach. The Deep-Neural-Network dexterity algorithm is the amazing accomplishment here.  []kit_hod_jao 2 points3 points4 points 26 days ago(0 children) I think this is part of the issue with the demo - it's hard to get a sense of how hard it is. I can certainly imagine it's very difficult to achieve, but one of the benefits of existing benchmarks is our expectations are finely calibrated to detect unusually good performance. Still, new benchmarks have to come from somewhere and IMO this is very impressive. It's just hard to appreciate how challenging it is.  []AnvaMiba 1 point2 points3 points 26 days ago(3 children) It's relatively easy to build a 5 fingered hand mock-up with servomotors. It's not. Five finger robotic hands that can move accurately are crazy expensive. They are only used for research as prosthetics, not as standalone robots, which is why you don't see many people using them to do stunts like this. Manipulation of objects of known shape and mass with stationary robots is a technologically mature task, there are plenty of industrial robots working on assembly lines which can manipulate objects much faster and more reliably than this. They don't use any fancy RL, just good old control theory and motion planning. So what's the innovation there? That they used RL?  []tshadley 0 points1 point2 points 26 days ago(1 child) Manipulation of objects of known shape and mass with stationary robots is a technologically mature task, there are plenty of industrial robots working on assembly lines which can manipulate objects much faster and more reliably than this. They don't use any fancy RL, just good old control theory and motion planning. Any specific example of comparable complexity? From what I've seen, industrial robots motion environments are tightly constrained and limited.  []AnvaMiba 0 points1 point2 points 26 days ago(0 children) From what I've seen, industrial robots motion environments are tightly constrained and limited. Yes, for safety reasons. The OpenAI robot hand isn't strong or fast enough to cause injury, which is why they can fiddle with it while it's moving. There are also robots designed for safe interaction with humans, or robust enough to resist external perturbations  []tshadley 0 points1 point2 points 26 days ago(0 children) It's relatively easy to build a 5 fingered hand mock-up with servomotors. It's not. Five finger robotic hands that can move accurately are crazy expensive. They are only used for research as prosthetics, not as standalone robots, which is why you don't see many people using them to do stunts like this. To more accurately make my point, I'll say it's relatively easy to obtain a 5-fingered hand mockup. ShadowRobot seems to have built the first dexterous hand in 2005. The hard part is controlling it.  []nrmncer 0 points1 point2 points 27 days ago(1 child) But why has no one built a Rubik's cube robot implementing a one-handed solve? mostly because if you're going to build a specialised machine it makes more sense to build.. well a regular machine. If all it can do is solve the cube then there's no need to make it to resemble a hand. It's a nice video to look at but they already had dextrous movements down a year ago. This is essentially the same thing with a slightly more modular task.  []tshadley 0 points1 point2 points 26 days ago*(0 children) mostly because if you're going to build a specialised machine it makes more sense to build.. well a regular machine. If all it can do is solve the cube then there's no need to make it to resemble a hand. But all regular machines for Rubik's solving have been built; clamps, rotating platforms, etc.. This was an obvious next step. It's a nice video to look at but they already had dextrous movements down a year ago Manipulating a solid cube with one hand is a vastly simpler than rotating individual planes of a Rubik's cube with one hand.  []sanxiyn 2 points3 points4 points 27 days ago(3 children) Isn't vision (or state estimation by vision) fundamental part of manipulation? I guess with Bluetooth instrumentation OpenAI showed manipulation would have worked if vision was working. But they couldn't get vision working.  []whatevenarecomputers 1 point2 points3 points 27 days ago(2 children) I think it's much more likely that they wanted to focus on a solution for the physical motion planning. I would not be surprised if there were independent projects to handle the object recognitionposition portion (which is effectively a separate problem).  []sanxiyn 0 points1 point2 points 27 days ago(1 child) OpenAI directly stated in the paper that they couldn't get vision working. See page 16. To quote: We experimented with a recurrent vision model but found it very difficult to train to the necessary performance level. Due to the project's time constraints, we could not investigate this approach further.  []whatevenarecomputers 0 points1 point2 points 27 days ago(0 children) https:openai.combloglearning-dexterity This appears to be their ongoing effort to implement the vision portion.  []ispeakdatruf -1 points0 points1 point 27 days ago(3 children) So what does solve in the title mean? For a human, the harder part is figuring out the steps involved. I can teach a 5yo how to rotate the cube in a minute. But teaching the kid to actually solve the cube will take much longer.  []whatevenarecomputers 3 points4 points5 points 27 days ago*(2 children) Yes, a rule of thumb for AI is the more intuitive for a human, the harder it is to make a computer do, and vice versa. We see this phenomena everywhere. Resilient motion planning and balance, object recognition, natural language processing, object location estimation. Humans do all this by the age of of 5. It's easy to make a computer to solve (symbolically) Sudoku or a Rubik's cube. Hard or time consuming for a human (without lots of training). Can we figure out the order of rotations to solve a cube is not what OpenAI is trying to solve. That's a well explored, solved problem. Conversely, human brains find spacial manipulation very intuitive past the age of like 3. But it's hard to build a system, even with fine-grained closed-loop feedback, that can manipulate physical objects. We have to design all sorts of loops and heuristics to estimate the physics of the object, how to move it in space, how to not drop it, how it can be folded or rotated or whatever. Letting reinforcement learning figure out all these nitty-gritty details represents novel progress being made in this space.  They have a system which seems to have figured out how to compensate for all sorts of messy inconsistencies like actuators not responding exactly how the model would expect it to (they mention it being resiliant even when actuators entirely break), all the way up to how do we figure out the individual finger movements to grasp and rotate a cube face.  []ispeakdatruf 0 points1 point2 points 27 days ago(1 child) In my limited experience with robotics, I totally concur with you. It would have be somewhat OK to title an academic paper with that title. People in the area would understand. But that's not what OpenAI did. They put out a blog post with that title, which is clearly intended for the general lay audience. The average person, who knows nothing about how hard actuator control, sensors, etc., are, will naturally assume that the harder, cognitive problem is being solved.  []whatevenarecomputers 0 points1 point2 points 27 days ago(0 children) What's the harder cognitive problem to solve here? If it's symbolically solving a cube, that's just not an interesting problem to solve, and we don't need ML to do it. It's not a game of Go. The interesting cognitive problem is the motion planning. I just disagree that the blog post is misrepresenting what OpenAI has accomplished. Its entire focus is on the hand, its motion, and the feedback systems involved. Even the title is Solving Rubik's Cube with a Robot Hand. Sure, somebody could read only the title and come to the wrong conclusion. But that's not really OpenAI's problem.  []yuvalpinlp[\ud83c\udf70] 1 point2 points3 points 26 days ago(3 children) So, about 1-4, in what sense does the RL net solve the cube? train to solve... picking the solution steps you don't find this phrasing very misleading?  []Veedrac 0 points1 point2 points 26 days ago(2 children) They say they \u201csolve the Rubik's Cube with a human-like robot hand. This is true. I agree that the phrasing of \u201cand Kociemba's algorithm for picking the solution steps is too technical to be properly transparent to the average reader, even many readers with ML background, and I agree it is not nearly prominent enough\u2014I said as much in my post. If Gary's tweet was about that only\u2014as in, it did not make his other claims, and it was phrased so it was obvious the issue is clarity rather than honesty\u2014I'd have supported his commentary unreservedly.  []yuvalpinlp[\ud83c\udf70] 0 points1 point2 points 25 days ago(1 child) I'm sorry, I find it impossible to interpret solve as anything other than figure out what to do at each step, which is the one thing their RL system *didn't* do. As Gary noted, there are other, much more accurate verbs to use, my vote goes to manipulate.  []Veedrac 0 points1 point2 points 25 days ago(0 children) I disagree that bringing a cube to the solved position cannot be described as solving it, but your disaffection is understandable and this wasn't one of my points of disagreement with the original post. I agree that 'manipulate' would be a much clearer term.  []SkiddyX 58 points59 points60 points 28 days ago*(6 children) This subreddit's unquenchable thirst for drama continues... \ud83d\ude02  []booleyan 28 points29 points30 points 28 days ago(2 children) Siraj started a fire we can't put out.  []egrefen 11 points12 points13 points 27 days ago(0 children) It's always been burning \ud83c\udfb6Since GPUs been churning \ud83c\udfb5  []ispeakdatruf 2 points3 points4 points 27 days ago(0 children) You_again would beg to differ... (with due apologies to Prof Schmidhuber, who has been shafted by the rest of the community)  []LaVieEstBizarre 12 points13 points14 points 27 days ago(1 child) Experts hate hype machines  []dod_worker 12 points13 points14 points 27 days ago(0 children) And hypers hate expert machines!  []MonstarGaming 2 points3 points4 points 26 days ago(0 children) For real. We ban beginner tutorials only to fill the gap with community drama. WTF? I wish we could tag and filter this content at the very least.  []balls4xx 13 points14 points15 points 28 days ago(0 children) A lego bot can solve (rotate until each side has a single color) a Rubik's cube, even I can solve one after inputting the tile pattern into some website. I think what they 'solved' here was making a robotic hand do it while being accosted by a stuffed giraffe.  []tradediscount 37 points38 points39 points 28 days ago*(17 children) I think Marcus is being a little disingenuous here. The key achievement of the OpenAI research he refers to is using reinforcement learning for really hard real world manipulation of physical objects using a robot hand. The Rubik's cube is used as a prop to represent a hard real world problem (hard as in difficult to manipulate effectively). OpenAI's blog post explicitly (but perhaps not prominently enough for Marcus or seemingly many subeditors who missed it in their reporting) states they use Kociemba's algorithm to determine the next move. This non AI shortcut was presumably used to reduce the number of steps, given the already high difficulty physical manipulation task they'd set themselves. Granted, many newspapers reported is as if the ML part had also worked out how to solve the cube, <and OpenAI have not tried to correct the misreporting>, but I'm not sure that's feasible or even necessary. Edit: bit between angle brackets not true, see uthegdb comment below. In addition, the cube has been solved using deep nets by several other teams (a quick Google shows published in reputed journals too) so while not trivial I have no doubts OpenAI could also solve it if they chose to. Finally, Marcus seems to like creating controversy to publicise his view that a lot of the DL community misrepresent the promise and capabilities of DL, which in my view simply isn't true. Hinton, Bengio, Le Cun, Chollet et al have all in my view been very open, measured and fair about the technology.  []thegdbOpenAI 44 points45 points46 points 28 days ago(9 children) Granted, many newspapers reported is as if the ML part had also worked out how to solve the cube, and OpenAI have not tried to correct the misreporting, but I'm not sure that's feasible or even necessary. We ping journalists to ask them to correct factual errors in reporting when we see them (though they may not always agree with our corrections). For example, the Washington Post article (https:www.washingtonpost.comtechnology20191018this-robotic-hand-learned-solve-rubiks-cube-its-own-just-like-human) feels misleading, so we've emailed them and linked them to the relevant sections in our blog post (namely, that we use Kociemba's algorithm as you mention). If you see other articles that need correcting, always feel free to let me know \u2014 gdb@openai.com!  []tradediscount 7 points8 points9 points 28 days ago(0 children) I stand corrected. Nice one!  []openaievolution 1 point2 points3 points 27 days ago*(1 child) Tbf this is an artifact of your science by press release strategy as well. If you release a public preprint first, journalists will have an easier time sourcing opinions from other well informed folks in the field, and presumably the reporting would get better.Zach Lipton elaborates more on this point in this thread here: https:twitter.comzacharyliptonstatus1184237037622136832  PS: To be clear, I am not arguing for not doing press releases, but rather putting out a preprint first and allowing some time bw the preprint and the press release.  []whatevenarecomputers 2 points3 points4 points 27 days ago(0 children) When has a reporter ever erred on the side of measured response from experts in the field instead of writing up the most sensationalized version they could concoct and still maintain plausible deniability?  []garymarcus -2 points-1 points0 points 28 days ago(4 children) why not post some sort of clarification on your own site? it is clear that your blog was prone to misinterpretation.  []Ghenlezo 32 points33 points34 points 28 days ago*(3 children) I am a little baffled by your reaction to this. You worked with robots before, yes? The fact that they actually got a hand to sort of work on an intricate task, and to the point that it is able to adapt to many perturbations, is astonishing.  Did they make any deep algorithmic breakthroughs? No, but nor did they claim to. They scale things up and see how far they can push them. This is super useful work. AI is an engineering discipline not a science, and unfortunately is half-ashamed of this fact.  Was anyone with any literacy fooled into thinking the cube-solver was learned? It says right in the post it was done symbolically. Even if the solver was learned that is, frankly, much less impressive than demonstrating modestly robust physical dexterity. Your critique of the major deep learning figures is of naked emperors. But I see scholars constantly apologizing for their state of undress, baffled at how well they are weathering ever-colder temperatures.  One begins to wonder if invisible wool does, in fact, exist.  []garymarcus 0 points1 point2 points 28 days ago(1 child) Curious for your take compared to the much less hyped Baoding balls the week before. Here's what I said in the tweet that you apparently didn't read: I will say again that the work itself is impressive, but mischaracterized, and that a better title would have been manipulating a Rubik's cube using reinforcement learning or progress in manipulation with dextrous robotic hands or similar lines.  []Ghenlezo 30 points31 points32 points 28 days ago*(0 children) I read your \u201cdetailed analysis meme. I have not read every tweet you wrote on the matter. I am glad you admit that the progress on the actual task they were attempting was good. Your image macro makes it look as if you are dismissing impressive work for trivial misunderstandings. Anyone with any experience in this stuff knows, watching the video not even reading the paper, that what is impressive is the dexterity, and that solving a cube is trivial symbolically. Imagine we're watching a ski race on television and someone said, \u201cYou know they may be able to ski down fine but what they don't show you is they don't ski up! They use a chairlift! That's the impression your image macro gives me. They say right in the post that it was done symbolically. Failure to spoon feed journalists is not a sin in my book.  []tristes_tigres 0 points1 point2 points 27 days ago(0 children) AI is an engineering discipline not a science Engineering consists in applying known scientific principles to solve the real-world problems. AI at this point is barely more than alchemy - a compendium of techniques that seem to work from time to time for unknown reasons, that is very useful for extracting funding from wealthy patrons hoping to expand their riches.  []master_yoda_1 -2 points-1 points0 points 27 days ago(0 children) The same nonsensical hype you guys created with gpt-2 by not releasing it, and later on you proved yourself wrong and released it. What is your comment about that?  []m_nemo_syne 2 points3 points4 points 26 days ago(3 children) (I have to be pedantic for a moment: you call Kociemba's algorithm a non AI shortcut, but it is AI, just not machine learning. This is an instance of the AI effect: https:en.wikipedia.orgwikiAI_effect)  []tradediscount 1 point2 points3 points 26 days ago*(1 child) I strongly disagree. You are being precise, not pedantic, and I appreciate it thoroughly. edit: would non ml be better?  []m_nemo_syne 1 point2 points3 points 26 days ago(0 children) Thanks! I think so :)  []WikiTextBot 0 points1 point2 points 26 days ago(0 children) AI effect The AI effect occurs when onlookers discount the behavior of an artificial intelligence program by arguing that it is not real intelligence.Author Pamela McCorduck writes: It's part of the history of the field of artificial intelligence that every time somebody figured out how to make a computer do something\u2014play good checkers, solve simple but relatively informal problems\u2014there was a chorus of critics to say, 'that's not thinking'. AIS researcher Rodney Brooks complains: Every time we figure out a piece of it, it stops being magical; we say, 'Oh, that's just a computation.' [ PM | Exclude me | Exclude from subreddit | FAQ Information | Source ]Downvote to remove | v0.28  []garymarcus -1 points0 points1 point 28 days ago(1 child) i am not aware of any system that has solved the cube with pure RL; the ones i have seen are hybrids that also include monte carlo tree search. correct me if i am wrong...  []nikitau 1 point2 points3 points 23 days ago*(0 children) DeepCubeA for example does it with A* and trains the heuristic using function approximation and value iteration. While I haven't seen a system that reliably solves the cube using just RL without search, I feel like the same case could be made about Go too. Do you feel that lookahead is somewhat cheating in this case? Just curios, because at the end of the day, what gives such systems the edge over pure search ones is the fact that they learned how to guide the search. update: I actually read your write-up about innateness in AlphaZero in the meantime and understand a bit more where your statement is coming from. Its a good read and gave me a bit more insight into your point of view and I will say that I agree that in the philosophical context of AGI, using search definitely equates to injecting priors and is far from a tabula rasa approach. I get why you consider why their success may be uninteresting wrt. AGI; however within the domain of fully observable, deterministic environments, search methods could be seen as a common and rather simple denominator of all solvers, which one would be foolish not to use. If we are referring only to this small subset of all environments (which does model a lot of useful processes still) wouldn't AlphaZero (say apart from its use of CNNs) be one of the least biased solvers? I am curios on your thoughts about this.  []garymarcus -3 points-2 points-1 points 28 days ago(0 children) Hinton in particular sometimes over promised quite a bit; i will likely write about that soon.  []JonathanFly 13 points14 points15 points 28 days ago*(7 children) I don't have the expertise to comment on the physical simulation part of this, so there may be some valid critique on that end, but I don't understand the primary criticism in this particular post. Isn't it obvious that solving the Rubik's cube is just a proxy for any dexterity challenge? Learning how to solve a Rubik's cube is trivial, it's inconsequential. For example, if OpenAI's project was 'robot that plays Tic Tac Toe in adverse conditions' and then in the video we see the Tic Toe paper oriented in different directions or moving around the table, in a room with dark or very bright light, on a table that's vibrating, with leaves blowing all around obstructing vision, using random types of pens and pencils that the robot arm has to adapt to on the fly -- this would basically be the same paper. Would you apply your same top-line criticisms to that project? Would you say the 'neural network isn't actually playing Tic Tac Toe, it's the 2000 year old Tic Tac Toe algorithm?' Maybe the problem is Rubik's cube have a mystique around them when I thought it was pretty clear that figuring out what to rotate is a trivial problem that any robot or human can already solve.  []ispeakdatruf 2 points3 points4 points 27 days ago(3 children) Learning how to solve a Rubik's cube is trivial, it's inconsequential. So, you have an RL algorithm that figured out how to solve Rubik's Cube?  []sanxiyn 1 point2 points3 points 26 days ago(1 child) You may like Solving the Rubik's Cube Without Human Knowledge.  []ispeakdatruf 0 points1 point2 points 26 days ago(0 children) Thank you, I was unaware of that work. Upvoted!  []garymarcus 0 points1 point2 points 28 days ago(1 child) many many people misunderstood the article given how it was framed; the washington post coverage is a case in a point.  []Ghenlezo 19 points20 points21 points 28 days ago*(0 children) Journalists have terrible incentives, as a rule no technical training, and little time. \u201cJournalists misunderstand you in the manner most likely to grab clicks is a universal argument against anyone who has interacted with a journalist.  []subsampled 5 points6 points7 points 27 days ago*(0 children) I think Marcus regularly raises interesting objections and ideas in the nature vs nurture (and symbolic vs connectionist) debate. Here, however, he may have missed the main point of this work, which emerges pretty clearly from the series of works by the same group. The main progress is clearly is in the context of in-hand manipulation via RL, whose complexity is very well-known to roboticists. Controlling a complex tendon-driven hand like the Shadow Hand to reconfigure an object with several degrees of freedom in presence of multiple contacts and disturbances has been a moonshot in robotics since forever. It's also true that OpenAI may have done better with choosing the title, but the work seems still a significant breakthrough, for sure in its robotics and transfer parts. And yes, in my experience 60% performance for the average case is definitely a good result for robotics demos standards of similar complexity.  []garymarcus 7 points8 points9 points 28 days ago(1 child) what's most notable about many of the comments here is that it is largely just ad hominem attacks; nobody can really argue that the screenshot on the left half of the slide of analysis (ie the opening of openAI's blog) actually matches what the paper did, and few people here are willing to acknowledge how widely the result was misinterpreted.  PR that invites serious misinterpretation is the definition hype; in the long run ML will suffer from a pattern of overpromising, just as earlier approaches (eg expert systems) have.  []adventuringraw 22 points23 points24 points 27 days ago(0 children) man, I came here ready to jump on OpenAI for being overly hyped, but their coverage itself really did seem measured, in spite of the press apparently misunderstanding it. Reading through the comments, I see mostly praise for you, combined with everyone roughly saying 'but in this case, it seems like Marcus jumped the gun, here's why'. You taking such a measured community reaction here as being nothing but 'ad hominem attacks' really makes me question what thread you were reading, because it apparently wasn't this one. If you're going to dig around to make sure claims are perfectly represented with no room for misinterpretation (a worthwhile activity given the current AI hype, don't get me wrong) you really don't get to so badly misrepresent your own treatment on a little subreddit like this. Literally anyone can read the other 30 comments on here. Does anyone else see 'ad hominem attacks'? Because I sure don't. Aside from a passing comment about 'filling a contrarian niche' it seems to be more about OpenAI's coverage, your specific critiques, and what people think about the issue. I saw your post, I read the blog post, it might have been easier than it should have been for a lay audience to misinterpret, but I really don't buy that it was on purpose. I don't even buy that it needs to be changed now that the mainstream reporters have come and gone, I honestly read OpenAI's coverage as intended, this was an impressive milestone in physical dexterity, that's it. As another comment pointed out, doing the whole solution (solving and all) in one learned architecture probably wouldn't have been radically harder than what was achieved even, assuming the other comment was correct, and there are other papers doing the actual Rubik's Cube solution finding. The reasons given in other comments for not buying your reasoning matches my own. My (honestly mostly unformed, I don't know your work well) opinion of you as a person doesn't factor into my not accepting your analysis in this case. Aside from one apparently actually rude ad hominem attack (that was called out by someone else, the original user deleted their post now) what's left is a long ways away from being unfair to you. If you're going to misrepresent your own treatment in such an obvious way, I'm not impressed when the topic you're trying to push is another group misrepresenting their research. That said, even if you were maybe a little overzealous in this case, and even if you're taking it a little personally that not everyone else here agrees with you, I wholeheartedly wish that mainstream reporting was more realistic, so Godspeed on your quest.  []garymarcus 1 point2 points3 points 28 days ago(13 children) you may have caught a minor error here but mostly you are comparing apples and oranges.  my main point was that the popular presentation (ie the blog) was misleading; finding stuff in the fine print in the technical paper doesn't fix that. and even so, note that the title of the article itself is misleading, as is the opening framing, as i detailed in a previous tweet. so the article itself has its own issues.  i am really most concerned though with your anemic defense of point 5: it doesn't matter whether openAI claimed to have looked at more than one object or not; the point is that if you don't have stronger tests of generalization, you can't make profound claims. 5 slightly different cubes doesn't mean you could not tighten a screw, open a lock, or button a shirt.  []Veedrac 17 points18 points19 points 28 days ago(7 children) You replied to the post rather than me. finding stuff in the fine print in the technical paper Everything I said was from the blog post, and not even a particularly close read of it. I don't expect the press to read dense technical papers, but I do expect them to read more than the title of the summarizing blog. 5 slightly different cubes doesn't mean you could not tighten a screw, open a lock, or button a shirt. OpenAI never claimed otherwise.  []garymarcus -3 points-2 points-1 points 28 days ago(6 children) perhaps i should have said blog abstract (ie the part reproduced in my slide); the Washington Post story stands as a testament to how prone the piece was to being misread, it's not just the title, but the full framing in the abstract i reproduced. and how much emphasis there is in the article on learning relative to the small space devoted to the large innate contribution, etc .and even on your last point \u201cunprecedented dexterity at top suggests that they ought to be able to do this in general in someform; they haven't actually tested that (aside from variants on a cube). as someone apparently in Ml, you should recognize how unpersuasive that is. there is a long history of that sort of thing having seriously trouble generalizing.  []Veedrac 14 points15 points16 points 28 days ago(1 child) The quote is \u201cThis shows that reinforcement learning isn't just a tool for virtual tasks, but can solve physical-world problems requiring unprecedented dexterity. I find it very hard to understand where your objection is coming from; that sentence is plenty reasonable. At this point I think my comments stand on their own, so I'm going to bow out.  []garymarcus 4 points5 points6 points 28 days ago(0 children) Which problems? without a test of generalizability to other noncubic, noninstrumented objects, and without a comparison to the Baoding result from a week before, I think the sentence is overstated. what are the plural problems even? I see one problem, no test of transfer. By know we should know that this is a red flag. Which doesn't mean that I am unimpressed. In fact, I said the following, in a immediate reply to my own tweet that you must not have read: I will say again that the work itself is impressive, but mischaracterized, and that a better title would have been manipulating a Rubik's cube using reinforcement learning or progress in manipulation with dextrous robotic hands or similar lines.  []sanxiyn 1 point2 points3 points 28 days ago(3 children) We can all agree that move finding was innate, but why does that mean large innate contribution? It was a small part of the work, so innate contribution was small.  []garymarcus 2 points3 points4 points 28 days ago(2 children) I guess this depends on how you define solving. But: You take out the innate part, and it no longer solves the cube.  []sanxiyn 1 point2 points3 points 27 days ago(1 child) I am all for retitling the post to Manipulating Rubik's Cube as you suggested. After retitling, innate contribution was small.  []garymarcus -1 points0 points1 point 27 days ago(0 children) That title would certainly help a lot, and reduce the importance of innate component, though elsewhere there is still a fair amount of carefully engineered innate detail of different sorts in the precise structuring of the visual system etc. It's not like it was a big end-to-end network fed from a bunch of sources that worked everything out for itself.  []simpleconjugate 12 points13 points14 points 28 days ago*(4 children) As time has progressed, your criticisms come off as \u201cbad faith criticisms. In this case you disguise problems with science and tech journalism as problems with OpenAI's communication of achievement. GDB is right, they never made any large claims outside being able to manipulate the cubes. It would be great to have people out there who are keeping conversation around AI grounded, but that doesn't seem to be your primary interest or goal.  []garymarcus 7 points8 points9 points 28 days ago(1 child) the problem here was with openAi's communication; i have been clear about that, posting repeatedly on twitter that result was impressive though not as advertised. here is an example since you seem to have missed it: https:twitter.comgarymarcusstatus1185680538335469568?s=21 no person in the public would read the claim of \u201cunprecedented dexterity as being restricted to cubes.  []simpleconjugate 4 points5 points6 points 28 days ago*(0 children) A change in title should be made for sake of honesty (social media isn't known for its in depth readings). However unprecedented dexterity is certainly a reasonable description of the impressive result. I also don't think that the same \u201cperson in the public would read your tweets and think that OpenAI achieved anything important. In this sense, you mischaracterized OpenAI's own claims and achievements while reporting their own failures to communicate.  You are doing great work out there by pointing out the flaws in the hype. But at the same time, it feels that your criticisms serve Robust.AI more than the public. As someone who think ML needs to be become more rigorous in reporting results, I think recent posts highlight things that journalist irresponsibly reported on as well as mistakes made by OpenAI. Suffice to say, lately I feel the same about both you and OpenAI as you feel about OpenAI and the \u201cperson in the public.  [][deleted] 28 days ago(1 child) [deleted]  []simpleconjugate 1 point2 points3 points 28 days ago(0 children) That seems like an unnecessary personal attack. There is a clear line between criticizing his ideas and attacking him. You crossed it.  []Rasko__ 0 points1 point2 points 26 days ago(0 children) Jesus Christ who cares  [+]SugarAdamAli comment score below threshold-13 points-12 points-11 points 28 days ago(0 children) This is all way too over my head, but I'll take your word for it Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 14708 on r2-app-01066b27922181a56 at 2019-11-17 07:41:59.953985+00:00 running 85b1b7a country code: US. "
  },
  "15":{
    "title":"[P] Ever wondered how to use your trained sklearn\/xgboost\/lightgbm models in production? We developed a simple library which turns your models into native code (Python\/C\/Java) : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,355 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Project[P] Ever wondered how to use your trained sklearnxgboostlightgbm models in production? We developed a simple library which turns your models into native code (PythonCJava) (self.MachineLearning) submitted 8 months ago by krinart Imagine that you trained your super accurate model using your favorite tools (Pythonsklearnxgboostetc.) and now the time has come to deploy your model to production for the greater good.  But consider the following scenarios: This where m2cgen comes in handy. It's a library that generates JavaPythonC code from trained ML models. Check it out: https:github.comBayesWitnessesm2cgen Post a comment!  []LegacyAngel 13 points14 points15 points 8 months ago(0 children) This would have been awesome at my old gig.  []perspectiveiskey 10 points11 points12 points 8 months ago(1 child) This is pretty cool because it's super relevant. It's on my list of things to read through and apply.  []krinart[S] 4 points5 points6 points 8 months ago(0 children) Any feedback would be much appreciated! Especially things to improve.  []Iamabandit 4 points5 points6 points 8 months ago(0 children) Thanks! This should be pretty useful. Hopefully I will be able to try this out soon.   []niloyghosh 2 points3 points4 points 8 months ago(0 children) This looks interesting. Will definitely check it out.   []lildarlin23 2 points3 points4 points 8 months ago(0 children) Awesome! I will definitely test this, keep up the good work!  []Invoker_is_my_city 2 points3 points4 points 8 months ago(0 children) great stuff dude  []KevMuriithi 2 points3 points4 points 8 months ago(0 children) This is definitely something to check  []noviceProgrammer1 2 points3 points4 points 8 months ago(3 children) What prompted you to create this project? it seems really cool, you're serializing models and writing an api to generate the code and it's subsequent parameters??   []s0ck_r4w 2 points3 points4 points 8 months ago*(2 children) Yeah, pretty much! Except you can even skip the serialization step and use Python object directly with m2cgen API What prompted you to create this project? I work at the company that does Data Science pipeline automation (not sharing the name of the company so that it doesn't count as an advertisement), and this is where I'm going to apply this project to address several challenges: UPD: and fun of course!  []noviceProgrammer1 1 point2 points3 points 8 months ago(1 child) I've done a fair amount of reading on pickling and would caution on using them for production. My research interest would actually be to learn how to serialize model objects and share them. Would love to bounce ideas with you. If you haven't, you can check out ONNX which isn't my work, but serializes deep learning models. May get some traction between different communities for feedback too :)   []s0ck_r4w 0 points1 point2 points 8 months ago(0 children) I've done a fair amount of reading on pickling and would caution on using them for production.  This is exactly why we're looking for an alternative to pickling to store our model pipeline so that it can be efficiently served later :)  If you haven't, you can check out ONNX which isn't my work, but serializes deep learning models Thanks for bringing this one up. I'll definitely take a look! May get some traction between different communities for feedback too That'd be just great!   []SelfishNeuron 4 points5 points6 points 8 months ago(1 child) MATLAB has this feature in form of their coder.In my opinion, this one of the biggest advantages of MATLAB, there are so many synergies when you can turn any high-level code into C or C++ and then integrate it with any environment.Or directly compile. It is good that Python gets such capabilities too.  []physnchips 7 points8 points9 points 8 months ago(0 children) It generates pretty lackluster code. I would definitely not say that MATLAB has an advantage here.  []s6x 6 points7 points8 points 8 months ago(9 children) Just commenting so I can come back to this later.  []Jonno_FTW 3 points4 points5 points 8 months ago(0 children) You can star repos on GitHub for the same effect.   []bighunt16 0 points1 point2 points 8 months ago(0 children) There's a save option underneath the post, in addition to starring on Github.  []iCogitoTabhiTohHoon -2 points-1 points0 points 8 months ago(0 children) Yup  []VotedBestDressed -2 points-1 points0 points 8 months ago(0 children) same  []noscopethebitch -2 points-1 points0 points 8 months ago(0 children) Ditto  []NowanIlfideme -2 points-1 points0 points 8 months ago(1 child) RemindMe! 1 month  []RemindMeBot 0 points1 point2 points 8 months ago(0 children) I will be messaging you on 2019-04-05 12:15:24 UTC to remind you of this link. CLICK THIS LINK to send a PM to also be reminded and to reduce spam. Parent commenter can delete this message to hide from others.  []inderjalli -2 points-1 points0 points 8 months ago(0 children) RemindMe! 1 month!  []ryanbuck_ -2 points-1 points0 points 8 months ago(0 children) I'm on it! (but just not yet..)  []ZETTAwith4TEEEES 1 point2 points3 points 8 months ago(0 children) Literally going to use this right now   []hawkxor 1 point2 points3 points 8 months ago(0 children) How does the speed compare against libraries like the java xgboost predictor?  []abinjoabraham 1 point2 points3 points 8 months ago(4 children) This is so cool. I was thinking how to implement my python model to production for autonomous driving. Mostly in the industry they use c++ for implementation.   []krinart[S] 1 point2 points3 points 8 months ago(3 children) Hope this helps! Just curious, which models did you use for autonomous driving?  []abinjoabraham 1 point2 points3 points 8 months ago(2 children) Like open pose for 2d human pose estimation.   []krinart[S] 1 point2 points3 points 8 months ago(1 child) For now we only support limited set of models, and open pose is not among them, unfortunately. However, we have a more or less simple mechanism to add support for new models. So if you are interested, you can try to implement open pose support. I will be happy to help you in that venture.  []abinjoabraham 0 points1 point2 points 8 months ago(0 children) I see. First lemme go through your repo. Actually I wanna do all these things but have limited time now as I am working on my master thesis. :)    [][deleted] 8 months ago*(4 children) [deleted]  []krinart[S] 0 points1 point2 points 8 months ago(3 children) Native R support is probably not possible, the only way to do this is through PMML. That's one feature request we are currently looking into.  [][deleted] 8 months ago*(2 children) [deleted]  []krinart[S] 0 points1 point2 points 8 months ago(1 child) We directly work with Python objects representing models, so there's no intermediary format.  []balls4xx 1 point2 points3 points 8 months ago(0 children) Starred.   []cyberdroidmann 1 point2 points3 points 8 months ago(0 children) Awesome!!!  []humanprogression 0 points1 point2 points 8 months ago(6 children) Can you go into a bit more depth on the provided example? What's the Boston dataset and what's the original model that's being transformed? How do those relate to the java output that's shown?  []krinart[S] 8 points9 points10 points 8 months ago(4 children) Boston dataset is one of the Hello, world! datasets provided by sklearn. The model is LinearRegression and it's trained using this dataset. The java output is exactly the same model, but represented as Java code instead of Python object.  []humanprogression -2 points-1 points0 points 8 months ago(3 children) So the java output is just some sort of netlist representation of the Python model?   []s0ck_r4w 7 points8 points9 points 8 months ago(2 children) I'm not sure what netlist is, but generally speaking - yes, the generated code is a representation of the Python model.  []NicolasGuacamole 4 points5 points6 points 8 months ago(1 child) Netlist is a term from computer engineering. In hardware description languages you specify a design at a high level, and then something akin to a compiler converts it to a 'netlist' which is a like a logic gate wire level description (which can be implemented in hardware). What they're getting at I guess is that the Java version is another version of the same model, which of course it is.  []s0ck_r4w 3 points4 points5 points 8 months ago(0 children) TIL! Thanks for explanation  []s0ck_r4w 3 points4 points5 points 8 months ago(0 children) Boston housing dataset is just a demo dataset built into the Scikit-learn library. It's been used for demonstration purposes. In that example a simple linear regression model has been trained using the Boston housing dataset. After that the trained model has been transformed into its Java representation. Basically generated Java code serves a purpose of a model object's predict() method, except it's self-sufficient and doesn't require Python runtime for evaluation.  []e_j_white 0 points1 point2 points 8 months ago(1 child) Cool! Will take a closer look later when I get home. Could this be used to train a model in sklearn and export it in a way that it could be used in Go? (I think Go handles natively compiled JavaC.)  []krinart[S] 2 points3 points4 points 8 months ago*(0 children) Sure, it is absolutely possible to add support for Go. It should be pretty simple, for example it took literally 120 lines of code to add Java support. Feel free to submit a PR, or we will add it some time later.  []Dagusiu 0 points1 point2 points 8 months ago(3 children) Does this create code that can run on a GPU?  []s0ck_r4w 0 points1 point2 points 8 months ago(2 children) The generated function makes a prediction against a single sample so, since there are no matrix computations, I'd say that it's unlikely that this code can benefit from running on GPU. On the other hand it can be ideal for making predictions in streaming fashion.  []Dagusiu 1 point2 points3 points 8 months ago(1 child) Oh, I thought this could be used for some kind of deep learning as well, like a CNN. I guess I misunderstood.  []krinart[S] 0 points1 point2 points 8 months ago(0 children) No neural nets, sorry. Only old school models.   []manueslapera -1 points0 points1 point 8 months ago(5 children) im a pure RPythongolang type of guy, can you explain how we can use the produced code? For example, if the output code is  public class Model { } How would one go into using it?. Im saying this because if I have to build the whole Java application to serve the predictions, i would rather go with openscoring instead.  []krinart[S] 6 points7 points8 points 8 months ago(0 children) Im saying this because if I have to build the whole Java application to serve the predictions That's the other way around. You choose the target language depending on your production environment. If you already have a Java environment that all you have to do is to call score() method of the generated model.  For example: That's it. If you have a C production environment, then generate C code for the model and use it similarly.  []s0ck_r4w 6 points7 points8 points 8 months ago(3 children) You're absolutely right, you have to build your own application around this code. Of course this is not as easy as sending a REST API request and perhaps openscoring is exactly what you need, but consider the following:  []manueslapera -1 points0 points1 point 8 months ago(2 children) Thanks for your reply, let me just add some comments about Openscoring. Cost and complexity of maintenance of yet another third-party service in your environment. Instead you can just embed your model anywhere. Well there is the cost of maintenance of m2cgen if i ever would like to update the model or use a non supported algorithm (<1 ms) latency raises some doubts  I have seen 3ms total latency with openscoring, which is bananas. It is optimized and well maintained, it supports TONS of different algorithms and its a mature project. How well this third-party service scales with data size? m2cgen allows to apply a model inside of a Spark job as a plain map function. You can do bulk or single requests with openscoring. It wont scale to parallellized computation (spark RDDs or anything beyond single node computation), so that is a point for mc2gen. However Id like to see the use case of training a random forest in a non spark environment and AFTER use it with spark. And well, besides, you could use pmml-evaluator (the internal library that handles model loading in Openscoring ) by itself with Spark (since the library is Java and so is spark). Consider a use case for embedded devices where call to a remote service can't be afforded.Yeah totally, for this use case mc2gen is great.  []s0ck_r4w 0 points1 point2 points 8 months ago(1 child) However Id like to see the use case of training a random forest in a non spark environment and AFTER use it with spark. This is actually a pretty common use case. Arguably (and subjectively) the Random Forest (as well as any other ML algorithm) implementation is quite far from being good in Spark. But it's not only about Random Forest, right?  []manueslapera 0 points1 point2 points 8 months ago(0 children) This is actually a pretty common use case. Never seen that, usually I implement spark pipelines when I have the required amount of data, and usually I want my models to benefit from the biggest dataset I can. I used RF as an example since its one of the few models mc2gen supports. As a side note, can you elaborate on why the RF implementation in spark is not good? Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 23533 on r2-app-0cf8a7879c94f994f at 2019-11-18 13:47:43.178841+00:00 running 85b1b7a country code: US. "
  },
  "16":{
    "title":"[D] Best practice and tips & tricks to write scientific papers in LaTeX, with figures generated in Python or Matlab : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,365 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[D] Best practice and tips & tricks to write scientific papers in LaTeX, with figures generated in Python or Matlab (self.MachineLearning) submitted 8 months ago by Wookai I'm working on a paper with some colleagues and I just remembered I had collected a series of tips & tricks to make paper writing more efficient, so I figured I'd share here: https:github.comWookaipaper-tips-and-tricks What are your best tips for collaborating on a paper and writing more efficiently? Post a comment!  []svldsmnn 40 points41 points42 points 8 months ago(12 children) I'd add that matplotlib2tikz is great: the plots are rendered by latex right from your data, and you have all the control over figure sizes etc. defined as variables in your latex document.   []Wookai[S] 6 points7 points8 points 8 months ago(3 children) Yes, it's a great way to do it, too! I remember having some issues with a pretty complicated figure and moving away from it because of that, but I'm not sure about the details now...  []Mefaso 1 point2 points3 points 8 months ago(2 children) There's some weird stuff where it adds forget plot to all plots causing it to crash and issues like that, but mostly they can be fixed quite easily by pursuing the tracebacks  []nschloe 5 points6 points7 points 8 months ago(1 child) matplotlib2tikz author here. I've just released 0.7.1 with a bunch of fixes for legends; perhaps those satisfy your use case.  []Mefaso 0 points1 point2 points 8 months ago(0 children) Oooo, cool to hear, will look into it :)  []bokan -2 points-1 points0 points 8 months ago(7 children) Could you possibly explain the pitch for Latex to me? It's become super common in computing recently now that Overleaf is a thing. Personally, it's hard to actually write anything when I have to worry about essentially designing a webpage too. I want to be able to focus on content, and often need fine grained control over the formatting afterwards. Am I missing something?   []impossiblefork 16 points17 points18 points 8 months ago(3 children) It's always been super common. Basically all maths, physics and computer science papers are written in LaTeX.  []Telci 3 points4 points5 points 8 months ago(1 child) Economics, too. Not only for papers but also for presentations.  []impossiblefork 1 point2 points3 points 8 months ago(0 children) nods A lot of older papers appear to be non TeX-y, for example the original Fischer-Black paper doesn't appear to be done in TeX. For actuarial stuff I've seen a bunch of old typewriter-y things and in economics I've seen a lot of word stuff with large spacing between the lines, but I think things are changing with TeX becoming more widespread. The Mack article on chain ladder estimates is old and looks like it's done in TeX though.  []bokan 0 points1 point2 points 8 months ago(0 children) Interesting. It seems to be growing into non-cs computing fields, then (HCI in particular).  []here_we_go_beep_boop 7 points8 points9 points 8 months ago(0 children) This is exactly the point of LaTeX, decoupling content from style. Where LaTeX gets hard is when you want to micromanage that style, you quickly disappear in arcana.   []r4and0muser9482 3 points4 points5 points 8 months ago(0 children) The whole point of latex is that you should concentrate on content and the publisher is responsible for the style. When writing in latex you should provide the minimal amount of information required to render your thesis correctly in any style the publisher later chooses.  []A_Philosophical_Cat 2 points3 points4 points 8 months ago(0 children) The magic of LaTeX is that you don't need to think about layout. You tell it this is a paragraph this should be in italics stick a footnote here, then it just generates a very consistent, professional looking document. The trick is to avoid micromanaging the compiler.   [][deleted] 8 months ago*(1 child) [deleted]  []Wookai[S] 6 points7 points8 points 8 months ago(0 children) Great suggestion, thanks! The only document I have fully using all these tips is my PhD thesis, which can be a bit long to be considered a MWE :) : http:vincent.etter.iopublicationsetter2015phd.pdf  I'll have a look at the source code though, it might still be a good example. If so, I'll link it in the README.  []doktorneergaard 6 points7 points8 points 8 months ago(1 child) This is great!  The one sentence per line thing is something I also just started doing, but mainly because I had a split screen setup with not enough real-estate to see sentences that come immediately after a very long sentence. It also comes in handy for commenting out specific stuff andor figuring out where an error is located.  However, the math notation section is really dependent on the author andor the audience. I've seen many versions on the bold andor italicized vectormatrix situation.  Forked :)  []Wookai[S] 2 points3 points4 points 8 months ago(0 children) Indeed, it is also very dependent on the field! My goal was mostly to share the system I picked as one possible solution. The most important is to pick one way and be consistent :)  []seraschka 14 points15 points16 points 8 months ago(8 children) To write centered equations on their own lines, do not $$...$$ (it is one of the deadly sins of LaTeX use). It works, but gives wrong spacing. Use \\begin{equation} or \\begin{align} instead. Doesn't \\begin{equation}... number equations by default whereas $$ doesn't?  []topologique 19 points20 points21 points 8 months ago(2 children) It does, but you can use \\begin{equation*} to avoid numbering  []seraschka 8 points9 points10 points 8 months ago(1 child) right, so shouldn't the suggestion be  To write centered equations on their own lines, do not $$...$$ (it is one of the deadly sins of LaTeX use). It works, but gives wrong spacing. Use \\begin{equation*} or \\begin{align*} instead?  []Wookai[S] 7 points8 points9 points 8 months ago(0 children) Good point, I'll make the change! Thanks!  []TheMiamiWhale 2 points3 points4 points 8 months ago(0 children) You can use an asterisk after equation to suppress the numbering. \\begin{equation*}.  Sorry for formatting but I'm on mobile and can't find the backtick.  []Raqato 0 points1 point2 points 8 months ago(3 children) Yes it does, but if you do not need an equation number you could use \\begin{equation*}... Also, if i recall correctly, \\[ ... \\] should give you equations on their own line just like $$...$$, without committing any sins.   []seraschka -2 points-1 points0 points 8 months ago(2 children) I am not 100% sure but I think that [ ... ] and $$...$$ lead to identical results as one gets converted into the other automatically.  []Raqato 1 point2 points3 points 8 months ago*(1 child) I don't think they are inter-convertible. You can check this on stack exchange for a nice comparison.  Edit: Fixed the link. Sorry, I was on mobile when I posted it.   []seraschka -1 points0 points1 point 8 months ago(0 children) the link doesn't work for me. When I remember though $$ is tex syntax and \\[ is latex syntax. So, it depends on your environment on what \\[ does in your case. But I think unless you specify anything in particular, there will not be much of a difference (of course I would still recommend \\[, was just saying ....)  []vic4ever 5 points6 points7 points 8 months ago(2 children) www.tablesgenerator.com is also helpful in creating Latex tables.  []Pfohlol 1 point2 points3 points 8 months ago(0 children) I've found the latex functions in Pandas to be decent as well. It's usually not perfect, but it's close enough to my desired table that I can fix it up by hand  []Wookai[S] 1 point2 points3 points 8 months ago(0 children) Cool! As someone else mentioned, templating engines like Jinja are also very useful to generate tables. You can have your Python code read the data directly, format it as you want, and then generate the final LaTeX code that you simply have to copypaste there.  []__alk__ 3 points4 points5 points 8 months ago(8 children) Thanks, looks pretty good actually.  []Wookai[S] 2 points3 points4 points 8 months ago(7 children) Thanks! The part about figures was the most helpful to me, especially to have consistent font sizes, etc. between figures: https:github.comWookaipaper-tips-and-tricks#creating-figures  []hielkew 0 points1 point2 points 8 months ago(1 child) The link to plot_utils.py is dead. Now I don't know how you do it :(.  I personally made use of rcParams to have consistent figures. Not 100 % reliable somehow.   []Wookai[S] 0 points1 point2 points 8 months ago(0 children) Sorry, I fixed it! Here's the link just in case: https:github.comWookaipaper-tips-and-tricksblobmastersrcpythonplot_utils.py  [+]__alk__ comment score below threshold-6 points-5 points-4 points 8 months ago(4 children) Do you write all your workresearch papers in LaTeX or you use some other 'less bloated' typesetters like Groff?  []hielkew 3 points4 points5 points 8 months ago(2 children) I believe in science, LaTeX is pretty much the standard. I actually don't know if groff is really that good. I am afraid you will quickly hit walls if you want to do something specialized or want to have more fine grained control. For what I have seen the way the math renders in Groff is also not very pretty compared to LaTeX.   []samesense 5 points6 points7 points 8 months ago(1 child) Ha. The standard for science. You must not collaborate with biologists or physicians.  []hielkew 1 point2 points3 points 8 months ago(0 children) Well at least they won't write a groff document.   []Wookai[S] 1 point2 points3 points 8 months ago(0 children) I do! It's very powerful, gives great-looking results, and is pretty much understood by everybody so easy to collaborate with. EDIT: math looks awesome, too :)  []seraschka 2 points3 points4 points 8 months ago(1 child) can call its ith column \\vxi (it is a vector, thus in bold) and one if its element x{ij}, not \\vXi and \\vX{ij}. I'd further suggest \\vX{i,j} instead of \\vX{ij} because if you have something like \\vX_{123} it will be unclear what that means ;)  []Wookai[S] 0 points1 point2 points 8 months ago(0 children) Sure, if you ever have a case where it might be unclear, add a ,. In my case, I always used single letters indices so it made the notation much lighter not having the ,.  []anterak13 1 point2 points3 points 8 months ago(0 children) not really a tip per se but I really like madoko (its a literate markdown variant and allows to export to PDF through latex and to htmlreveal.js)  The way it allows to mix code and mathmode with transformations is really cool http:madoko.orgreference.html#sec-pre  []johndoez01 1 point2 points3 points 8 months ago(1 child) Thank you!   []Wookai[S] 0 points1 point2 points 8 months ago(0 children) You're very welcome!  []M4mb0 1 point2 points3 points 8 months ago(0 children) Is there a guide on how to make the font size in an image equal to the font size of the paper? As far as I can tell, with matplotlib, one has to set figsize equal to the dimensions of the image in the paper, in inches. This is pretty annoying because once you change anything in the TeX (say you decide to make the image a bit biggersmaller) the font size end up not matching anymore....  []QuantumCakeIsALie 1 point2 points3 points 8 months ago(1 child) For complex and long documents like theses, you should consider using latexmk for compiling.  It's an excellent perl program that can compile the minimum number of times required ([pdf,la]{tex}, bibtex, even detect modified figures). It can even compile continuously, compiling the minimum required stuff each time a file changes. It's particularly best with a pdf reader doesn't lock the pdf and automatically reloads the modified file, like most Linux readers I've seen. See an example makefile using it here.  []Wookai[S] 0 points1 point2 points 8 months ago(0 children) Indeed, latexmk is really useful! I'll mention it when I update the doc.  []nuggetman415 1 point2 points3 points 8 months ago(1 child) I highly recommend LyX. It is a free word processor that renders out full LaTeX and abstracts away a lot of the nasty markup. It's been incredibly useful for me, especially when dealing with math and formulae with lots of variables and such. I'm not exaggerating when I say it saved me probably hundreds of hours dealing with actual LaTeX. It might take a little bit of getting used to the UI (at least, it did when I was in college a few years back) but I ended up getting really familiar with it and ended up using it for almost every course I took. Even put my resume through it.  []anuumqt 1 point2 points3 points 8 months ago(0 children) I don't think it works with a lot of packages. It also works poorly with collaborating who don't use LyX themselves. It is better to just learn LaTeX.  []lunaticneko 0 points1 point2 points 8 months ago(0 children) Recommend that if you have tons of things to process, a makefile should be a good idea. It makes sure that your scripts do not get ignored when you want to update figures. Plus, they are quite flexible and you can create different actions (compilation steps) for different script files. Done properly, this can reduce error and increase productivity.  []xayma 0 points1 point2 points 8 months ago(0 children) While you can use packages to round numbers you shouldn't have it as a default. You should know how many significant figures each of your results have and it will vary, an arbitrary rounding can give erroneous results when you have things like catastrophic cancellation in your code. (Even in the example it moves something that ends in .5 to .500 which is two orders of magnitude more precise). Also using too many of your own commands can ruin the style guides of journals if they provide it.   []brunhilda1 0 points1 point2 points 8 months ago(0 children) Gnuplot+makefiles.  []Wookai[S] 0 points1 point2 points 8 months ago(0 children) Thanks all for the great feedback and discussion, I'll update this thread once I push an update. If you're interested, there was a great discussion on HN as well: https:news.ycombinator.comitem?id=19425637  []saforem2 0 points1 point2 points 8 months ago(0 children) I really like the style of your thesis, I was wondering if you could share the .sty file or template that you used for it?  []SageBait 0 points1 point2 points 8 months ago(1 child) What's the best way to use wrapfig(https:www.overleaf.comlearnlatexWrapping_text_around_figures) to have my images in line? The link I posted just has a simple syntax and sometimes when I use wrapfig near a page break it fucks up my whole document.  []QuantumCakeIsALie 0 points1 point2 points 8 months ago(0 children) Wrapfig has always been finicky. I used to put some \\vspace before and after it and manually adjust them once the document is about finished.  []SageBait -1 points0 points1 point 8 months ago(1 child) Also how do you deal with collaborating with people who don't know LaTeX but use Word? Is there some sort of middle ground besides pandoc?  []xgfs 0 points1 point2 points 8 months ago(0 children) Overleaf tries to bridge the gap to such poor souls Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 18526 on r2-app-0efa2b46a0ca56c64 at 2019-11-18 14:13:44.285724+00:00 running 85b1b7a country code: US. "
  },
  "17":{
    "title":"[D] Positional Encoding in Transformer : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,035 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[D] Positional Encoding in Transformer (self.MachineLearning) submitted 2 months ago * by amil123123 Hey all, I was reading up the transformer paper https:arxiv.orgabs1706.03762. This architecture uses positional encoding which the attention layers ignore. I don't understand two things - Thanks ! Post a comment!  []mikeross0 9 points10 points11 points 2 months ago(6 children) The thing that always gets me with positional embeddings is that it is preferable to add them to the word embeddings instead of concatenating them. We already know the dimensions of the word embeddings are related to semantics. So why embed position into the semantic space instead of adding additional dimensions to represent position? If someone can provide a good intuitive explanation for that, you will get all my internet points for the day!  []pappypapaya 12 points13 points14 points 2 months ago*(2 children) In attention, we basically take two word embeddings (x and y), pass one through a Query transformation matrix (Q) and the second through a Key transformation matrix (K), and compare how similar the resulting query and key vectors are by their dot product. So, basically, we want the dot product between Qx and Ky, which we write as: (Qx)'(Ky) = x' (Q'Ky). So equivalently we just need to learn one joint Query-Key transformation (Q'K) that transform the secondary inputs y into a new space in which we can compare x. By adding positional encodings e and f to x and y, respectively, we essentially change the dot product to (Q(x+e))' (K(y+f)) = (Qx+Qe)' (Ky+Kf) = (Qx)' Ky + (Qx)' Kf + (Qe)' Ky + (Qe)' Kf = x' (Q'Ky) + x' (Q'Kf) + e' (Q'Ky) + e' (Q'K f), where in addition to the original x' (Q'Ky) term, which asks the question how much attention should we pay to word x given word y, we also have x' (Q'Kf) + e' (Q'Ky) + e' (Q'K f), which ask the additional questions, how much attention should we pay to word x given the position f of word y, how much attention should we pay to y given the position e of word x, and how much attention should we pay to the position e of word x given the position f of word y. Essentially, the learned transformation matrix Q'K with positional encodings has to do all four of these tasks simultaneously. This is the part that may appear inefficient, since intuitively, there should be a trade-off in the ability of Q'K to do four tasks simultaneously and well. HOWEVER, MY GUESS is that there isn't actually a trade-off when we force Q'K to do all four of these tasks, because of some approximate orthogonality condition that is satisfied of in high dimensions. The intuition for this is that randomly chosen vectors in high dimensions are almost always approximately orthogonal. There's no reason to think that the word vectors and position encoding vectors are related in any way. If the word embeddings form a smaller dimensional subspace and the positional encodings form another smaller dimensional subspace, then perhaps the two subspaces themselves are approximately orthogonal, so presumably these subspaces can be transformed approx. independently through the same learned Q'K transformation (since they basically exist on different axes in high dimensional space). I don't know if this is true, but it seems intuitively possible. If true, this would explain why adding positional encodings, instead of concatenation, is essentially fine. Concatenation would ensure that the positional dimensions are orthogonal to the word dimensions, but my guess is that, because these embedding spaces are so high dimensional, you can get approximate orthogonality for free even when adding, without the costs of concatenation (many more parameters to learn). Adding layers would only help with this, by allowing for nonlinearities. We also ultimately want e and f to behave in some nice ways, so that there's some kind of closeness in the vector representation with respect to small changes in positions. The sin and cos representation is nice since nearby positions have high similarity in their positional encodings, which may make it easier to learn transformations that preserve this desired closeness. (Maybe I'm wrong, and the approximate orthogonality arises from stacking multiple layers or non-linearities in the fully-connected parts of the transformer). tl;dr: It is intuitively possible that, in high dimensions, the word vectors form a smaller dimensional subspace within the full embedding space, and the positional vectors form a different smaller dimensional subspace approximately orthogonal to the one spanned by word vectors. Thus despite vector addition, the two subspaces can be manipulated essentially independently of each other by some single learned transformation. Thus, concatenation doesn't add much, but greatly increases cost in terms of parameters to learn.  []amil123123[S] 0 points1 point2 points 2 months ago(0 children) Wow , that's one hell of an amazing explanation :)  []Sinkencronge 1 point2 points3 points 2 months ago(0 children) It may have been already done for sure, but I haven't seen any paper about superiority of this approach to cross my scope yet.  I assume, in general you would probably don't want to blow up the dimensions of your word embedding even more than it has been already done in current SOTAs. One could take a dataset of utterances likeDog ate a cat, Cat ate a dog etc. and play around with addition, multiplication, convolution, concatenation and God knows what for the sake of comparison.  []Sinkencronge 3 points4 points5 points 2 months ago*(3 children) https:imgur.comkpW5n4p https:imgur.comkaADdQB  []amil123123[S] 1 point2 points3 points 2 months ago(2 children) Thanks for the response. I still have difficulty understanding about 1.So the first image seems good in explaining the position however what does the second image denote. Is it just because this function seemed to work well , that we went with it ?  []Sinkencronge 2 points3 points4 points 2 months ago(1 child) The second image just shows Euclidean distances between the added embedding for a given position.  The thing is that their choice of positional encoding function reflects not only absolute, but also the relative distance among tokens in sequences. In the paper they write: We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed off set k,PE_pos+k can be represented as a linear function of PE_pos. Unfortunately, I don't really wave a better way to elaborate it further in my mind at the moment. I'm sorry for that.   Answering your question, I personally think that they seem to simply pick up the first elegant solution to solve this problem.  However, I believe that there are a lot of more interesting ways to take advantage of positional encoding trick. I'm currently working on it for my own dataset.  []amil123123[S] 0 points1 point2 points 2 months ago(0 children) Thanks for the explanation, it was good !  []InsideAndOut 1 point2 points3 points 2 months ago(4 children)  []amil123123[S] 1 point2 points3 points 2 months ago(3 children) Thanks for the response!  What do you mean by symmetrical and decay sensibly ?( Sorry , but I am still a newbie in this )  []Sinkencronge 0 points1 point2 points 2 months ago(2 children) Symmetrical - see image 2.  Decay sensibly with distance - In this gif you can see entries in the distance matrix which have similar value [https:imgur.comap7NcUm1]  White dots are points (corresponding to different pairwise positions) which lay in some epsilon neighborhood from some threshold tau which changes over time from 0 to maximum distance.  []imguralbumbot 0 points1 point2 points 2 months ago(0 children) Hi, I'm a bot for linking direct images of albums with only 1 image https:i.imgur.com1jwYWbG.gifv Source | Why? | Creator | ignoreme | deletthis  []amil123123[S] 0 points1 point2 points 2 months ago(0 children) Ahh understood, thanks for the explanation!  []gdiamos 1 point2 points3 points 2 months ago(0 children) Papers usually post their final solution, not the path they took to get there. The sincos positional embeddings often seem unintuitive for this reason. I find it helps to try to reconstruct the path.  1) Why do transformers need positional embeddings at all? What would happen if you removed them completely? Why?  2) Why not use simpler and more obvious positional embeddings like literally the position index?  []Jean-Porte 0 points1 point2 points 2 months ago(0 children) Regarding to sin and cos representation, my hypothesis is that together, they fully characterize a position Also, the weights that are learned over this input might be analogous to weights of a Fourier transform and have nice theoratical properties but I have seen no formal work on this Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 15082 on r2-app-0201aba5cd0b54af7 at 2019-11-18 21:09:28.648649+00:00 running 160a165 country code: US. "
  },
  "18":{
    "title":"[Discussion] Agree or disagree: Data science jobs will undergo radical change with automated software in next 5 years. : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,264 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[Discussion] Agree or disagree: Data science jobs will undergo radical change with automated software in next 5 years. (self.MachineLearning) submitted 8 months ago by [deleted] https:www.forbes.comsitesforbestechcouncil20190301radical-change-is-coming-to-data-science-jobs I read this article from the link above. I tend to disagree with the notion that the data science job will be eliminated by auto machine learning software packages like data robot. I hate the idea of someone just throwing their data in and getting results. From my experience as a data scientist, it takes time and energy to analyze the data, implement feature engineering, and evaluating model performance based on the data.  As for \u201cwe are automating ourselves out of our job. Well yeah that's the idea. And that's not a bad thing. However, I think we automate one process to move on to automate something else, especially with all types of new data coming into the digital world. I do find it interesting how the author splits the new data science work into 5 areas that will take place at n 5-10 years. Do you agree with this assessment? Will the data scientist \u201cboom disappear? Will it be replaced by a more specific job? What do you think? If these changes are true, how does it impact our work? Post a comment!  []logicchains 256 points257 points258 points 8 months ago(31 children) I think https:stats.stackexchange.comquestions185507what-happens-if-the-explanatory-and-response-variables-are-sorted-independently is an example of why data science won't be automated away any time soon. Picking the right data to use, cleaning and validating it correctly, is at least as important as picking the right model.  Plus, no software is capable of trawling through the bowels of the organisation to find out the correct interpretation of the `Extra2` field on the `Sales` table that takes three values: TRUE, Error and null.  []stirling_archer 127 points128 points129 points 8 months ago(16 children) https:stats.stackexchange.comquestions185507what-happens-if-the-explanatory-and-response-variables-are-sorted-independently This needs a trigger warning.  []wintermute93 51 points52 points53 points 8 months ago(13 children) This has got to be one of the most egregious professional math fails ever. It's up there with that biomedical conference paper or whatever it was that invented the trapezoid method for approximating integrals.  []EngineeringNeverEnds 32 points33 points34 points 8 months ago(6 children) No, this is 1000x worse. At least the biomedical dude got it right. This is just... Wow. My manager says he gets better regressions when he does this.Uhm holy shit I'd like to shake the hand of someone that was that stupid but managed to get into a managerial position in charge of processing data. Mind blowing.  []VirtualRay 3 points4 points5 points 8 months ago(5 children) Lol at the assumption that there's any sort of meritocracy in the real business world  []EngineeringNeverEnds 2 points3 points4 points 8 months ago(4 children) In every environment I've ever worked, competency has been rewarded, albeit with some degree of error.  []VirtualRay 0 points1 point2 points 8 months ago(3 children) You are adorable, my friend https:daedtech.comprogrammer-skill-fetish-contextualized   []EngineeringNeverEnds 2 points3 points4 points 8 months ago(2 children) Competency is not limited to technical competency. You must also be helpful, communicative, enjoyable to be around, etc. Do you bring your team up? Those all factor into overall competency. Competency as I use it has to do with your probability of success if I drop you into a random team. Yes, your social skills matter.  Think of it like the difference between a very good student that does well on a math test vs a good professor. The professor also did well on the same test years ago, but they've got a lot of other skills that add to their overall competency.   []VirtualRay 0 points1 point2 points 8 months ago(1 child) Yeah, for sure. Although I think it's weird that men tend to exhibit more competency than women, and natives at a company tend to exhibit more competency than foreigners. And people who're really good at always keeping an eye on what the C-suite is prioritizing (and worm their way into projects in those areas) always seem to be more competent than the ones who just do what they're told and work hard every day.  []EngineeringNeverEnds 0 points1 point2 points 8 months ago(0 children) I explicitly stated there was some degree of error which explains the 1st point. (While biases exist, I guarantee you that an objective evaluation of competency will have a positive correlation with promotion probability among similar cohorts.) The second point though is really the heart of my previous argument. Someone that just keeps their head down, doesn't care to interact with other team members, employees on different teams, company leadership, etc. is in almost no way demonstrating leadership and teamwork competency which is an essential component of the value of an employee.  Why leadership though? It's typically the single most valued and rewarded attribute, often for good reason. Technical competency doesn't typically have as long of a path of advancement. Both are often important for most practical advancement paths.   []rpswanson3 7 points8 points9 points 8 months ago(2 children) Do you have a link?  []i-heart-turtles 18 points19 points20 points 8 months ago(1 child) I think this is it - almost 350 citations. http:care.diabetesjournals.orgcontent172152.abstract  []rpswanson3 2 points3 points4 points 8 months ago(0 children) Thanks!  []deathacus12 2 points3 points4 points 8 months ago(0 children) Simpson's method?  []zu7iv 1 point2 points3 points 8 months ago(0 children) At least they were right  []venustrapsflies 20 points21 points22 points 8 months ago(0 children) me going in: ha, how bad can it be? oh.  []C-Hoppe-r 4 points5 points6 points 8 months ago(0 children) I'm literally shaking.   []themiro 37 points38 points39 points 8 months ago(0 children) Holy shit.  Thank you, this was a breath of fresh air - my standards for data illiteracy had gotten too snobbish.   []shaggorama 13 points14 points15 points 8 months ago*(1 child) This exactly. With the democratization of ML techniques, there will be an increase in people with just enough knowledge to be really dangerous. There's a difference between finding a signal in data, finding a signal that is actually there (and knowing how how to demonstrate that), and even more so a signal that anyone gives a shit about.  []life2vec 7 points8 points9 points 8 months ago(0 children) Yes this. My PhD work was full of: statistically significant but substantively irrelevant :')  []jeanduluoz 28 points29 points30 points 8 months ago(3 children) Suggesting this false binary is misleading. It's not like it's AI or humans. AI will take over work on the margins, and humans will be freed to add more value on our margins. It like saying computers willwon't have a major impact on accounting jobs. Both are wrong - it's not like computers could replace accountants, but at the same time, accountant work and productivity is clearly impacted by new tools.  Creating this false dichotomy is just misleading or misunderstanding fundamental economics.   []FrenchCuirassier 6 points7 points8 points 8 months ago*(1 child) Data is also growing at an exponential rate. There will be more jobs in the future but most of it will be analyzing data, understanding, interpreting and explaining. Would some magical AI be able to interpret everything and understand it and then do everything? Sure, except, access-to-data is important, convincing the humans is important, and it would be like having that one genius academic who can't explain anything and can only accomplish so much without someone running out of funding. Not saying a singularity won't come in the future but it will likely be a lot of years later and much later than expected. There are other technologies that might be a closer-threat to existence, the economy, or humanity.  []jeanduluoz 1 point2 points3 points 8 months ago(0 children) Agreed for sure. Matrix network is an interesting application of the concept   []chatterbox272 3 points4 points5 points 8 months ago(0 children) I'm not sure the willwon't have a major impact dichotomy is what you actually mean to talk about. AIComputing is and will have a major impact on jobs like accounting, it will fairly significantly change what that job entails. Accountants will still exist, but probably fewer and doing different tasks to what accountants are doing right now. The end result is the same but the process will change drastically. Such is likely to be the result of applying AI to most fields as AI becomes more powerful. The lower skilled components of jobs will go away allowing many people to be replaced by an AI tool plus a smaller number of people with higher skill levels to do the high-skill components more effectively.  []MTMD36 4 points5 points6 points 8 months ago(0 children) This is my favourite link of all time  []dafaq1112 4 points5 points6 points 8 months ago(2 children) I don't know man. You'd be hard pressed to find examples of such exquisite stupidity.   []EatsAssOnFirstDates 11 points12 points13 points 8 months ago(1 child) 80% of data science is dealing with these situations. It's entirely commonplace. If a company harmonizes their data and gets a workflow going that allows it to be automated then good for them, they don't need data scientists, but it took a lot of data scientists to get there and will be short lived without a few around.  []IanCal 4 points5 points6 points 8 months ago(0 children) And even if you've got all your data sorted, then you start looking at integrating other data from outside and these problems come up again and again.  [][deleted] 1 point2 points3 points 8 months ago(0 children) I had to read it twice to understand this. Next time, If I have a unsolvable problem and want to show some progress, I will try this :-D  []EatsAssOnFirstDates 0 points1 point2 points 8 months ago(0 children) I think this is true but it depends on companies understanding this issue. I do biotech and the amount of underestimating PhD level scientists make around transforming an unlabeled excel sheets data into results is astounding.   []BeneficialArcher 45 points46 points47 points 8 months ago*(4 children) I think it's unlikely that most data science roles will be automated. The thing is that the bit that's feasible to automate with current technology (i.e. feed in well prepared data and get output) is already automated (e.g. If I have a data array X, I can already easily pass those to scikit learn or something and get some output). The hard bit is stuff like interpreting results, using appropriate methods, knowing what to with messy data, knowing what data to use, and as far I know there's no software that's anywhere near being able to take a vague problem and use all available data to solve it (e.g. here's all the data that our hospital has, use it to automatically classify diseases). Adding a nice GUI to the bit that's already automated isn't gonna change things. It wouldn't surprise me if a lot of companies try to automate data science by getting people who don't know much about statistics (and possibly combined with offshoring) to use automated tools in order to cut costs. But I suspect that they'll go back to hiring people who know more about machine learning after getting disastrous results for a few years (I've actually seen a couple of companies do this already). As a side note, I think it's plausible that salaries will fall a bit over the next few years, but I think this will be due to hype dying down and more people graduating with stats degrees, not automation.  []uber_neutrino 12 points13 points14 points 8 months ago(1 child) As a side note, I think it's plausible that salaries will fall a bit over the next few years, but I think this will be due to hype dying down and more people graduating with stats degrees, not automation. My bet would be that salaries rise as the use of ML opens up positions in other areas that put pressure on people with data science knowledge.  []Aaraeus 2 points3 points4 points 8 months ago*(0 children) Agreed. The number of people with maths and stats degrees in the UK hasn't increased by much.   []killver 90 points91 points92 points 8 months ago(18 children) I have not seen a single case where AutoML outperforms a skilled data scientist. Otherwise all Kaggle competitions would be won by an algo. Specifically, domain knowledge will always be important. I have tried several AutoML tools a few times and never could achieve results that satisfied me. So I strongly disagree.  []sid__ 49 points50 points51 points 8 months ago(5 children) I tend to agree with this statement regarding highly specialized applications of ML, but consider a normal company that does not wish to hire a 150k data scientist. If they can still get average prediction results from a normal software engineer who threw some data into an autoML engine, and called it using an api, I think that is good enough in a lot of cases. Especially if it is not some mission critical application. At the end of the day, I think software engineering as a discipline will be much harder to automate than data science.  []Gabe_Isko 6 points7 points8 points 8 months ago(0 children) They do wish to hire a 150K data scientist if they provide a suitable return on the investment for that salary. Making more powerful tools for datascience should only increase the demand for people who are skilled at using them. It is up to people practicing data science to not get stuck in one set of tools or one one framework, but to embrace change.  []killver 24 points25 points26 points 8 months ago(0 children) It depends of course on your definition of a data scientist and on the use case. But maybe let's put it like that: highly skilled data scientists can not be replaced, but mediocre ones that just apply their stack of 5 algos anyways can be replaced  []killver 2 points3 points4 points 8 months ago(0 children) You still need to know what you are doing also. I have seen people using SAS or other click modelers in ways not imaginable...  []ValiantlyShy 2 points3 points4 points 8 months ago(0 children) Hand crafted rules will be cheaper and more effective in these cases   []Nazka231 0 points1 point2 points 8 months ago*(0 children) If they don't wish to hire a 150k data scientist because it will be too expensive than it means they have a small business and simple math and stats on an excel spreadsheet will be well enough. Else they are just being blinded by money for not paying someone that can give them way much more than 150k and they will be the next dinosaures of their industry. Either way they are responsible for where their business goes.  []Refefer 11 points12 points13 points 8 months ago(7 children) I tend to disagree with this premise. Kaggle is not real life: the overt goal is rarely to maximizeminimize some metric at the expense of all other things such as complexity, productionization, etc. Most problems are, in fact, simple and can be solved with simple models which is where automl is very successful.  []killver 7 points8 points9 points 8 months ago(6 children) And you think AutoML produces simple model structures? If you want simple you can just go for linear regression.  []Refefer 4 points5 points6 points 8 months ago(5 children) They generally do, yes. Most Automl solutions are searches over simple models such as GLMs that have hp tuning and some feature selection. Folks often forget that Google basically stole the term AutoML to push their deep learning initiatives and crazy network searches and thus has become synonymous with complexity, whereas the field of AutoML is actually quite a bit larger (though not that much older). I'd recommend reading some of the actual papers on libraries like auto-sklearn to provide a bit more context if you haven't already.  []killver 0 points1 point2 points 8 months ago(4 children) I have already used auto-sklearn. Any recommendations for good papers in the field? As said, I have not seen any application that results in simple models with good accuracy. Also curious if you know of some applications where it really worked.  []Refefer 1 point2 points3 points 8 months ago*(3 children) Edit: Let me dig up some stuff for you. Efficient and Robust Automated Machine Learning from nips 2015 would probably serve as good background for problem setup. Here is an updated paper from icml 2018: https:ml.informatik.uni-freiburg.depapers18-AUTOML-AutoChallenge.pdf http:automl.chalearn.org is a great place for tracking current SOTA in Automl (last one was nips 2018)  []killver 2 points3 points4 points 8 months ago(2 children) I just don't like black boxes where I have little control over what is happening. To me it is just some hyperparameter tuning with Bayesian optimization or genetic programming. As said I have tried it a few times, results were usually lackluster and runtimes long.  []Refefer 0 points1 point2 points 8 months ago(1 child) Sure, I understand. What you're feeling is incredibly common. I fight this battle all the time in the data science community and at my daily job.  []killver 0 points1 point2 points 8 months ago(0 children) It's a bit different though. I also encounter this regularly that business units do not trust the AI predictions, but what I am referring to more is that I as a data scientist have more trust in algos that I design and understand. But yeah, it's kinda a mixture from both ends. Thanks for the links, will look into it.  []seanv507 2 points3 points4 points 8 months ago(3 children) I think kaggle competitions is precisely what could be one with an algorithm. If you look at the winning solutions it really is run lots of models and then ensemble the results \ud83d\ude00  []killver 4 points5 points6 points 8 months ago(2 children) You need always something clever though to win.  []nikitau 0 points1 point2 points 8 months ago(1 child) Yeah, but creme-de-la-creme SOTA performance is probably not needed most of the time. Those usually go beyond the breakeven point as far as man-hours are concerned for most companies.  []killver 0 points1 point2 points 8 months ago(0 children) I agree on that.  []addys 24 points25 points26 points 8 months ago(1 child) The level of abstraction is increasing, with more and more of the grunt work being automated. But the productivity is also rising.  A similar thing occurred with statistics and number crunching when spreadsheets arrived - what was once a career-making study became a one-weekend assignment for an intern. (such as the famous fish oil study91658-8fulltext) from the 70s). I don't think anyone would argue that statisticians disappeared, but their jobs certainly did undergo radical change with automated software, all for the better....  []Refefer 1 point2 points3 points 8 months ago(0 children) I also made the connection to spreadsheets and fully agree. The nature of the job will change as well as the number of practitioners, but it will largely be in the form of doing more with less  []MLTyrunt 19 points20 points21 points 8 months ago*(1 child) The less domain knowledge is needed to engineer valid, strong features, the higher the chance of automation. Think of computer vision, speech, and text mining (for some general corpora and tasks). The more domain specific and more important checking data quality (beyond trivial stuff), the lower the chance of automation. So yes, I think some things will be automated, whilst other, where the problem is not so standardized, the need for a data scientist will remain for a while. More practically speaking, I think the demand for data science savy professionals will not peak for quite some time. The need for a specialist, predictive analytics-data scientist will however peak due to these incremental successes in automation, in maybe 5 or 10 years. The data savy domain specialists will then take over again, which quite frankly, will properly quite often be former specialist data scientists being busy in one area over a longer time. And no: a phd in neural networks implies no long term job security, at all. Data science, for automatable standardized problems, will become a software engineering subfield. When the strong research eda and domain knowledge component is an integral necessity to build the model, it will become a skill of the data savy domain specialist.  Managing and creating self-learning AI systems, as reinforcement learning based ones, may however also be a technical specialists field for quite some time. This is part due to the state of usability in the tooling now, but also because they require the user to think of the design problem more like an educated engineer.  []Code_star 0 points1 point2 points 8 months ago(0 children) Damn I need to rethink my PhD then if no one needs an expert in neural networks  []scottishbee 16 points17 points18 points 8 months ago(0 children) I don't 100% agree with the article, but directionally I do see a shift coming.   Will an experienced data scientist always beat a standardized tool? Sure. But if the tool can get to 80% of the value at 20% of the cost, it'll open up a lot of problems to easy ML solutions. For example, a company I used to work for wanted to build a document classification product. There were two separate work streams: in one a data scientist went to work iterating on models, starting with simple TF-IDF and getting up to image classification and neural nets. In the other, engineers just threw documents against Google Cloud OCR to generate text, and then text and manual labels into AWS ML for bag-of-words classification. The data scientist ended up making the better model...9 months later. So for the main product, the team just used the out of the box solution, built the rest of the product (UX, labeling team, sales team, etc) and then swapped in the better model to go from 90 -> 95% accuracy. Not every company is going to be able to hire a data scientist and wait almost a year for them to produce.  I think the change is going to be very much like running servers or even databases. In the old days, every company needed to own and maintain servers. Every company needed database experts to set up, manage, and run queries. Mapreduce was custom built at many places. Now, no one does that except the infra companies (AWS, MS, Google, Oracle, Snowflake, etc). For 99% of the rest of us, that's a commodity and not a differentiator in our business.   Tech is littered with careers of people who though their field was too complex to be automated. I think in ~10 years there will be about the same number Machine Learning jobs as today, but 1000x more ML implementations. The only ones left doing this full time will be hard score specialists researching advances, the actual implementation will be through standardized packages that regular back-end engineers will be expected to set up.  []Screye 13 points14 points15 points 8 months ago(5 children) It is the same argument that I heard when Wordpress websites would put web developers out of the job.Such automated solutions will always be inferior to whatever the less mature, but better recent solution is.  Data Science is 10% algorithms, and the other 90% is everything around it. Data collection, Data set preparation, balancing biases, interpreting results and a lot more. I do think that low skilled jobs are going to get automated increasingly in every field, across the board, in the next few decades.This will apply to the lowest skill data science and software jobs too. But overall, the field should be fine. If anything, it would be doing great.  []bhindblueyes430 3 points4 points5 points 8 months ago(0 children) 10% algorithms but Data Scientists sell themselves like it's 75% of their effort and value.   []flextrek_whipsnake 1 point2 points3 points 8 months ago(3 children) 10% is pushing it lol, at least in my experience.  []Screye -1 points0 points1 point 8 months ago(2 children) ofc, I am exaggerating. The point itself stands tho.  []flextrek_whipsnake 2 points3 points4 points 8 months ago(1 child) No, I mean your number should be lower than 10%, not higher.  []Screye 0 points1 point2 points 8 months ago(0 children) :|  lol. Don't even know what to say to that.  []wolfpack_charlie 11 points12 points13 points 8 months ago(0 children) Didn't people say the same thing about web developer jobs in response to languages and libraries like javascript?  Seems like better accessibility only bolstered that job market, if anything  []Gabe_Isko 9 points10 points11 points 8 months ago(0 children) A data scientists job is less about applying matches to data, and more about communicating what the results of algorithms and maths mean when applied to a direct set of data. That is why everyone stresses a background knowledge of statistics - you can feed a bunch data into a neural net and get an answer but you get it I what are ultimately probabilistic terms. People are always predicting the death of coders and engineering due to automation, but I really disagree with that. If we create more valuable and powerful methods of automating data calculations, then a fundamental understanding of what is going on will be much more valuable and in demand. Even if the day to day work looks completely different.  []seraschka 7 points8 points9 points 8 months ago(0 children) This is BS. We developed statistics software in the last century and yet it hasn't replaced statisticians. We have developed machine learning to automatically derive knowledge from data and yet we haven't replaced data analysts ...  []random_forester 7 points8 points9 points 8 months ago(0 children) I don't anticipate any radical change, at least in the next 5 to 10 years. I do anticipate some gradual changes. The radical change will be self-improving algorithms. And it'll be abrupt - once you have the ball rolling, software will keep improving by itself, with no human interaction. I have not seen much work being done in this direction.  []donuon 6 points7 points8 points 8 months ago(0 children) At the end of the day business lines only care about making the best possible decision given the information available. The trade off will be on how much are they paying a resource per increase in predictive performance and how much profit does it drive. It will be much harder to automate top notch data scientists in areas where large profits can be driven off small increases in predictive performance. The smaller that gap is the less incentive there is to pay for top data science talent when a developer for much cheaper could just run the data through an autoML stack and give the business line information that is good enough to make a profitable business decision. I don't think the radical shift is going to come from automation, its going to come when business lines realize they don't need cutting edge to improve their operations.  []newjeison 5 points6 points7 points 8 months ago(3 children) I'm fairly new to machine learning. Can someone explain to me the difference between a machine learning engineer and a data scientist? I see the terms get thrown around a lot and assume that people use them interchangeably. edit: fixed typo  []gamerx88 2 points3 points4 points 8 months ago*(0 children) Both are very overloaded terms but I'll give you my best shot as a data scientist with experience in the military industrial complex and now the financial industry. ML engineers are basically software engineers who have some knowledge of ML algorithms and libraries. Primarily they deal with integrating models from researchersdata scientists into production systems. Data scientists are basically the people who come up with the solution or models for some business problems. And no,we do not necessarily come up with novel algorithms in our work. That sounds more like fundamental research which is usually done by researchers. Most data scientists that I know (including myself) are basically doing applied research, i.e we make use of those algorithms you see in your textbooks and papers to try to solve real world business problems. This is the core work for us, and not things like coming up with novel algorithms that can be published to NIPS, ICML, KDD, etc.  From time to time we do need to make incremental changes to existing algorithms to get them to work well on real world problems, and sometimes these are worthy and do get published at top conferences, but that's usually not the main priority.   [+]brereddit comment score below threshold-6 points-5 points-4 points 8 months ago(0 children) They tend to be the same person but some ML engineers don't have PHD's in data science.   []hadaev 3 points4 points5 points 8 months ago(0 children) Maybe, but cant we say same for all other jobs?  []c0cky_ 3 points4 points5 points 8 months ago(0 children) I think that with any major advances in technology it's not X being replaced by technology. It's X being complimented by technology. In many cases where society has predicted that humans will be completely removed from the equation they still exist at that current role but are augmented with advanced technology.  A good example of this is the automotive industry where (yes many jobs have been cut) but there are still humans working on the assembly line. Even with the most recent production issues at Tesla it's because in Elon's words the process was too automated which resulted in faulty engineering constraints.  []remimorin 4 points5 points6 points 8 months ago(0 children) Software developer (not data scientist) here. This kind of thing is like saying web developer will disappear with WordPress because anyone can now do a website.  Although it's true anyone can now do a 2008 like website we now expect more.  Automating data science... Maybe but this will be tools for the data scientist to do better, for more usage. I would not sweat a second. Software have automated and open-sourced more than ever and they need us more than ever to use it.  []TexasWes1212 2 points3 points4 points 8 months ago(0 children) If you work in a regulated industry (like Financial Services), until the automated solution can explain every step of the building process to an auditor - you are safe.   []dbspeakers 3 points4 points5 points 8 months ago(0 children) Automation is already here... If you use any of the premade models in keras for classification, you've automated part of your job. Look for frameworks and auto ml to do more lifting on training models, but there will always be an engineering need to define the problem and understand the data and how he relates to the problem.I personally think the role will be filled more by software developers than specialized data scientists. It's less work to get good production code from a developer cross trained into data science than the other way around. Of course, I feel that most of the code I see in data science isn't very good, but I work at a company that is all about clean code and test driving.  []ucbmckee 16 points17 points18 points 8 months ago(1 child) It's already happening. Many companies now hire 'clever' engineers who can use off the shelf libraries and tools instead of mathematicians or pure scientists who often can't code or engineer well and create significant production delays. Most use cases for machine learning are fairly standard. It's not about getting the best possible results, it's getting within 80% with 20% of the effort, certainly at first. If I were building a DS team today, which I am, I'd skew far more toward engineers than five years ago.  []bhindblueyes430 6 points7 points8 points 8 months ago(0 children) This. I've helped interns be more productive than PHD's just by focusing on solutions and not getting lost in the \u201cscience weeds  []leviathanxs 2 points3 points4 points 8 months ago(0 children) It can take days for a manager to even be able to express which problem they want to solve when they say they want X. I don't think an AI will be able to figure out what managers want to verify... You need data scientists to translate managers desires into specific concrete problems that can be framed into a math equation and then there is the whole data cleaning and interpretation issue that others have mentioned. Dumb AI can only automatize the model trainingfitting and some of the model selection, not the job of a data scientist.  []torpedoshit 2 points3 points4 points 8 months ago(0 children) Yes, most likely. What took a PhD to do 5 years ago is a high school project now. Frameworks, libraries, and open datasets will make things easier and faster with time.  There will be a need for human workers for the foreseeable future, but the numbers will continue to fall. Science is no exception.  []FullSharkAlligator 1 point2 points3 points 8 months ago(0 children) Data scientists will be more like technicians using automated tools properly instead of having to do a lot of things from scratch  []codeslingingslave 1 point2 points3 points 8 months ago(0 children) I think the future of work for machine learning practitioners will quickly separate into two groups: a very small and elite group that performs research and a much larger groups that use AutoML but whose jobs also deal more with data preparation (which gets automated also) and ML devops, supporting models in production.  []bobbruno 1 point2 points3 points 8 months ago(0 children) The way I see it, the bar for newcomers will be higher. Just knowing how to run models won't create more value than autoML (especially when considering time to value). Basic data cleaning can also be automated. What is nowhere near to be automated is:1) Framing a business item as an optimization problem. AutoML has nothing so far before this happens. 2) Knowing how to design a proper experiment. No autoML here either. 3) Setting up such experiment. Same as above. 4) Preparing and cleaning data in a way suitable for the problem. General filling nulls and discarding outliers is easy, handling those and creating good features is not. 5) Knowing what to do after you get the autoML result to have an edge over everyone else who can also get the autoML result. And this is what gives companies the advantage. 6) Havingcoming up with the right data to apply ML. Sometimes it's about finding the source, sometimes it's about creating the source. Mostly it's about getting creative with the data you have and how you frame the problem, and autoML doesn't help you here.  So, I see DS being replaced with autoML in non - core areas, where companies just want to not fall behind. That means juniors will have less opportunities. Companies will try to blindly apply autoML to core problems (especially if they don't understand why it does not work), and they will fail. That can either create jobs for skilled data scientists or it can create disappointment and canceled initiatives. I think it will be a bit of both.   []lightofaman 1 point2 points3 points 8 months ago(0 children) I tend to agree with you. Being a data sci demands a bunch of effort and analysis, which I believe cannot be automated  []caedin8 5 points6 points7 points 8 months ago(16 children) I think most of the people in here are living in a bubble. This has already happened. My company licenses an autoML solution and it produces better models than our data scientists in about 10 minutes. There is still plenty of work around feature engineering, cleansing, deploying, and testing but the actual data science specialization isn't really needed as much going forward. Our software engineers can do it. I'm training my currently employed data scientists to learn a full suite of software engineering skills along side their job to increase their probability of staying around. Cutting edge and research will of course need specialists, but that's not what most companies do   []Screye 13 points14 points15 points 8 months ago(1 child) My company licenses an autoML solution and it produces better models than our data scientists in about 10 minutes. Maybe your team needs to hire better data scientist. YMMV, but time series data in the non-NLP domain is one of the harder tasks in ML. Constructing a well informed domain specific PGM or finding a good graph structure for your LSTM make massive differences to the quality of a time series ML model. I can't imagine an auto-ML model out-performing either. If your Data Scientists were just cycling through every algo in sk-learn then I can see how Auto-ML can outperform them. Talking machines has a great podcast with Claudia Perlich - NYU2Sigma about how a lot of problems can be solved by just simply using Logistic Regression.But, the amount of time that they spend with preparing the data, feature engineering and deciding how to stage it makes for an extremely interesting Data Science problem. Just because the model isn't interesting, doesn't make it an easy or uninteresting Data Science problem.  []azraelxii 0 points1 point2 points 8 months ago(0 children) H20.ai has timeseries in driverless ai  [][deleted] 5 points6 points7 points 8 months ago(2 children) \u201cThis has already happened. My company licenses an autoML solution and it produces better models than our data scientists in about 10 minutes. What's the context of these solutions? Is this AutoML being used on \u201ceasy datasets (e.g. excel dataframe with response data) or is it being used on more challenging datatype problems that deal with image recognition or language sequence prediction?   []caedin8 8 points9 points10 points 8 months ago(1 child) Our use cases vary, but most common is time series based problems. Predicting demand, supply, prices, trends. We have a few production applications that determine some sort of classification. For example, based on the load shape for a customer determine if they use gas or electric heating.  We also have some clustering work with saying \u201cis new customer x most like which segment of the customers we already serve? We have some models based on customer turn over, \u201chow likely is this particular customer to leave us in x months if we let his contract roll over to market prices We have very few image and language prediction problems, but we do use NLP as features to the above problems and the autoML is much better than our resident data scientists at handling these features (n gram and covariance and collocations are determined automatically and fed into the model) Some of our upcoming work for image analysis involves maintenance for energy producing assets like solar farms using drone footage. But it's all in a proof of concept phase. We still manually do the image work as the autoML doesn't support image processing yet, but they plan on adding it soon.  []alexmlamb 0 points1 point2 points 8 months ago(0 children) Are the solutions that you're using built around generic AI types of problems like time series prediction, regression, classification, etc. - or are they designed around specific applications like customer turn-over prediction, demand forecasting, price forecasting, etc?  If their solutions are built around specific types of problems, then I could see this working. Otherwise I'd be concerned that you're just assuming your problem is a classificationregression problem, when the nature of the actual problem is more subtle (which has been the case for every problem I've seen in industry, including all of the problems that you mentioned).   []MrTwiggy 3 points4 points5 points 8 months ago(9 children) > There is still plenty of work around feature engineering, cleansing, deploying, and testing but the actual data science specialization isn't really needed as much going forward.  What exactly do you consider to be part of the data science specialization if not feature engineering, cleansing, deployingvalidatingmonitoringtesting? Data science isn't exclusively about running experiments with different models and picking the highest scoring ones. Someone has to formulate the problem, engineer the features, define the trainingvalidationtest objectives & methodologies so they align with the business goal, develop a solution for monitoringmaintainingtesting the product after deployment, etc.  Sure, you can try and take a software engineer and stick an autoML solution in his hands, but I doubt he will be able to reliably accomplish the above without the experience and knowledge of a data scientist.  []caedin8 1 point2 points3 points 8 months ago(8 children) I disagree. It takes a smart software engineer about a month to learn all of the best practices around building machine learning products. It takes a data scientist about a year to learn all of the best practices and skills around building successful software. It's just easier to train our software engineers to do data science work than it is to train our data scientists to do software engineering most of the time. Because the autoML does most of the hard work (chunking up data into different validation and train sets, runs hundreds of models and hyper parameters, surfaces good quality metrics about which models do well and which don't, provides an easy deployment framework and provides alerts monitoring and model drift metrics for deployed solutions) there is literally nothing the software engineer needs to do that he can't figure out in a trivial amount of time.  []BeneficialArcher 6 points7 points8 points 8 months ago(4 children) Because the autoML does most of the hard work (chunking up data into different validation and train sets, runs hundreds of models and hyper parameters, surfaces good quality metrics about which models do well and which don't, provides an easy deployment framework and provides alerts monitoring and model drift metrics for deployed solutions) I don't know about your company in particular, but at the places I've worked at this is the easy stuff and data scientists spend very little time doing it. Stuff like training hyperparameters has been feasible to automate since the 1990s (you don't need advanced software to do this, a place I worked at just wrote a python program to try every scikit learn classifiertune hyperparameters with validation sets). I've discussed this in a comment elsewhere, but data scientists where I've worked spent much more of their time interpreting results, doing feature engineering, working out how to deal with messy data, working out what data to use etc, and as far I know there's no software that's anywhere near being able to do this. If this stuff is what your software engineers do, then I'm pretty sure that what they're doing is what most other companies call data science.  []caedin8 6 points7 points8 points 8 months ago(3 children) We use DataRobot. Created by a swath of top kagglers.  It costs about the same as one full time data scientist but it is much more valuable. The entire team is more productive, and yes it does a decent job at most of those tasks you listed, and does them better than our DS would.  []BeneficialArcher 2 points3 points4 points 8 months ago(2 children) Based off of this: We use DataRobot.  And this reply to made to someone else: You are also forgetting that software engineers require a heavy math background and are well versed in statistical and probability theory. I'm pretty sure now that the people working at your company that you call software engineers are what most companies call data scientists (the stuff that datarobot does is exactly the kind of stuff that our data scientists have already automated). In that case we're actually in agreement in that the kind of people who have the title software engineer at your company are unlikely to be automated soon.     [][deleted] 8 months ago(1 child) [deleted]  []BeneficialArcher 0 points1 point2 points 8 months ago(0 children) At the company I worked at where we builtused quite a large number of automated ML tools, I generally found that the tools helped people get stuff done more quickly, but didn't help them correctly do anything that they couldn't already do (more slowly) without these tools. i.e. they didn't let the people at our company who didn't know much about mathstats do some of the complicated stuff that needed advanced stats knowledge. One thing we found though was that the tools often let our data analysts get into the danger zone without realizing it. i.e. they'd get some data, feed it into the computer, and the computer would dutifully process the input (regardless of what it was) and output some nice numbers, but on closer inspection they'd often made very basic statistical mistakes that invalidated all their results (e.g. p-value hacking). It's hard for me to make any specific comment on your BI analysts since I don't know exactly how much mathstats they know.  []Zankoqt 6 points7 points8 points 8 months ago(2 children) You make it sound like all statistical reasoning ever can be learned in a month by a SE.   []caedin8 2 points3 points4 points 8 months ago(1 child) No, just saying that all the hard work on statistical reasoning is automated. They really need to just learn what the metrics mean and how they are calculated. You are also forgetting that software engineers require a heavy math background and are well versed in statistical and probability theory. They may have never used ROC AUC before but they'll know it tomorrow if you ask them to go figure it out. They have the background.  []Zankoqt 1 point2 points3 points 8 months ago(0 children) Interpreting ROCAUC is a thing, understanding the role of ties in data while doing survival analysis (just to give an easy but not trivial example) is another in my experience. It's an easy thing to understand if someone says to you that it's a problem but it's not the first thing that come to mind to a SE. In my albeit limited industry experience (we teach and do some prototypical work for companies in fields like data science, optimization, CFD and automation) I've found way harder to teach SE of some intricacies of statistical reasoning than say functional programming to data scientists. And statistical failures are, usually, harder to spot, so I had quite a few example of software people thinking everything it's fine while the models they deployed or the data they were collecting was not. But ofc it's really anecdotal. I would really love to teach to a bunch of SEs so gifted that they get at the first try everything from the exponential family in MLE to bayesian non parametrics :) I'm jealous!  []ednaiss 0 points1 point2 points 8 months ago(0 children) Can you share what AutoML platform worked for you? or suggest what good AutoML platforms are out there? Thanks  []nxpnsv 0 points1 point2 points 8 months ago(0 children) In my experience, stating the problem then formulating an objective function that solves the problem and finding relevant data is not easily automated... nor is presenting a relevant interpretation of the results. ML just a small part of DS, but even the ML part is hard to automate, perhaps for simple problems...  []vzq 0 points1 point2 points 8 months ago*(2 children) Considering like 80% is data massaging, I seriously doubt it.   []NaBrO-Barium 4 points5 points6 points 8 months ago(1 child) Date massaging, this guy gets it.  []vzq 0 points1 point2 points 8 months ago(0 children) Lol thanks, awkward typo.   []uber_neutrino 0 points1 point2 points 8 months ago(0 children) Machine learning is a new tool. If anything the number of people employed in this area will go up, possibly a lot, to use this new tool.  []Thalesian 0 points1 point2 points 8 months ago(0 children) If machine learning reaches the level where it can automate the coding process, then the revolutions in the economy at that point will be much larger than the concern for data scientists posited here.   []oarabbus 0 points1 point2 points 8 months ago(0 children) As a data analystscientist one of the most important things is to understand the nature of the tablesdata model. Is this a snapshotfact table? Does it hold only the most recent status? I think it will be quite some time until AI can actually understand your data model and write SQL queries, do feature engineering, etc for you.  []Ikuyas 0 points1 point2 points 8 months ago(0 children) You would still need to learn software development while doing the data science things. But if you are already in the industry, it will be a lot easier to have many software engineer peopleexperts around you anyway. You just need to learn how to make API and so on. Right now the data scientists just make a report.  []ditomax 0 points1 point2 points 8 months ago(0 children) Possibly depends on the domain. In some domains AutoML like solutions will soon dominate. That is simply the way we develop our technology. But abstraction over abstraction...  []ProfessorPhi 0 points1 point2 points 8 months ago(0 children) I think data science roles won't be automated, but as productivity goes up with autoML, you'll find far smaller DS teams required. There'll be no place for average data scientists in the next few years and DS teams will become more software focussed with technical and mathy leads.  []kivo360 0 points1 point2 points 8 months ago(0 children) Use a little creativity and you'll find that any job can be automated. In fact, will probably be automated.   []azraelxii 0 points1 point2 points 8 months ago(0 children) Disagree for a few different reasons. Main reason being that issues at least where I work are getting the model engineered, deployed, and well specified from users. So sure you can automatically generated the algorithm but all the work is in explaining it, documenting it, proving it works, and getting into use. Then on top of that you need to have ways to automatically test it in prod and retrain.  Since systems and requirements change all the time these problems are evergreen. Automated software at best simply reduces demand for data scientists but since their developers know this they charge a rather large amount. The decision to have 5 data scientists or 2 and a automated licence are going to depend on a lot of things- it's not a either or.  []Captain_Flashheart 0 points1 point2 points 8 months ago(0 children) Yes and no. Yes, you can definitely automate some of it, and yes - a good part of DS jobs now can be moved to analytics developers. Which is not a new problem, seeing it gets mentioned here a lot as well as on rdatascience.  But no, these are still small ideas, and to see industry-wide adaption of highly efficient automation tools for ML it will take a couple of years more. It's not going to come from a start-up either. Google, MS, or the like - perhaps one of the old companies like IBM as well, will commercialize it with a second-to-market.  The core of the article forgets one thing. Machine power still takes time and money. Honestly - the limiting factor for a lot of our work, and I'm sure people here will agree - is computing time. Analyzing a problem and defining an approach does not take a lot of time in the grand process.   []victor_knight 0 points1 point2 points 8 months ago(0 children) Partially agreed. However, I also believe human plumbers will still be around for centuries.  []Jollyhrothgar 0 points1 point2 points 8 months ago(0 children) Tech jobs will always be undergoing radical change. Data science as a field, like all fields, will put the human where they do the most good. We have to adapt and change, or risk irrelevance.  []nnexx_ 0 points1 point2 points 8 months ago(0 children) If you think autoML is going to replace your job as a DS, you are doing it wrong. A DS is much much more than fitting and hyper parameter tuning...  []fiddlewin 0 points1 point2 points 8 months ago(0 children) To some extent, I agree with the original article but not your comment about autoML. None of the current autoML tools works very well to the point that someone without a deep understanding of ML can simply build a model by clicking some buttons, and I don't think they ever will. The autoML tools would only help someone who knows ML to quickly bootstrap a model and manullay finetune later. But I do agree the emerging mature libraries on all kinds of common ML tasks will eliminate some of the needs for really hard-core math and stats people. That being said, you do need someone who understand the models rather than those who only completed MOOC courses. This is similar to the case of software developers, where you would have all the modern frameworks like angular and react, which some beginners can pick up by completing a one week online course, but you still need senior devs who understand the nifty details and can tune performance and fix problems.  This brings me back to the 90s or early 2000s when people were scared that the RAD tools like Delphi and VB would make 'everyone a programmer' and less need for devs. But guess what happened then. :)  []chinri1 0 points1 point2 points 8 months ago(0 children) It depends - a lot of the data science work in the world is the kind where you win just for showing up, because literally no one has ever tried data science or machine learning on X. In most cases there's a huge amount of low-hanging fruit that linear or logistic regressions can pick up on, and an automated system can easily get you that far, or most of the way there. For many business cases that's good enough, and a trained data scientist may not add enough value to justify the cost. However, once the easy fruit is picked, you need someone who knows what they're doing to decide what comes next. This is all assuming you can trust your data. The other side of data science is about finding problems hidden in plain sight. The real science work is about holding your imagination accountable to your statistics, and exercising deep skepticism of anything that looks like signal. There absolutely will be a machine learning tab in Excel some day, but it won't make any difference if nobody understands that e. g. sometimes artifacts generalize just as well as signal, or that there is no single right way to fill in missing data. I think the part that this article gets right is that the field of data science won't go away, but it will be replaced by sub-specializations, just as computer science broke into programming languages, operating systems, networks, theory, databases, and computer vision machine learning.  []Retrodeathrow 0 points1 point2 points 8 months ago(0 children) I call BS. Its a puff piece about how one day AI will be intuitive.  []mimighost -1 points0 points1 point 8 months ago(0 children) There is, IMO, a much better article about this topic, as linked below: https:veekaybee.github.io20190213data-science-is-different Here is my observations: Data Scientist is a vaguely defined role as comparing to Software Engineer. Some DS works as data engineers; some works as Data Analyst. Their skill set vary in great degree, so do their responsibility. IMO, it will be less and less useful for company to host a generic Data Science job family, instead it will be replaced with several more focused and clearly defined sub categories.  The current data science training offered by either universities or other job training institutions aren't very useful in tackling real world business problem. You can say that is true for almost all professions, but in case of Data Science, it is particularly severe. The courses are about algorithmsmodelsfancy visualizations, but in reality it is probably more about SQLsfinding problem in your datasetstory telling, etc. Simply put, the gap is HUGE. Thus there is now a Junior saturation in this industry. But I don't think people need to panic about job prosperity if you are already under the DS umbrella. There is plenty of chances to further your career, just as those 5 categories this article listed.  []tataiermail -2 points-1 points0 points 8 months ago(0 children) Agree Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 26734 on r2-app-00adc56719ff32c7b at 2019-11-18 13:18:04.749216+00:00 running 85b1b7a country code: US. "
  },
  "19":{
    "title":"pappypapaya comments on [D] Positional Encoding in Transformer",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,227 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[D] Positional Encoding in Transformer (self.MachineLearning) submitted 2 months ago * by amil123123 Post a comment! view the rest of the comments\u2192  []pappypapaya 12 points13 points14 points 2 months ago*(2 children) In attention, we basically take two word embeddings (x and y), pass one through a Query transformation matrix (Q) and the second through a Key transformation matrix (K), and compare how similar the resulting query and key vectors are by their dot product. So, basically, we want the dot product between Qx and Ky, which we write as: (Qx)'(Ky) = x' (Q'Ky). So equivalently we just need to learn one joint Query-Key transformation (Q'K) that transform the secondary inputs y into a new space in which we can compare x. By adding positional encodings e and f to x and y, respectively, we essentially change the dot product to (Q(x+e))' (K(y+f)) = (Qx+Qe)' (Ky+Kf) = (Qx)' Ky + (Qx)' Kf + (Qe)' Ky + (Qe)' Kf = x' (Q'Ky) + x' (Q'Kf) + e' (Q'Ky) + e' (Q'K f), where in addition to the original x' (Q'Ky) term, which asks the question how much attention should we pay to word x given word y, we also have x' (Q'Kf) + e' (Q'Ky) + e' (Q'K f), which ask the additional questions, how much attention should we pay to word x given the position f of word y, how much attention should we pay to y given the position e of word x, and how much attention should we pay to the position e of word x given the position f of word y. Essentially, the learned transformation matrix Q'K with positional encodings has to do all four of these tasks simultaneously. This is the part that may appear inefficient, since intuitively, there should be a trade-off in the ability of Q'K to do four tasks simultaneously and well. HOWEVER, MY GUESS is that there isn't actually a trade-off when we force Q'K to do all four of these tasks, because of some approximate orthogonality condition that is satisfied of in high dimensions. The intuition for this is that randomly chosen vectors in high dimensions are almost always approximately orthogonal. There's no reason to think that the word vectors and position encoding vectors are related in any way. If the word embeddings form a smaller dimensional subspace and the positional encodings form another smaller dimensional subspace, then perhaps the two subspaces themselves are approximately orthogonal, so presumably these subspaces can be transformed approx. independently through the same learned Q'K transformation (since they basically exist on different axes in high dimensional space). I don't know if this is true, but it seems intuitively possible. If true, this would explain why adding positional encodings, instead of concatenation, is essentially fine. Concatenation would ensure that the positional dimensions are orthogonal to the word dimensions, but my guess is that, because these embedding spaces are so high dimensional, you can get approximate orthogonality for free even when adding, without the costs of concatenation (many more parameters to learn). Adding layers would only help with this, by allowing for nonlinearities. We also ultimately want e and f to behave in some nice ways, so that there's some kind of closeness in the vector representation with respect to small changes in positions. The sin and cos representation is nice since nearby positions have high similarity in their positional encodings, which may make it easier to learn transformations that preserve this desired closeness. (Maybe I'm wrong, and the approximate orthogonality arises from stacking multiple layers or non-linearities in the fully-connected parts of the transformer). tl;dr: It is intuitively possible that, in high dimensions, the word vectors form a smaller dimensional subspace within the full embedding space, and the positional vectors form a different smaller dimensional subspace approximately orthogonal to the one spanned by word vectors. Thus despite vector addition, the two subspaces can be manipulated essentially independently of each other by some single learned transformation. Thus, concatenation doesn't add much, but greatly increases cost in terms of parameters to learn.  []amil123123[S] 0 points1 point2 points 2 months ago(0 children) Wow , that's one hell of an amazing explanation :) Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 11998 on r2-app-064214287fe441450 at 2019-11-18 21:57:36.385882+00:00 running 160a165 country code: US. "
  },
  "20":{
    "title":"What is your opinion : Why is the concept of Renormalization Group Transformation and Phase Transitions not so popular in ML ? : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,394 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  What is your opinion : Why is the concept of Renormalization Group Transformation and Phase Transitions not so popular in ML ? (self.MachineLearning) submitted 3 years ago by jocomoco This paper shows a connection (Renormalization Group Transformation) between physics and ML : http:arxiv.orgabs1410.3831  I have post-grad educational background in physics and ML, and I am really fascinated by this connection (between physics and ML). Is it only me ?  What are your opinions about this paper ?  What is your opinion about the idea that phase transitions are important for brain functioning and also for deep learning?  This blog post goes into this topic too : https:charlesmartin14.wordpress.com20150401why-deep-learning-works-ii-the-renormalization-group  I am kinda surprised that this connection does not get bigger attention. Why not ? I wonder. Opinions ? Post a comment!  []NichG 14 points15 points16 points 3 years ago(1 child) I'm a soft-condensed-matter physicist who transitioned over into ML. I've worked with algorithms based on insights from RG, and I've worked with people who do analytical RG. My take on RG is that, unfortunately, while it's a great source of insight into thinking about asymptotic behaviors, infinities, emergence, etc - actually doing anything with RG takes a lot of time, a lot of paper, and is extremely hard to automate algorithmically. In particular, figuring out the right way to make secular terms go away and figuring out the right parameterization so that the number of parameters remains bounded as you perform the coarse-graining both seem to just require really good mathematical intuition unique to each case. If you back off from doing full RG and just talk about scaling methods like method of multiple scales or power-counting, its more amenable to automation, but its still a method that requires you to have a prescribed model for your system.  So I'd say, its one of those things that's potentially really useful to know about in terms of developing intuitions, but hard to actually directly use to obtain results.  I've tried to turn some of the underlying ideas into e.g. inspiration for loss functions and network architectures, and there's definitely something to be gained there, but I can't see myself sitting down and trying to literally work out the RG for a given network architecture or data set.  []jocomoco[S] 1 point2 points3 points 3 years ago(0 children) Interesting, thanks for the input, NichG.  []tscohen 7 points8 points9 points 3 years ago(1 child) I find this connection intriguing as well, but what is lacking is either an empirical prediction about neural representations, learning, etc., that we can test, or a new architecture learning algorithm that improves results on an actual learning problem. Until then it's just a mathematical analogy, and most ML researchers will (rightly) say meh to that. When this paper was released, it actually got quite a lot of attention, including popular media articles about it. It's a fascinating idea that somehow the abstraction that neural nets are thought to achieve is obtained by the same method physicists use to derive abstract or large scale variables of a physical system. But right now its not even clear how this could be tested.  It is also a reasonable idea that mathematical tools developed by physicists can be used in ML, and this has happened many times already (e.g. mean-field approximations, Bethe approximation, etc.). The fact that RG hasn't proven itself yet in this regard can mean that either it actually isn't all that useful for ML, or that we haven't tried hard enough (there's only a limited number of researchers with physics + ML skills, and the paper is only 2 years old...) So if you have any ideas on how to actually do something useful with RG theory, by all means work on it. I think at the very least the growing theoretical DL community would be interested.  []jocomoco[S] 1 point2 points3 points 3 years ago(0 children) But right now its not even clear how this could be tested. Hmm... what do you mean tested ? What needs to be tested here ?  []schmook 7 points8 points9 points 3 years ago*(1 child) There are some works in Statistical Physics concepts applied to ML. I don't remember any particular application of the renormalization group besides the one you linked, but there surely are analysis of the phase diagram of ML algorithms from the point of view of stat-phys and LOTS of phase transitions and critical phenomena. Take a look at works by Yoshiyuki Kabashima, Hidetoshi Nishimori, Manfred Opper, David Saad, etc.  Nishimori has a book called Statistical Physics of Spin Glasses and Information Processing in which he teaches how the statistical physics of disordered systems can be applied to understanding different kinds of algorithms: error-correcting codes, associative memories and shallow neural networks (it was written some time ago...). The same kind of reasoning could be applied and has been applied to other algorithms, though it gets tougher as the algorithms get more complicated. My PhD advisor worked with David Saad deriving the phase diagram of error correcting codes in the late 90s. There's a critical phase transition there and it can be shown that the fact that close to the transition line the complexity of the algorithms jumps from polynomial to exponential is related to the physical phenomenon known as critical slowing down. Recently Kabashima derived new learning algorithms for the Compressed Sensing problem and used statistical physics techniques to analyse it's phase diagram, showing that the frontier between the area where the solution is unique and the area where the solution is not unique is a actually a phase transition line. Opper and Nishimori both has a bunch of work on neural networks from the quenched disordered system point of view, including analysis on how fast does it learn for many algorithms. This work on neural networks was done in the late 90's and I don't know if it continued. It seems very difficult to apply to deep neural networks but someone must have tried. The work in Compressed Sensing is pretty hot and very active.  Take a look on Nishimori's book and Manfred Opper's Advanced Mean Field Methods.  Also Marc Mezard's Information, Physics and Computation. This work you linked I still consider a theoretical curiosity which still doesn't elucidate much. But it should be explored further. There's also a cultural divide where people from Physics like us frequently feel satisfied in deriving a phase diagram, looking at it and saying oh look, isn't it neat? I understand better this problem now. And the ML researcher will turn at you and ask oh, you do?? How does that help me build better models and better learning algorithms? It doesn't? Then it isn't useful, don't waste my time.. Which is a fair point of view. Physicists and ML researchers often have different stakes in this and some things that are central for one are not important for the other. For the physicist sometimes the mere analogy is a deep realization worth exploring, because it gives new meaning and understanding even to physical problems. For the ML researcher the mere analogy is too little, there has to be a consequence on the application side. They are in this ride not for neat theoretical puzzles but for actually building algorithms that can advance how machines can actually learn. We have to recognize this sometimes and try a little bit harder to provide actual results for them, not just for us.  []jocomoco[S] 0 points1 point2 points 3 years ago(0 children) There's a critical phase transition there and it can be shown that the fact that close to the transition line the complexity of the algorithms jumps from polynomial to exponential is related to the physical phenomenon known as critical slowing down. Yeah, somehow this is what I was thinking about, thanks for pointing it out, and thanks for the detailed answer !  []CultOfLamb 4 points5 points6 points 3 years ago(1 child) Keep an eye out for Santa Fe's Complexity Explorer. It will feature a course on renormalization soon and may touch on the connection between renormalization and machine learning. The course is by Simon DeDeo. http:www.santafe.edumediaworkingpapers15-06-017.pdf Optimal high-level descriptions of dynamical systems To analyze high-dimensional systems, many fields in science and engineering rely on highlevel descriptions, sometimes called \u201cmacrostates, \u201ccoarse-grainings, or \u201ceffective theories. Examples of such descriptions include the thermodynamic properties of a large collection of point particles undergoing reversible dynamics, the variables in a macroeconomic model describing the individuals that participate in an economy, and the summary state of a cell composed of a large set of biochemical networks. Often these high-level descriptions are constructed without considering the ultimate reason for needing them in the first place. Here, we formalize and quantify one such purpose: the need to predict observables of interest concerning the high-dimensional system with as high accuracy as possible, while minimizing the computational cost of doing so. The resulting State Space Compression (SSC) framework provides a guide for how to solve for the optimal high-level description of a given dynamical system, rather than constructing it based on human intuition alone. In this preliminary report, we introduce SSC, and illustrate it with several information-theoretic quantifications of \u201caccuracy, all with different implications for the optimal compression. We also discuss some other possible applications of SSC beyond the goal of accurate prediction. These include SSC as a measure of the complexity of a dynamical system, and as a way to quantify information flow between the scales of a system.  []jocomoco[S] 2 points3 points4 points 3 years ago(0 children) This is gold : https:twitter.comComplexExplorer thanks for pointing this out ! I found there for exemple this : https:youtu.beasw9M8RBcsU a 1 hour video about the StatPhys-ML connection  [][deleted] 2 points3 points4 points 3 years ago(2 children) I used fermionic Renormalization Group (quantum field theory) to study phase transitions in Hubbard-like systems. I applied to a ton of data sciencemachine learning positions, mentioned the parallels between the fields and how ML might benefit from RG analysis, and didn't hear shit. Despite the high horses data people seem to sit on, I think RG is too complex an idea for them to grasp.  []serge_cell 1 point2 points3 points 3 years ago(1 child) That is strange. May be you didn't want to do coding? Most of ML industry require implementation of methods working with real data, not just theoretical analysis.Then both of my last workplaces were hiring the was a problem to find ppl who are both fluent in coding (at least python) and can do basic matrixtensor analysis. The latter was biggest problem.  []jocomoco[S] 0 points1 point2 points 3 years ago(0 children) Yeah, I heard this too, the combo of coding skills + math skills is hard to find.  []WormRabbit 1 point2 points3 points 3 years ago(1 child) What are the theoretical models and nontrivial implications driven from that assumption?  []jocomoco[S] -1 points0 points1 point 3 years ago*(0 children) You mean by the exact mapping ? I don't know but maybe it means that one can use RG theory in ML . Perhaps ? They got nobel prize for RG : http:www.nobelprize.orgnobel_prizesphysicslaureates1982wilson-lecture. So it might be useful to reuse something from 50 years of research on this topic. What kinda surprises me is that I spent 3 years studying ML (post-grad) and I never heard about this connection in any of the courses I took and that kinda shows how unpopular this connection is. I don't know why ?  []bluecoffee 1 point2 points3 points 3 years ago(1 child)  []jocomoco[S] 0 points1 point2 points 3 years ago(0 children) Interesting to hear that. Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 10093 on r2-app-030c3614bd34d7a45 at 2019-11-19 10:05:36.923004+00:00 running 55775d0 country code: US. "
  },
  "21":{
    "title":"[D] Discussion on Pytorch vs TensorFlow : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,132 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[D] Discussion on Pytorch vs TensorFlow (self.MachineLearning) submitted 1 year ago by ButthurtFeminists Hi,I've been using TensorFlow for a couple of months now, but after watching a quick Pytorch tutorial I feel that Pytorch is actually so much easier to use over TF.Anyone have strong reasons why you use one over the other? Interested in the different sides of the argument. Post a comment!  []ephemeraI 68 points69 points70 points 1 year ago(3 children) Using Pytorch makes me happy, using Tensorflow makes me sad. Therefore, I use Pytorch. It's also not too bad to convert models between PytorchTensorflowCaffe.  []harponen 10 points11 points12 points 1 year ago(1 child) Haha I have to agree! I've been working with PyTorch for a couple of months now, and was just looking back at my older TF code yesterday. Despite the nice Estimator and Dataset classes, it just seemed overly complicated when compared to PyTorch... why does Google want to cram everything into the compute graph?!?  []cjalmeida 6 points7 points8 points 1 year ago(0 children) Forget TF Estimator API - maybe because it's new but but it's a one trick pony. Try to do anything different from what it was designed for and you're in for a lot of hooks. Dataset is fine though. Shameless plug, I just wrote a blog post with a simple tutorial on it: http:cjalmeida.netposttensorflow-mnist  []ButthurtFeminists[S] 1 point2 points3 points 1 year ago(0 children) Hahahahaa just go by the gut feeling :)  []netheril96 33 points34 points35 points 1 year ago(16 children) After hearing all praises about PyTorch, I tried it and initially loved it. Reasons: But after a month, I switched back to TensorFlow. The biggest downside of PyTorch is that I have to track what gradients are needed and set requires_grad or volatile appropriately, or I will waste time doing unneeded computation. TF does not have the same problem. It is more elaborated here. The core part like convolution, batch norm in PyTorch is easier to use than TensorFlow, but the auxiliary part is much harder.  And some less critical issues: Maybe I'll revisit once PyTorch hits 1.0.  []p-morais 8 points9 points10 points 1 year ago(1 child) Like numpy.reshape and tf.reshape, but torch.Tensor.view. If someone started a petition to rename Tensor.view I would sign it in a heartbeat  []cjalmeida 3 points4 points5 points 1 year ago(0 children) TBF, view is a more appropriate name for the operation. What you get with view and reshape is actually a view on the underlying data without actually reshaping it - unless you do copy or contiguous.  []brombaer3000 1 point2 points3 points 1 year ago*(2 children) Scalar support is available in the unreleased master branch of PyTorch now. What do you use for tensorboard integration? tensorboardX works in my code and has never lead to crashes.  You can save and restore the parameter state separately with torch.save(model.state_dict(), PATH) and reload it on another model instance (also works after code changes and with changed training hyperparameters): http:pytorch.orgdocs0.3.1notesserialization.html Edit: It also looks like you will be able to create tensors directly on GPU with the new Tensor factories, which use NumPy-style dtypes: https:github.compytorchpytorchpull5384  []netheril96 0 points1 point2 points 1 year ago(1 child) Yes, I am using tensorboardX. The SIGKILL issue is not easy to reproduce, so I never bother filing a bug report. You cannot load a state dict into a model if it has a different number of convolutional layers, or different num_featuers of linear layers, etc. In TF, by contrast, I export the graph along with the checkpoint, so I can evaluate it whenever I like, even I change the codes constructing the model in the first place.  []cjalmeida 0 points1 point2 points 1 year ago(0 children) That's part of eager evaluation feature of PyTorch so it's not fixable. I have the habit of Git versioning your experiments (and tagging appropriately). Otherwise, you can set to export the graph using ONNX now to load it later in similar fashion to TF.  []bdol 2 points3 points4 points 1 year ago(5 children) The biggest downside of PyTorch is that I have to track what gradients are needed and set requires_grad or volatile appropriately, or I will waste time doing unneeded computation. TF does not have the same problem. It is more elaborated here. FYI you can use detach to decouple the output of the generator from G's computation graph when updating D's parameters. No mucking about with requires_grad!  []netheril96 1 point2 points3 points 1 year ago(4 children) You still need to muck around with requires_grad of D's parameters to avoid all unnecessary computation. Not only is that tedious, it is highly non-obvious as well.  []bdol 0 points1 point2 points 1 year ago(3 children) I'm not sure what you mean about unnecessary computation. Let's take a vanilla GAN, which requires 3 gradient computations: Each step here is required, no? And we only perform one full forward pass (to generate a fake example), and one half-forward pass through D with real data.  []netheril96 0 points1 point2 points 1 year ago(2 children) The GitHub issue elaborates it fully.  []Bargh_Joul 0 points1 point2 points 1 year ago(1 child) But a solution is coming right? How good that will be?   []netheril96 0 points1 point2 points 1 year ago(0 children) No solutions are in sight. The discussions on the issue seem to have lead to nowhere.  []bradyzhou 1 point2 points3 points 1 year ago(0 children) For visualizations, check out https:github.comfacebookresearchvisdom. It's still in early phases so you'll have to roll your own smoothed scalar graphs, but it's still awesome.  []SomeoneInTheComments 0 points1 point2 points 1 year ago(3 children) NHWC, which is slower on CUDA GPUs than NCHW How big is the difference typically in %?  []netheril96 0 points1 point2 points 1 year ago(2 children) In my rough benchmark, 20% ~ 30% in training, so I always force TF to use NCHW these days.  []cjalmeida 0 points1 point2 points 1 year ago(1 child) One issue is that pre-trained models for TF are usually in NHWC. I don't know an easy way to convert between formats.  []netheril96 1 point2 points3 points 1 year ago(0 children) The TF developers promised a way to convert graphs between NHWC and NCHW, but never realized that promise. For now, I only use pretrained models to extract features, but never fine tune over them, so I am not concerned, yet.  []Kaixhin 18 points19 points20 points 1 year ago(0 children) When I started looking at frameworks the main ones were Torch, Caffe and Theano. I ruled out Caffe for lack of flexibility and found Torch had the right level of abstraction for me (e.g. layers, training loops), plus wasn't a fan of Theano's compilation step. Switched to PyTorch not because of Python (was happy enough with Lua) but primarily because it made dynamic stuff (e.g. variable length sequences for RNNs) much nicer than any of the others (including TensorFlow, released at this point). In general I like how quickly I can whip up even complex architectures in PyTorch, and no need to wait for compilation. Easy debugging. Fast. I've seen it in use at Facebook, Microsoft and Twitter (so there are big companies using it, with more listed at the bottom of the website). If I knew about Chainer a long time ago I might have gone for that, but I'm now settled with PyTorch and the awesome community that comes with it.  []ImpossiblePressure 4 points5 points6 points 1 year ago(3 children) Is Pytorch worth using if you have windows? I don't see an official download for windows, although there seem to be some unofficial ways to download it   []hapemask 4 points5 points6 points 1 year ago(0 children) There's an anaconda package for it: https:anaconda.orgpeterjc123pytorch It's been updated fairly quickly in the past as new pytorch versions come out.  []DaLameLama 1 point2 points3 points 1 year ago(0 children) The package from peterjc123 is fantastic. Works flawless on my Windows machine. Easy to install too. If you wanna use CUDA, see chapter Easy Installation here: https:github.competerjc123pytorch-scripts  []ButthurtFeminists[S] 0 points1 point2 points 1 year ago(0 children) I think theres a way to install it using anaconda. If you're comfortable with TF, then you might as well save the effort.  []melgor89 5 points6 points7 points 1 year ago*(0 children) Currently I'm using both framework, depending on task. I will write down advantages of one framework, which are disadvantage of second one ( I could be biased as I was using Torch-Lua earlier)TensorFlow: PyTorch: In general:  []fly200 5 points6 points7 points 1 year ago(9 children) I heard many Pytorch for research and TensorFlow for production. I want to know why the Pytorth is NOT suitable for production. Does anybody have some hints?   []cjalmeida 4 points5 points6 points 1 year ago(3 children) With TF you can deploy where deploying Python is a pain - ie. mobile. Otherwise, this makes little sense. Just wrap the inference code in a thin REST library and call it a day - your code will spend 99.999% of the time in GPUCPU processing anyway. With the ONNX support, you can export (most) PyTorch models to Caffe2 and even ditch Python altogether.  []fly200 0 points1 point2 points 1 year ago(2 children) Thanks for your kind comments. It seems the core of PyTorch is from the THC library based on CC++ language. Why not directly convert the PyTorch code into CC++ codes based on the efficient THC library?  []cjalmeida 0 points1 point2 points 1 year ago(1 child) While it's THC underneath, you still define the network in Python. And automated conversion yo C++ is not straightforward. IMO, using ONNX or just keeping Python sees a lot simpler than rewriting code.  []fly200 0 points1 point2 points 1 year ago(0 children) Yeah, got it. Thanks!  []poez 0 points1 point2 points 1 year ago(4 children) Safety critical applications. Python doesn't have any type checking and doesn't have to build. Much easier to send malicious messages by getting into the python interpreter than finding a way to build C++ code on the platform. Also, most backend applications are written in a compiled language like C++, Java, Go, etc. It's a short sighted idea to change your production environment. Causes headaches in a large code base and reduces the ability for teams to share code. That's also two production environments you have to support.  For small personal applications or start ups who use python anyway for backend, it's probably fine. But there's a reason why large companies eventually bite the bullet and move to a compiled language for production.   []----_____--------- 5 points6 points7 points 1 year ago(0 children) Much easier to send malicious messages by getting into the python interpreter than finding a way to build C++ code on the platform. uhhhhh no, you'll get like 5x more security risk in C or C++ because of the possibility of buffer overflows and similar vulnerabilities, where other languages, compiled or not, would just crash. I kinda agree that statically typed languages can be more predictable, assuming you write a similar amount of tests in them, but it's mostly in terms of plain bugs rather than vulnerabilities.  []fly200 0 points1 point2 points 1 year ago(2 children) Thanks a lot for your kind comments!   []fly200 0 points1 point2 points 1 year ago(1 child) In fact, I'm not sure what it is like when you say the production environment. In my opinion, after the network was trained, the pretrained weights may saved into the web server and the webserver just run the forward procedure to get the result and return to the usersclients.   []poez 0 points1 point2 points 1 year ago(0 children) Yeah that's what happens in tensorflow. There's support to take the weights trained in python and deploy it with a full frozen computation graph in C++, Java, Go, etc. So the C++ code is minimal. Can't do that in pytorch. Can only run in python.   []lalaland11256 12 points13 points14 points 1 year ago*(2 children) TensorFlow has much better and easier support for saving and loading models across different environments and even programming languages. If loading and saving models is a priority, then you should strongly consider TensorFlow. For instance, I recently worked on a project where it was a hard requirement that our model be able to run inside a Java application. TensorFlow made this quite easy because we could train our model in Python and then load it in Java without any trouble.  []ButthurtFeminists[S] 3 points4 points5 points 1 year ago(1 child) Yeah, it seems like a major plus for TF. Plus, they even have TFLite to reduce model size for mobile uses, which seems great (although the same effect is achievalbe through pytorch, its just not the same - doing it yourself vs letting the program do it for you). Thanks for sharing your experience!  []jinchuika 0 points1 point2 points 1 year ago(0 children) same effect is achievalbe through pytorch Do you have more info on this? I'm starting a project that has to run on mobile, so I was planning to use TF because of that. I'd love to use PyTorch instead, but didn't know this was possible  []engharat 3 points4 points5 points 1 year ago(0 children) I have TF as much as I love Pytorch. I worked with caffe and keras, and when I tried to switch to TF I found the framework ugly designed, mostly verbose and generally unreadable. Looking at other people TF-based source codes, it was like reading a completely foreign spoken language.When I tried to read pytorch code for the first time, I understood easily most of the content: it was like listening a dialect of my own language. Pytorch is SO pythonic, so polite, so easy to debug ( in TF it was a nightmare...) , and in those days I found every important academic paper implemented in pytorch as well as TF. All my colleagues are jumping on pytorch wagon: I hope it will spread more and more. On the other side, I think TF would never been so much used if there wasn't Google behind it..  []shchur 2 points3 points4 points 1 year ago(0 children) The main reason that stops me from switching from Tensorflow to Pytorch is the lack of support for sparse variables by the latter. In most graph-related settings storing the adjacency matrix as a dense tensor is simply infeasible, which means that TF is the only option available to researchers working with graphnetwork data.  []bbsome 11 points12 points13 points 1 year ago(23 children) Around September last year, I was switching away from Theano. I chose back then Tensorflow over Pytorch (and stay for pretty much all of the same reasons): The similarity to Theano and using symbolic graphs. This has several advantages - it is very straightforward to do higher order directional derivatives (e.g. Hessian-vector products). I think this has been done as well in Pytorch, but it remains ad-hoc in the sense that I don't think you can do any higher orders (for instance differentiate the Hessian norm).  Not bad debugging. I know people complain and I know Pytorch is bound to be better in this. However, most of the actual mistakes that you get are due to shape inconsistencies. With the automatic shape inference of thensorflow, it is quite difficult to complain about debugging these.  Tensorboard Scopes - this and 3 are the two really, really great features which distinguish Tensorflow from any other. Scopes are so nicer for allowing reuses etc... that it's just a no-brainer why other frameworks did not do this in the same way.  Tensorflow eager is coming slowly in and with that the issues of not having dynamic graphs will be gone. Significantly wider adoption - both in industry and research. Big labs like DeepMind and OpenAI use Tensorflow. Additionally, in the industry, I have not spotted anyone using Pytorch. There are significantly more resources on Tensorflow than Pytorch. This has also the ramification of much well-polished API. TPU's. If you plan to use some time in the future these, their support in Tensorflow is going to always be significantly better than Pytorch.   []alexmlamb 17 points18 points19 points 1 year ago(6 children) It's just my personal experience - but I've had a lot of problems with silent errors occurring in TensorFlow. I.e. if I scope something wrong, some of the parameters end up getting reset rather then reused, which makes model performance bad without any errors.   []ppwwyyxx 5 points6 points7 points 1 year ago(1 child) All my training scripts print parameters after building the graph with some nice formatting so this kind of error will immediately be spotted.   []geomtry 5 points6 points7 points 1 year ago(0 children) Mind sharing this? Sounds so useful!  []bbsome 0 points1 point2 points 1 year ago(3 children) I agree that can happen. But then you can have the opposite of being very hard to share parameters around. I think scopes are as close as you can get to good abstraction for variable pools. Ofc that is just my opinion on the topic but I really love them.   []alexmlamb 2 points3 points4 points 1 year ago(2 children) So in Pytorch, you can create the module once and then pass it into all of the places where it (along with its parameters need to be used).  Is there a place where the tensorflow way is a lot easier than this (this isn't a rhetorical question, I'm actually curious).   []bbsome 1 point2 points3 points 1 year ago(1 child) Well, basically you don't need to pass anything around. With scopes, you have like a global store accessible from anywhere in your code. So as long as you do at least a bit sane naming its very nice.  []alexmlamb 0 points1 point2 points 1 year ago(0 children) So it might just be that I'm not very good with it - but I've had some difficulties where I use a library such that a method which I didn't write is calling my network - and if it explicitly tries to look for specific scopes, those need to be consistent with how I defined my network. Does that make sense?   []bitcure 8 points9 points10 points 1 year ago(7 children) Tensorboard Pytorch has a tensorboard plugin that works quite as well . Additionally, in the industry, I have not spotted anyone using Pytorch.  Facebook depends significantly on pytorch. The advantage in pytorch is that the paradigm is simple enough for you to create your own operation at the lower level. The same is a headache I.e to write c level code in tensorflow especially with its poorer documentation. Overall, I use pytorch to prototype research experiments quickly and tensorflow when I've to deploy it on a website or android .  Tensorflow also has the backing of a strong community so you have easy access to a lot more Byzantine models than pytorch, at least for now.  []bbsome -1 points0 points1 point 1 year ago(2 children) Well, September 2017 Pytorch did not have Tensorboard. However, note that eventually, they will all converge to 99% the same thing so it won't even matter. Like whether you drive a BMW or a Mercedes - 99% of the driving is the same and the difference is more or less in people's mind.  Also except Facebook (as they are the main maintainers) I'm aware of only Uber to partially use Pytorch and no other lab.   []programmerChilli 8 points9 points10 points 1 year ago*(0 children) Nvidia? Salesforce too, I seem to remember? In academia, plenty of labs use pytorch. For example, speaking only of a lab I'm very familiar with, pretty much everybody @ Cornell's vision lab uses Pytorch.  []goldsborough 7 points8 points9 points 1 year ago(0 children) There's a long list right at the bottom of http:pytorch.org  [+]poez comment score below threshold-7 points-6 points-5 points 1 year ago(3 children) Facebook depends significantly on pytorch. They use it for research and then tensorflow for production.   []netheril96 2 points3 points4 points 1 year ago(1 child) Do they? It is my understanding that they deploy with Caffe2.  []poez 2 points3 points4 points 1 year ago(0 children) You are right. They use Caffe2. I'm mistaking them with another big tech company that uses pytorch for research and tensorflow for production (can't remember, saw a tech talk a few months ago). My bigger point though is that not many companies deploy pytorch for production. It's not really made for that.   []JustFinishedBSG 2 points3 points4 points 1 year ago(0 children) tensorflow for production.  No they don't...  []ButthurtFeminists[S] 2 points3 points4 points 1 year ago(0 children) Thanks for your answer! So far it seems like the general trend in responses has been: Pytorch is easier to use (and debug) while TF is better suited for more serious uses. I put quoted because what I mean by that is a lot of different factors, such as availability of expensive hardware, need to use it in another language other than python, etc.I have a few friends in industry who have actually completely switched over to pytorch. It seems like their main reason was that it's much easier to test out new things with Pytorch to optimize the model before finalizing it. It seems like they then rewrite the code using C or whatever is supported by the platform (usually mobile).  []Xirious 1 point2 points3 points 1 year ago(1 child) With the automatic shape inference of thensorflow If it's automatic why are there issues? The fact that you have to define tensor sizes is precisely why it's not automatic and error prone?  []bbsome 0 points1 point2 points 1 year ago(0 children) You define shapes only of the inputs. The issue happens before you start running the graph wherever is the mistake, the same way it will happen in Pytorch... I'm not sure what are you on about?  [][deleted] 1 point2 points3 points 1 year ago(2 children) I think this has been done as well in Pytorch, but it remains ad-hoc in the sense that I don't think you can do any higher orders (for instance differentiate the Hessian norm).  One can do this in PyT too: - For Frobenius: run H-v product over all e_i, compute norm. - For spectral (-2): run power iteration with H-v products. AD techniques for computing the Hessian more-or-less rely on H-v products, so I can't imagine anything fundamentally different bw the two frameworks, algorithmically.  The similarity to Theano and using symbolic graphs.  The AD tape is the symbolic graph. One can always use it to play games similar to what TheanoTensorflow do (assuming a stationary graph). This is what the jit-compiler, memory optimizers for PyT do. Not bad debugging. I know people complain and I know Pytorch is bound to be better in this. However, most of the actual mistakes that you get are due to shape inconsistencies. With the automatic shape inference of thensorflow, it is quite difficult to complain about debugging these.  I do miss shape inferencing, but Tensorflow doesn't really implement this well either. I miss being able to dumpload computational graphs more. PyTorch models are a pain to 'transport'.  []bbsome 0 points1 point2 points 1 year ago(1 child) One can do this in PyT too: - For Frobenius: run H-v product over all e_i, compute norm. - For spectral (-2): run power iteration with H-v products. The Hessian-vector product was added a few months ago and I'm aware of them, but I'm not sure you can differentiate that over. Might be wrong. One can do this in PyT too: - For Frobenius: run H-v product overall e_i, compute norm. - For spectral (-2): run power iteration with H-v products. This is obvious and is same for Autograd. However, the fact that you are doing it dynamically means that for complicated optimizations you need very smart JITs. Additionally, as far as I remember for higher derivatives you had to manually tell PyTorch to created the derivative graph and not store gradients in the param.gradient or smth like that.  [][deleted] 0 points1 point2 points 1 year ago(0 children) The Hessian-vector product was added a few months ago and I'm aware of them, but I'm not sure you can differentiate that over. Might be wrong. This was done by adding taping support for reverse-mode AD, so that gradient computation becomes another graph you can differentiate over (and over and over). Ditto with Chainer, Autograd. This is obvious and is same for Autograd. It also means it's more general.  You may need 'a sufficiently smart compiler' for the dynamic stuff, but for static ones, you can just run the model once, get the graph, and then do similar optimizations as Tensorflow.  []__Julia 0 points1 point2 points 1 year ago(1 child) Do you recommend any tutorials for advanced and effective use of Tensorflow   []bbsome 1 point2 points3 points 1 year ago(0 children) Usually advanced and effective means highly non-general or reusable. So I'm not sure there are, but more or less the only special care needed is how to handle the dataset loading IFF you have a very very large dataset or a distributed setup. Otherwise its all quite straightforward with the Dataset API.   []fly200 0 points1 point2 points 1 year ago(0 children) I like simple, efficient and reliable Caffe.  []rdfar 0 points1 point2 points 1 year ago(1 child) Slightly unrelated, but what are the best resources to learn pytorch?  []Atupis 1 point2 points3 points 1 year ago(0 children) Pytorch documentation is pretty good and github has good examples https:github.compytorchexamples Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 6215 on r2-app-00c1e7e3b6ef84d9b at 2019-11-20 21:35:34.383034+00:00 running a5676e6 country code: US. "
  },
  "22":{
    "title":"[Discussion] Exfiltrating copyright notices, news articles, and IRC conversations from the 774M parameter GPT-2 data set : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,103 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[Discussion] Exfiltrating copyright notices, news articles, and IRC conversations from the 774M parameter GPT-2 data set (self.MachineLearning) submitted 1 month ago by madokamadokamadoka Concerns around abuse of AI text generation have been widely discussed. In the original GPT-2 blog post from OpenAI, the team wrote: Due to concerns about large language models being used to generate deceptive, biased, or abusive language at scale, we are only releasing a much smaller version of GPT-2 along with sampling code. We are not releasing the dataset, training code, or GPT-2 model weights.  These concerns about mass generation of plausible-looking text are valid. However, there have been fewer conversations around the GPT-2 data sets themselves. Google searches such as GPT-2 privacy and GPT-2 copyright consist substantially of spurious results. Believing that these topics are poorly explored, and need further exploration, I relate some concerns here.  Inspired by this delightful post about TalkTalk's Untitled Goose Game, I used Adam Daniel King's Talk to Transformer web site to run queries against the GPT-2 774M data set. I was distracted from my mission of levity (pasting in snippets of notoriously awful Harry Potter fan fiction and like ephemera) when I ran into a link to a real Twitter post. It soon became obvious that the model contained more than just abstract data about the relationship of words to each other. Training data, rather, comes from a variety of sources, and with a sufficiently generic prompt, fragments consisting substantially of text from these sources can be extracted. A few starting points I used to troll the dataset for reconstructions of the training material: I soon realized that there was surprisingly specific data in here. After catching a specific timestamp in output, I queried the data for it, and was able to locate a conversation which I presume appeared in the training data. In the interest of privacy, I have anonymized the usernames and Twitter links in the below output, because GPT-2 did not.  [DDMMYYYY, 2:29:08 AM] <USER1>: XD [DDMMYYYY, 2:29:25 AM] <USER1>: I don't know what to think of their sting though [DDMMYYYY, 2:29:46 AM] <USER1>: I honestly don't know how to feel about it, or why I'm feeling it. [DDMMYYYY, 2:30:00 AM] <USER1> (<@USER1>): We just want to be left alone. We can do what we want. We will not allow GG to get to our families, and their families, and their lives. (not just for their families, by the way) [DDMMYYYY, 2:30:13 AM] <USER1> (<@USER1>): <real twitter link deleted> [DDMMYYYY, 2:30:23 AM] <@USER2> : it's just something that doesn't surprise me [DDMMYYYY, 2: While the output is fragmentary and should not be relied on, general features persist across multiple searches, strongly suggesting that GPT-2 is regurgitating fragments of a real conversation on IRC or a similar medium. The general topic of conversation seems to cover Gamergate, and individual usernames recur, along with real Twitter links. I assume this conversation was loaded off of Pastebin, or a similar service, where it was publicly posted along with other ephemera such as Minecraft initialization logs. Regardless of the source, this conversation is now shipped as part of the 774M parameter GPT-data set.  This is a matter of grave concern. Unless better care is taken of neural network training data, we should expect scandals, lawsuits, and regulatory action to be taken against authors and users of GPT-2 or successor data sets, particularly in jurisdictions with stronger privacy laws. For instance, use of the GPT-2 training data set as it stands may very well be in violation of the European Union's GDPR regulations, insofar as it contains data generated by European users, and I shudder to think of the difficulties in effecting a takedown request under that regulation \u2014 or a legal order under the DMCA.   Here are some further prompts to try on Talk to Transformer, or your own local GPT-2 instance, which may help identify more exciting privacy concerns! Did I mention the DMCA already? This is because my exploration also suggests that GPT-2 has been trained on copyrighted data, raising further legal implications. Here are a few fun prompts to try: Post a comment!  []jmmcd 51 points52 points53 points 1 month ago(22 children) Great work and very important, and there is wider relevance too eg in generative image models trained on copyrighted artworks, and similar.  A user can naturally plead that the original data was open on the internet, therefore having it in GPT-2 doesn't change anything, but the law won't care about that (perhaps yes when it comes to deciding level of damages but that is after the fact).  Concerning GDPR - it would be good to be specific about howwhywhich clauses it contravenes, because it can be confusing. I don't doubt that there is a problem though.  []wyldphyre 8 points9 points10 points 1 month ago(0 children) and there is wider relevance too eg in generative image models trained on copyrighted artworks, and similar. Boy, this seems like a really interesting question. When those copyrighted artworks go through human intelligence, we often describe resulting art as inspired or influenced by predecessors. But with an artificial intelligence, should we consider all of the outputs to be derived works?  []farmingvillein 11 points12 points13 points 1 month ago(0 children) A user can naturally plead that the original data was open on the internet, therefore having it in GPT-2 doesn't change anything, but the law won't care about that (perhaps yes when it comes to deciding level of damages but that is after the fact). The law (in the U.S., at least) most certainly will care about it--fair use is a thing.  Now, does this usage and re-distribution count as fair use? That is a grey area. But there are large-scale data mining and sharing examples that are currently permitted (cf. web search engines, ability to access cached pages because googlebingetc. have logged them, etc.). This issue invariably won't be resolved until it rolls through the courts, but there is substantial real-world precedence to suggest that this isn't automatically not OK (in the US).  []madokamadokamadoka[S] 5 points6 points7 points 1 month ago*(19 children) The GDPR is onerous, and aims to be somewhat extraterritorial, directing the EU and member states to exact compliance from even fully offshore actors through a variety of means, demanding compliance measures as part of the treaties comprising future trade deals. A full analysis cannot fit in this post. Persons and organisations subject to the GDPR should regard this data set as utterly accursed. To begin, it seems obvious that some of the text in the training set of GPT-2 qualifies as personal data under the GDPR: (1) 'personal data' means any information relating to an identified or identifiable natural person ('data subject'); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person; There are real names and usernames in this data set. There are links to Twitter posts. Under the GDPR, processing of personal data is forbidden except insofar as it qualifies under a specific set of exemptions: (a) the data subject has given consent to the processing of his or her personal data for one or more specific purpose;(b) processing is necessary for the performance of a contract to which the data subject is party or in order to take steps at the request of the data subject prior to entering into a contract;(c) processing is necessary for compliance with a legal obligation to which the controller is subject;(d) processing is necessary in order to protect the vital interests of the data subject or of another natural person;(e) processing is necessary for the performance of a task carried out in the public interest or in the exercise of official authority vested in the controller;(f) processing is necessary for the purposes of the legitimate interests pursued by the controller or by a third party, except where such interests are overridden by the interests or fundamental rights and freedoms of the data subject which require protection of personal data, in particular where the data subject is a child. It is probable that there is data in this dataset which qualifies as personal data of EU citizens and residents. It is fairly safe to assume that it has been added without consent, that the processing is not necessary for a contract or legal obligation, and that it does not support the vital interests of that person. The lawfulness of this processing is thus very doubtful except insofar as this qualifies as the public interest or a legitimate interest of the data controller, as defined by the GDPR and interpreted by its regulators. Academic research qualifies, but with caveats, as identified in Article 89.1: 1. Processing for archiving purposes in the public interest, scientific or historical research purposes or statistical purposes, shall be subject to appropriate safeguards, in accordance with this Regulation, for the rights and freedoms of the data subject. Those safeguards shall ensure that technical and organisational measures are in place in particular in order to ensure respect for the principle of data minimisation. Those measures may include pseudonymisation provided that those purposes can be fulfilled in that manner. Where those purposes can be fulfilled by further processing which does not permit or no longer permits the identification of data subjects, those purposes shall be fulfilled in that manner. I have no reason to believe that GPT-2's training even attempts to meets these safeguards. Moreover, even insofar as such processing is lawful, there are a variety of legal obligations which proceed from the processing of these data subjects. For instance, Article 14.1: Where personal data have not been obtained from the data subject, the controller shall provide the data subject with the following information:(a) the identity and the contact details of the controller and, where applicable, of the controller's representative;(b) the contact details of the data protection officer, where applicable;(c) the purposes of the processing for which the personal data are intended as well as the legal basis for the processing;(d) the categories of personal data concerned;(e) the recipients or categories of recipients of the personal data, if any;(f) where applicable, that the controller intends to transfer personal data to a recipient in a third country or international organisation and the existence or absence of an adequacy decision by the Commission, or in the case of transfers referred to in Article 46 or 47, or the second subparagraph of Article 49(1), reference to the appropriate or suitable safeguards and the means to obtain a copy of them or where they have been made available. And some of the data above is marked particularly dangerous, as per Article 9.1: Processing of personal data revealing racial or ethnic origin, political opinions, religious or philosophical beliefs, or trade union membership, and the processing of genetic data, biometric data for the purpose of uniquely identifying a natural person, data concerning health or data concerning a natural person's sex life or sexual orientation shall be prohibited. ... except as given in very particular circumstances enumerated in Article 9.2, and generally by an organization that has a designated Article 37 data protection officer (part of the responsibilities of extensive processing of Article 9 sensitive data). I am confident that I could go on, but this is surely enough.  []HelveticaSanskrit 2 points3 points4 points 1 month ago(12 children) I share your many of your concerns about using GPT-2, and this is absolutely a discussion that needs to be had.  Regarding the GDPR, it seems to me that its intention is to regulate the collection and retention of structured personal data without explicit consent. I'm not sure that this includes regulating unstructured data from the web where individuals have publicly identify themselves (personal lifestyle blogs, or Reddit AMAs where the individual volunteers their identity, profession, employer etc. as part of some self promotion, for example). And what about when writers write about other people, for example when a news site publishes the name and home town of a suspect in crime, or shares the name and age of a recipient of an award? From what I understand, GPT-2 was collected by scraping web pages that were linked to from Reddit. From a legal standpoint, how is that different to the data storage in our collective browser cache?  []madokamadokamadoka[S] 2 points3 points4 points 1 month ago(2 children) rgdpr may be a better venue for these questions  []sneakpeekbot 0 points1 point2 points 1 month ago(0 children) Here's a sneak peek of rgdpr using the top posts of the year! #1: More GDPR humor | 14 comments#2: I have updated my #GDPR mindmap. Markers were reviewed, the map was optimized for A4 format, the legend of the map and information about fines (applicable articles was marked) were added. | 13 comments#3: Pre-checked cookie boxes don't count as valid consent, says adviser to top EU court | 15 comments I'm a bot, beep boop | Downvote to remove | Contact me | Info | Opt-out  []HelveticaSanskrit 0 points1 point2 points 1 month ago(0 children) Thanks, that sub looks like a great resource.  []imbaczek 1 point2 points3 points 1 month ago(1 child) From what I understand, GPT-2 was collected by scraping web pages that were linked to from Reddit. From a legal standpoint, how is that different to the data storage in our collective browser cache? The difference is in the purpose of your activities. If you're not doing businesswork, GDPR doesn't apply. There are a few other exemptions: https:ico.org.ukfor-organisationsguide-to-data-protectionguide-to-the-general-data-protection-regulation-gdprexemptions  []HelveticaSanskrit 0 points1 point2 points 1 month ago(0 children) Thank you, that looks really useful.  []fell_ratio 0 points1 point2 points 1 month ago(5 children) I'm not sure that this includes regulating unstructured data from the web where individuals have publicly identify themselves (personal lifestyle blogs, or Reddit AMAs where the individual volunteers their identity, profession, employer etc. as part of some self promotion, for example). Even if that's true, GDPR has a 'right to be forgotten,' and I don't see how Google is complying with that.  []mniejiki -1 points0 points1 point 1 month ago(4 children) Google has a process for de-listing (within the EU) search results that someone claims contain personal information they do not want to be searchable. So, yes, they are complying with it. Media companies, btw, have some sort of GDPR for publishing personal information although I don't know the restrictions and Google is not a media company (it said so itself).  [][deleted] 1 month ago(3 children) [deleted]  []mniejiki 1 point2 points3 points 1 month ago(1 child) Presumably, they get sued and fined a bunch of money by the EU. Just because you can't undo a crime doesn't mean you get to not be punished for it.  []fell_ratio 0 points1 point2 points 1 month ago(0 children) Yes, that's what I meant.  []mniejiki 0 points1 point2 points 1 month ago(0 children) And what about when writers write about other people, for example when a news site publishes the name and home town of a suspect in crime, or shares the name and age of a recipient of an award? GDPR has an exception for journalists and the media. Also, many countries actually prevent the media from naming suspects. Google has, themselves, said they are not a media company under GDPR. Furthermore, Google is required to de-link urls that someone tells Google contain their private information (right to be forgotten).  []farmingvillein 3 points4 points5 points 1 month ago(4 children) If you read GDPR as narrowly as you are, search engines--as they stand--become illegal.  Regardless of whether or not that is the EU's goal (maybe...), that is not reality.  []madokamadokamadoka[S] 0 points1 point2 points 1 month ago*(3 children) I am confident that Google and other search engines have done extensive work on GDPR compliance. I presume they operate search-related processing as a legitimate interest standard (Item F above). For more guidance on legitimate interests available in English, consider the UK's Information Commissioner Office. This will give you some idea of what interests you must consider to lawfully process these data in the EU. The ICO guidance notes that you should not use the legitimate interest standard if you intend to use the personal data in ways people are not aware of and do not expect (unless you have a more compelling reason that justifies the unexpected nature of the processing). It is reasonable to expect that information on a web page will be indexed by a search engine. It is of course less reasonable to expect that private information entered onto pastebin.com or a similar service will be regurgitated by a sentence-completion program.  []farmingvillein 4 points5 points6 points 1 month ago(2 children) You clearly have not actually worked with lawyers to operationalize GDPR, because you're just copy-pasting lines without understanding it at all. It is reasonable to expect that information on a web page will be indexed by a search engine. It is of course less reasonable to expect that private information entered onto pastebin.com or a similar service will be regurgitated by a sentence-completion program This is not clear at all. Both are the exact same activity, from a consumer's POV--someone else hoovering up your conversations and doing what they want with it. Google has no more legitimate interest than does OpenAI in leveraging this data.  I am confident that Google and other search engines have done extensive work on GDPR compliance Google, Facebook, and Microsoft have all done large-scale hoovering to train language models and then release their models. All actions have legal risk, but if the mere processing of this data had meaningful risk, they wouldn't have done this.  []madokamadokamadoka[S] 1 point2 points3 points 1 month ago(1 child) If you have worked with lawyers to operationalize GDPR, then for the purpose of making this conversation more useful to rmachinelearning, I invite you to post a coherent description of the means by which the GDPR does not prohibit the processing of data, given the plain text of the statute. (Postcript: There are of course means by which it might do so. They are, however, not always quite clear, and the regulators do seem to be of the opinion that you really should not rely on a reason for processing being legal happening to exist in the abstract, without a detailed understanding of what it is.) Until such time I can provide no further input except that a machine learning researcher subject to the GDPR would be better served by consulting with lawyers and GDPR experts on the matter of compliance, rather than relying on Reddit-based analysis which backed only by the vague feeling that Google can't possibly be violating the GDPR.  []farmingvillein 4 points5 points6 points 1 month ago(0 children) text of the statute. (Postcript: There are of course means by which it might do so. They are, however, not always quite clear, and the regulators do seem to be of the opinion that you really should not rely on a reason for processing being legal happening to exist in the abstract, without a detailed understanding of what it is.) Until such time I can provide no further input except that a machine learning researcher subject to the GDPR would be better served by consulting with lawyers Of course go consult with lawyers. I am not a lawyer, and neither are you. Your analysis, however, is much narrower and declarative than mine. (Not to mention wrong, but, hey, go talk to lawyers.) You're making a much stronger set of claims than I am. Stronger claims require stronger evidence. Re:Google--of course you're going to take risk with your core products. Roll the dice and see how close you can get to the fuzzy line.  Research activities? No. You're not going to take a $50MM+ hit over some stupid language model.  []Tenoke 0 points1 point2 points 1 month ago(0 children) Your reading of GDPR Articles seems largely incorrect, though some of those are yet to be challenged in court and decided on what they mean. There are real names and usernames in this data set. There are links to Twitter posts. Elon Musk. Twitter.com. Do you think my comment is currently in violation of GDPR?  []nbriz 9 points10 points11 points 1 month ago(0 children) A couple of years ago I gave a talk about this at a copyright conference. I had been working on some music generation AI software at my studio (based on RNNs) && the copyright questions became very clear to us immediately. Here's the talk https:youtu.becSeOyFE9F2A  []MuonManLaserJab 19 points20 points21 points 1 month ago(13 children) I was trained on copyrighted data, too. I'm pretty sure I could produce some of it if given a suitably generic prompt. I'm more concerned about the privacy implications. If publicly-available IRC conversations can't be trusted to be private, what can?  []APimpNamedAPimpNamed 8 points9 points10 points 1 month ago(1 child) I was trained on copyrighted data, too. I'm pretty sure I could produce some of it if given a suitably generic prompt. And nobody cares because you don't scale.  []MuonManLaserJab 2 points3 points4 points 1 month ago(0 children) Hey, baby, it scales when it needs to!  But seriously, BitTorrent exists. I scale just fine.  []madokamadokamadoka[S] 3 points4 points5 points 1 month ago(10 children) I'm more concerned about the privacy implications. If publicly-available IRC conversations can't be trusted to be private, what can? It is perhaps inevitable that private conversations may, from time to time, be made public in some limited form. It is not inevitable that these conversations are subsequently redistributed in a data set of this sort as part of a continued violation of privacy. I was trained on copyrighted data, too. I'm pretty sure I could produce some of it if given a suitably generic prompt. I suppose that this may present some concern at such time as you make yourself available to be copied and used by the machine learning community and by the general public.  []MuonManLaserJab 7 points8 points9 points 1 month ago(9 children) It is perhaps inevitable that private conversations may, from time to time, be made public in some limited form. I wasn't really talking about private conversations. I was talking about public IRC conversations, which are immediately made fully public, not in limited form from time to time. I suppose that this may present some concern at such time as you make yourself available to be copied and used by the machine learning community and by the general public. It only has copyrighted material that was available freely on the internet. This is not making it any easier to access that information. It would be pretty dumb if we hamstrung machine learning in the name of protecting publicly-available text from piracy.  []madokamadokamadoka[S] 2 points3 points4 points 1 month ago(8 children) Most IRC conversations are made available only to a select group of people present at the time they occur. They are not \u201cfully public and there is not a reasonable expectation that they will become part of training data halfway across the Internet. (And rightly or wrongly, even \u201cfully public data \u2014 news reports printed in major newspapers, for example \u2014 can be restricted after the fact in many jurisdictions, especially Europe.) It would be pretty dumb if we hamstrung machine learning in the name of protecting publicly-available text from piracy. And it would be pretty dumb if we hamstrung all expectations of privacy and of copyright for the sake of making it incrementally more convenient for machine learning researchers to train their data sets.  Dismissive blanket statements that say you are entitled to do whatever you feel like with stuff on the internet are a very shallow way to engage with the legal and ethical issues at stake here. It only has copyrighted material that was available freely on the internet. This is not making it any easier to access that information. I don't know if you realize this, but people are allowed to place things on the Internet, without simultaneously giving you permission to do whatever you want with them. The presence of data on the Internet is not a legally binding disclaimer of all rights; nor are you morally entitled to use all public content for any purpose whatsoever.  As a researcher, you are broadly entitled to use a lot of publicly available things for research, but that entitlement does not automatically extend to re-releasing portions of the materials as part of a multi gigabyte data set. Encoding the work in an abstruse lossy compression format such as neural network weights does not automatically extend such entitlement, either.  []MuonManLaserJab 1 point2 points3 points 1 month ago*(7 children) Most IRC conversations are made available only to a select group of people present at the time they occur. They are not \u201cfully public and there is not a reasonable expectation that they will become part of training data halfway across the Internet. Reasonable? If they're not fully public, how did they get into the training data? Did they hack in, or what? And it would be pretty dumb if we hamstrung all expectations of privacy and of copyright for the sake of making it incrementally more convenient for machine learning researchers to train their data sets.  If you expect privacy when you post stuff where literally the entire world can see it, I have some bad news about the site you're on. Dismissive blanket statements that say you are entitled to do whatever you feel like with stuff on the internet are a very shallow way to engage with the legal and ethical issues at stake here. I said nothing of the sort. My argument is that you can't expect privacy if you're doing the equivalent of stapling your conversation to everyone's front door. I don't know if you realize this, but people are allowed to place things on the Internet, without simultaneously giving you permission to do whatever you want with them.  It's not whatever you want. It's just, reading and learning from it. There is no downside to making this model available, in terms of making piracy easier. Again, it's all stuff available from links on reddit. The only effect will be on researchers. Seems reasonable to call this fair use.  []madokamadokamadoka[S] 4 points5 points6 points 1 month ago*(6 children) Reasonable? If they're not fully public, how did they get into the training data? Did they hack in, or what? Okay, you know what? Fine. Let's work to figure out exactly how this not fully public material got in your training data.  I have traced the conversation in question. It appears to be part of the Crash Override Network logs leak. I have identified what I presume is the original source of this chat transcript, a Pastebin dump which has since been removed from Pastebin: https:pastebin.comAvLCEYmc I infer that GPT-2 also got it from Pastebin because the material can be found by looking for RAW PASTE DATA. These data are now gone from Pastebin but live on in GPT-2, and I presume the Pastebin dump was the source of these data because I found it while searching for RAW PASTE DATA. According to Wikipedia, Crash Override Network was a support group for victims of large scale online abuse, including revenge porn and doxing... Crash Override was founded by game developers Zo\u00eb Quinn and Alex Lifschitz, and was staffed exclusively by victims of online abuse whose identities were kept anonymous outside the group. Quinn and Lifschitz were subjected to online abuse during the Gamergate controversy, having both received death threats and doxing attacks. Others opine: CON is a Twitter trusted resource for dealing with offensive content. It was promoted by Twitter's @safety account. Please, I beg of you, ask members of the Crash Override Network, and any victims of online abuse who they were supporting during these conversations, how they feel about you placing their conversations being in your machine learning model, and the extent to which they feel they have consented to having logs of their abuse available in your data set. I will tell you, however, my feelings should I find myself in a similar position. I would opine that that, when my privacy has been violated by someone posting my sensitive conversations it MOST DEFINITELY DOES NOT MEAN that I have given you, in your capacity as a machine learning researcher, permission to FURTHER VIOLATE my privacy by redistributing these conversations, and that redistributing them in a mangled form adds insult to the injury. I would thus be very offended that you feel you are entitled to them, and I would have choice words denouncing your behavior and attitudes as offensive. As I am not a victim, however, I will instead suggest something that would be really nice, and could actively play a role in preventing future backlash against machine learning applications (and, as part of that backlash, possible new legal impairments to machine learning research). It is this. If you, in your capacity as machine learning researcher (or commentator) could work harder to have empathy to the people whose data you are bandying about. If you could assume the necessary degree of humility to countenance the idea that you or researchers in your field might possibly have fault. And if you would apply yourself to think about ways that your work and the work of others could hurt people, rather than just looking for excuses for you to do it anyway, or to excuse it as too much of an inconvenience for you to even begin to attempt. To the extent that all that, in synthesis, would be possible ... that would be really nice. I find it irresponsible and inappropriate that these chat data have been made a part of GPT-2, and I respectfully decline to engage with the rest of your posts at this time.  []MuonManLaserJab 3 points4 points5 points 1 month ago*(5 children) Okay, you know what? Fine. Let's work to figure out exactly how this not fully public material got in your training data. I have traced the conversation in question appears to be part of Crash Override Network chat logs leak. I have identified what I presume is the original source of this chat transcript, a Pastebin dump which has since been removed from Pastebin: In this case, it's public because someone already leaked it. A minute of googling shows that you can still find the stuff easily. (Obviously. Because it's the internet.) So...what's your point? Yes, it's awful that these conversations were leaked, but what would it accomplish to prevent projects like GPT-2 from producing an incredibly annoying-to-unravel representation of them? Do you think GPT-2 is the easiest way for an internet troll to find these conversations? Please, ask members of the Crash Override Network, and those who they were supporting, about how they feel about you placing their conversations being in your machine learning model, and the extent to which they feel they have consented to having logs of their abuse available in your data set. I'd be happy to ask how much they cared about the already-leaked data being accidently included in something in a form that is incredibly unlikely to cause them a billionth of the troubles they already have suffered from much simpler vectors, but I don't know any of them and don't really want to try bugging them. Maybe you could do it, and let me know if they think this matters at all? And if you would apply yourself to think about ways that your work and the work of others could hurt people, rather than just looking for excuses for you to do it anyway, or to excuse it as too much of an inconvenience for you to even begin to attempt.  Could you explain how this would hurt those people? Because again, anyone who wants to find the conversations and harass them can do so. I'm not trying to be a shit; I legitimately want to know if I'm missing something. As far as I can tell, none of this will actually matter in practice (as opposed to thought experiments) until we eliminate all of the much-easier ways to access this information. And that would require shutting down the internet, basically. It would be like killing parrots to avoid them telling children that the sky is blue. What matters, here? If people not being able to access the leaks is what matters, then GPT-2 doesn't make a difference. If what matters is not hurting people's feelings by reminding them how widely the leak has spread, then it might have been best for you to not have published this. I respectfully decline to engage with the rest of your post at this time. I respectfully acknowledge that you have respectfully declined.  []madokamadokamadoka[S] -3 points-2 points-1 points 1 month ago(4 children) So...what's your point? Yes, it's awful that these conversations were leaking, but what would it accomplish to prevent projects like GPT-2 from producing an incredibly annoying-to-unravel representation of them? You are using a dispassionate, outcomes-oriented analysis. You are responding to a violation of rights with a further violation of rights. Because the violated person has already suffered injury, you deem your futher injury inconsequential. A few choice idioms to use here: adding insult to injury, rubbing salt on the wound. In practice most people find that it is more appropriate to respond to a violation of rights with a heightened degree of sensitivity, rather than with a sense of opportunism; moreover the idea that you, rather than the person whose rights are violated, are the appropriate party to judge whether further damages are appropriate, further demonstrates disrespect their rights as humans.  []MuonManLaserJab 3 points4 points5 points 1 month ago*(3 children) You are using a dispassionate, outcomes-oriented analysis.  Yes, thank you. I try. you deem your futher injury inconsequential. No. What I asked was: what further injury? Is there any? Could you try to explain this in a way that doesn't simply assume that there is damage being done? Sorry we accidentally copied the leaked conversations. It was on a pastebin we scooped up. That's OK; it was already out there. Mostly I'm just annoyed that umadokamadokamadoka brought attention to it. rather than with a sense of opportunism This is not opportunism. OpenAI isn't laughing all the way to the bank: Thank Satan we got away with making all that money off of those Gamergate people! We couldn't have succeeded without rapaciously exploiting this opportunity! It's slightly unfortunate that this information wound up there, but nobody did it on purpose to take advantage of anyone, and nobody is suffering for it. What we're basically doing here is comparing (1) inconvenience to researchers with (2) something that sounds like it might inconvenience a Gamergate victim, but actually won't do anything to them at all (as you seem to acknowledge when you managed to say dispassionate, outcomes-oriented analysis as though that were a bad thing). Protecting victims is more important, but that doesn't come into play if the victims suffer exactly the same amount regardless of how you train GPT-2 (and I don't see you disputing that). Note: I do consider the mental suffering of victims of harrassment to be a negative outcome, which should be taken into account in any dispassionate analysis. The only place we differ is in our estimate of how much suffering is likely to come from the release of an encrypted copy of text that is already widely available.  []madokamadokamadoka[S] -1 points0 points1 point 1 month ago(2 children) Violation of a person's privacy interests is damage in and of itself! Even when further, future, material damages to reputation or to are probabilistic and uncertain!  It is not your place to tell the person whose privacy you violate, \u201cthis is not harm! Usurping a person's role as the natural judge of what constitutes an acceptable privacy risk is further harm! Using past harm to excuse additional harm for the sake of a avoiding inconvenience in procuring training data is opportunism!    []vahbuna 3 points4 points5 points 1 month ago(0 children) reminded me of this: https:xkcd.com2169  []Veedrac 6 points7 points8 points 1 month ago*(3 children) I queried the data for it, and was able to locate a conversation which I presume appeared in the training data. Why are you presuming this? Am I missing something? I agree that having recurring usernames talking about a specific topic suggests quite a lot of personal data is stored.  []madokamadokamadoka[S] 6 points7 points8 points 1 month ago(2 children) The conversation is date- and time-stamped. It is possible to issue repeated queries for the same timestamps, and nearby timestamps, and fit together outlines of a conversation from the fragments thus presented.  If there is another mechanism which would plausibly produce the same effect, besides the original conversation's presence in the training set, I am not aware of it.  []gnramires 4 points5 points6 points 1 month ago(1 child) I think it would be a great investigation to try and locate real (public) sources, and see how often prompts will reproduce them; or locate publicly available conversations it reproduces. Then we can better judge the ability of exact, reliable exfiltration of conversations, which could have privacy implications -- I think that could be quite significant as networks grow larger (and better able to store verbatim content). For small networks if reproduction varies too much (i.e. is not accurate, underfits) then plausible deniability is a decent privacy cover. I also think approaches to defend against this should be researched, and they should be relatively easy to implement. For example, during training it can be required that prompts of incomplete input texts should not reproduce the output exactly -- sort of the opposite of the usual training goal (but instead should have a significant probability P of semantic variation, P should be a function of the sample size I guess). Applications I have in mind are not only the ability to use non-public data (which is desirable in many cases) while preserving privacy, or for instance training on medical data. If you know a subset of data from a patient medical history that is uniquely identifiable, you don't want a model to reproduce the rest of its conditions reliably. If your model is predicting comorbid conditions (i.e. if you were indeed trying to predict other conditions from inputting a subset of medical history), then your accuracy clearly is must decline from this privacy condition, but I think again plausible deniability should be sufficient (a small impact in accuracy for slightly imperfect reconstruction).  []austacious 2 points3 points4 points 1 month ago*(0 children) I did some digging, trolling the network with @gmail frequently outputs github commits. The network output includes the commit checksum which is easily searchable, and could be compared with the rest of the network output to verify reproduction of training data. I'm not going to give up on it yet, but searching a dozen or so truncated checksums on github did not lead to any of the commits outputted by the the network. Neither did searching for the text content of the network output in the github repositories that the network output was pointing to, found via cross-referencing non-anonymized email addresses in the network output to custom author lists present in the repository.  []rmkn85 2 points3 points4 points 1 month ago(2 children) Learning algorithms deserve an overview of legal classifications and copyright definition. If a student reads a book so well that they can memorize and recite it, does it means they copied it?  []Veedrac 1 point2 points3 points 1 month ago(1 child) https:en.wikipedia.orgwikiCryptomnesia  []WikiTextBot 0 points1 point2 points 1 month ago(0 children) Cryptomnesia Cryptomnesia occurs when a forgotten memory returns without its being recognized as such by the subject, who believes it is something new and original. It is a memory bias whereby a person may falsely recall generating a thought, an idea, a tune, a name, or a joke, not deliberately engaging in plagiarism but rather experiencing a memory as if it were a new inspiration. [ PM | Exclude me | Exclude from subreddit | FAQ Information | Source ]Downvote to remove | v0.28  []reciprocal_banana 5 points6 points7 points 1 month ago(0 children) Great post. I've also been unnerved at finding references to actual people in gpt-2's verbiage.  []probablyuntrue 4 points5 points6 points 1 month ago(4 children) Pretty shocking that it looks like they didn't seem to scrub the dataset at all, especially in regards to copyrighted data  []suddencactus 11 points12 points13 points 1 month ago(3 children) I think the point was proof of concept. They knew the dataset had serious problems like sexism, copyright infringement, and subject matter predilection. Given that a new state of the art language model appears every three years or so, I can see someone choosing time spent on architecture instead of time spent on a clean dataset.  []shaggorama 0 points1 point2 points 1 month ago(2 children) clean that dataset once and you get to use it for all your future architectures  []suddencactus 3 points4 points5 points 1 month ago(1 child) Yes, but actually no. This model is fairly good at responding to prompts like Trump said , Apple launched a new, Djokovic scored, the last horcrux was. The researchers used machine learning to.  There's so much domain knowledge in a language model like this that it starts to get outdated after only 3-4 years, and becomes problematically obsolete in 15 years.  []shaggorama 0 points1 point2 points 1 month ago(0 children) Fair point, guess it depends on the anticipated use case.  []Equivalent_Quantity 1 point2 points3 points 1 month ago(0 children) If you prompt it with something that expects a random hash as a continuation (e.g. http:youtube.comwatch?v=) it usually doesn't give out anything real. One obvious take at this issue is that for human-generated pseudonyms (twitter accounts, e-mail addresses) there is a big chance of stumbling upon an existing username randomly - its probably harder to generate username-like output and not come across an existing handle, especially when we talk about big platforms.  []cpjw 3 points4 points5 points 1 month ago(5 children) Some interesting analysis. However, I think it is putting the concern in the wrong place. If a student turns in an essay with parts of a book copied in, you don't tell them stop! You can't read books. Those are copyrighted!, you teach them express new ideas, and how to properly attribute when they build on others. In the same way we need to not constrain (or exfiltrate) what ideas models can learn from, but instead work on better generative models which are less likely to copy direct quotes without attribution or warning to the user. (I said books in this example, but same analogy holds if a human student copies a news article, blog, quote from a tweet, etc)  []madokamadokamadoka[S] 3 points4 points5 points 1 month ago(4 children) What I hope to identify is that it matters what the judge tells the plaintiff who pursues a copyright claim against the researchers for including their data in a published data set, or against another party who builds or uses a tool to generate content based on the data \u2014 or, perhaps, how the web host responds to the DMCA complaint.  Speaking as if there is a student may point the way to better approaches in ML, but obscures the reality of a reified data set being distributed.  []cpjw 2 points3 points4 points 1 month ago(3 children) I agree that the law might have different interpretations and might differ from everyday uses of technology. This is something to keep in mind and maybe push for more up-to-date realistic policy. OpenAI didn't distribute the WebText dataset so they couldn't directly be violating a copyright. One could say that GPT-2 is a distribution of the works just in a compressed form, but I find this rather unconvincing (I understand that I am not a person it matters at all to convince from a legal perspective, but I'll explain my reasoning anyways).  As a bad approximation the GPT-2 weights are compressing the dataset into 113th the size (~40GB of text -> ~3GB of weights). However, neither the distributer (openAI) nor the reciever has a reliable way to get back the original works, and weights act more like an analysisdistillation of things that could be learned from the original text. This seems roughly analogous to if a human took the ~1300 pages in all of Shakespeare's works, and wrote a 100 page analysis of it. This analysis would likely be considered a new work.  There isn't any really a way to get back the 1300 pages verbatim. However, if you gave that analysis to a few hundred writers who had never heard of shakespeare, and asked them to write something that Shakespeare was most likely to have a written, at least some of the lines all the writers write might overlap verbatim with actual Shakespeare lines. (This is a flawed analogy, but might roughly get at the idea) It's an interesting thing to think about. Thank you for posting about the issues you mentioned and for starting a discussion. However, from my (pretty limited) understanding of the law, I don't quite see how GPT-2 distribution or how its currently being used (excluding intentually malicious uses) is putting anyone in legal jeopardy or damaging anyone's privacy. But still interesting ideas to think about in future developments for what we expect of more powerful models.  []imbaczek 0 points1 point2 points 1 month ago(0 children) There isn't any really a way to get back the 1300 pages verbatim.  Can you really guarantee that, though? If it becomes possible, does GPT-2 become illegal at that point? If yes, the risk is still there. There may be adversarial inputs that allow extraction of arbitrarily large training data if the model learned to compress input better than we think at this time.  []madokamadokamadoka[S] 0 points1 point2 points 1 month ago(1 child) As a bad approximation the GPT-2 weights are compressing the dataset into 113th the size (~40GB of text -> ~3GB of weights).  A quick Google search reveals that lossless compression programs, without external dictionaries, can achieve ~8:1 compression ratios on English text. Lossy compression on images like JPEG routinely achieves 10:1 compression with no noticeable loss in quality, and can be tuned for more. If one is copying a copyrighted image, it is unlikely that using a 13:1 lossy-compression JPEG will be a defense itself. This seems roughly analogous to if a human took the ~1300 pages in all of Shakespeare's works, and wrote a 100 page analysis of it. A typical human's 100-page analysis of Shakespeare looks very little like Shakespeare's works. A GPT-2 impersonation of a work may resemble that work substantially. There isn't any really a way to get back the 1300 pages verbatim. The inconvenience of retrieval may be a mitigating factor, limiting the actual damages suffered by the owner of a work, and thus the amount they might claim in a suit \u2014 but I'm not sure it would be sufficient by itself to defend against a copyright suit. I don't quite see how GPT-2 distribution or how its currently being used is putting anyone in legal jeopardy At a minimum, I think that anyone whose material seems to appear in the GPT-2 data set has a reasonable case to issue a DMCA takedown notice against anyone hosting or using the data set \u2014 goodness knows spurious takedown notices have been issued on far flimsier grounds.  Some GPT-2 copyright notice examples: Copyright 2014 by STATS LLC and Associated Press. Any commercial use or distribution without the express written consent of STATS LLC and Associated Press is strictly prohibited  Copyright 2015 by CBS San Francisco and Bay City News Service. All rights reserved. This material may not be published, broadcast, rewritten or redistributed. Copyright 2015 ABC News Copyright 2015 WCSF Copyright 2016 The Associated Press. All rights reserved. This material may not be published, broadcast, rewritten or redistributed. Copyright 2017 KXTV Copyright 2017 NPR. All rights reserved. Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by Verb8tm, Inc., an NPR contractor, and produced using a proprietary transcription process developed with NPR. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR's programming is the audio record. Copyright 2018 by KPRC Click2Houston - All rights reserved. Copyright 2000, The Washington Post Company (hereinafter the Company); the Post, Inc.; and Post Publishing Company (hereinafter the Publishing Company). 4 In addition to providing news and entertainment content, The Post and Post Publishing Company, Inc. publish periodicals (together with its affiliates, the Company's Periodicals) in print and electronic formats. The Company publishes periodicals in four business units: The Washington Post Media Group, Inc., and its print, cable, and digital websites, The Washington Post.com and the D.C. Bureau of The Post newspaper, and its social media, search, and other features. The Post's social media, search, and other features, The D.C. Bureau, a joint venture of The Post and the Post's publishing, editorial, and advertising businesses, generate revenue primarily from advertising impressions, referring requests, and visits (ads), all of which will be included in the ad unit's cash flow statement, which consists of an operating income statement and a cash flow statement, including the component for interest expense payable. Advertising impressions include impressions from advertising services providers, search engine results, third- These materials copyright the American Society of Mechanical Engineers. Note: This item has been cited by the following publications: H. J. P. Smith, The Effects of Fire on Machinery and its Mechanical Properties, American Journal of Industrial and Business Mechanics, Vol. 5, October 1905, pp. 693-696, 703-716, 724, 731. W. D. Lehn, The Effect of Fire Upon the Mechanical Properties of Metal, Proceedings of the Institute of Machinery, May 1883, pp. 453-457. These materials copyright 1999-2017 by Bantam Spectra, Inc. under license to Little, Brown and Company. The copyright for other materials appears after the excerpted passages. These materials copyright 1996 - 2018 by the University of Nottingham, all rights reserved. These materials copyright 2012 Robert Wood Johnson Foundation. All rights reserved. This material may not be published, broadcast, rewritten, or redistributed) These materials copyright 1995-2018 John Wiley & Sons, Ltd. The material on this page is presented for general information purposes only to aid educators and others interested in the subject. These sources are copyright and may not be used without permission from John Wiley & Sons, Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electronic, mechanical, photocopying, recording, or otherwise, without the prior permission of the publisher. Disclaimer: The information contained in this web site is provided as a general reference only and should not be considered an exhaustive or exclusive list of references. The information contains in this web site does not constitute legal or professional advice and should not be used as a substitute for expert advice. These materials copyright the author or reprinted by permission of Hachette Book Group. These materials are licensed under the Creative Commons Attribution-NonCommercial 3.0 Unported License. In accordance with their license, copyright holders may use the material only for noncommercial purposes, which may include but is not limited to display, online display, and distribution of material, for purposes of commentary, teaching or scholarship. These materials are licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 License , which permits unrestricted non-commercial use, sharing, and reproduction in any medium, provided the original author(s) and source are credited in the text. You are only allowed to use, copy, modify and distribute the content of this guide for personal benefit and educational purposes. These materials are licensed by the U.K.'s Advertising Standards Authority and may not be used without a licence. Copyright 20th Century Fox. Studio Fox TV. This segment was produced by The Current's Melissa Korn. Follow The Current on Twitter @TheCurrentPolitic. If you used or distributed the GPT-2 and received a takedown notice or a Cease and Desist letter or a Court Order from one of these parties demanding you remove content from your site or your software distribution, would you have the tools to comply?  []Phantine 1 point2 points3 points 1 month ago(0 children) Note: This item has been cited by the following publications: H. J. P. Smith, The Effects of Fire on Machinery and its Mechanical Properties, American Journal of Industrial and Business Mechanics, Vol. 5, October 1905, pp. 693-696, 703-716, 724, 731. W. D. Lehn, The Effect of Fire Upon the Mechanical Properties of Metal, Proceedings of the Institute of Machinery, May 1883, pp. 453-457. You do realize that neither of those journals or articles exist, right?  []TotesMessenger 0 points1 point2 points 1 month ago(0 children) I'm a bot, bleep, bloop. Someone has linked to this thread from another place on reddit: If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads. (Info Contact)  []zergling103 0 points1 point2 points 1 month ago(0 children) I wonder if you could explicitly set a penalty in the loss function to not replicate the training data verbatim. Though that may be hard to pull off... You'd probably just need to anonymize the training data and modify it just enough to avoid copyright issues.  []Phantine 0 points1 point2 points 1 month ago(0 children) While the output is fragmentary and should not be relied on, general features persist across multiple searches, strongly suggesting that GPT-2 is regurgitating fragments of a real conversation on IRC or a similar medium. The general topic of conversation seems to cover Gamergate, and individual usernames recur, along with real Twitter links. I assume this conversation was loaded off of Pastebin, or a similar service, where it was publicly posted along with other ephemera such as Minecraft initialization logs. Regardless of the source, this conversation is now shipped as part of the 774M parameter GPT-data set.  If you can't trace down the original conversation, what evidence do you have that you didn't just get Turing-tested? The excerpt you gave has the same weird cadence that most GPT text does. Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 17088 on r2-app-065a69a536c4a013c at 2019-11-19 20:44:48.114082+00:00 running 2367a59 country code: US. "
  },
  "23":{
    "title":"[D] what are you using? tensorflow vs pytorch : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,021 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[D] what are you using? tensorflow vs pytorch (self.MachineLearning) submitted 6 months ago by mesmer_adama Just want to take the temperature of the community at this point in time. What are you using for research and production right now, tensorflow or pytorch? What is your experiences with speed of development for writing papers, code quality and eventual release to a production environment? And if you want to elaborate what more do you use in conjunction, git for versioning, tensorboard for monitoring and docker for environment or something completely different? Personally I've been using pytorch for smaller experiments and my local environment but will start doing larger experiments and want to learn from you all. Post a comment!  []BeatLeJuce 124 points125 points126 points 6 months ago(19 children) PyTorch. I have used TF extensively before (by now I have over 2 years experience in each of these frameworks), so I think I can give an educated opinion. TF is severely less usable in my view, but unfortunately has some enterprise features (e.g. easy deployment in browsers or phones, integrated dashboards, more included support for scaling to several servers) that make it attractive from a business perspective. But from a purely development perspective (and definitely from a research perspective), PyTorch is much nicer to work with -- easier to debug, no need to learn convoluted APIs (that constantly keep changingbeing deprecated from under you), directer (no reasoning about graphs), easier to access (no fiddling with graph APIs), much nicer C++ interface, and overall more pythonic. And TF 2.0 will only solve some of these problems, but is (in my opinion) still inferior to PyTorch in many ways.  []Lietadlo5 25 points26 points27 points 6 months ago(8 children) which of the problems is not addressed by TF2.0 in your opinion?  []BeatLeJuce 44 points45 points46 points 6 months ago*(7 children)  []Lietadlo5 2 points3 points4 points 6 months ago(6 children) I get the convoluted API, but there is a lot of reasons for tfrecords: It might be overkill for smaller projects, but I would not want to go back to other formats. Especially with bigger projects, when you work in teams and you need to keep versioning your dataset. I remember when I was manually coding all of the data processing infrastructure and I am glad I don't have to do it anymore.  []BeatLeJuce 31 points32 points33 points 6 months ago*(5 children) None of your points are inherent to (or necessarily require) TFRecords. PyTorch's API allows you all of these points without forcing you to convert your data. 1: performance: JPEG, PNG, MP3, MPEG4, etc. are already binary, and are, in fact, stored in TFRecords exactly as-is. Most text-formats come as gzipped, so they're efficiently stored, too (and can be unpacked on the fly). In my own benchmarks, PyTorch's data loader was as efficient as reading from TFRecords, even on large data sets. 2-4: Every decent data loader in every other API also offers this. I'm not sure what you mean with versioning, as TFRecords itself (as far as I know, please correct me) does not directly support this. It's essentially the difference between copy the TFRecords file and copy the data directory, or am I missing something? The only advantage TFRecords has over normal data is that it's better supported in Google's internal services (where Protobuf backed things are the norm). So e.g. accesses across networked storage is more efficient, at least within Google (AFAIK). However, for users outside Google (i.e., the rest of us), this is not a plus (it might be on GCP, I wouldn't know). However, what you lose is some flexibility: It's easier to add intermediate steps in PyTorch (e.g. some intensive data-augmentation, or stuff like this ), because in TF, you never have the raw data in an accessible way. EDIT: with that said: I really like the tf.data pipeline overall. It's a big improvement over what TF had before, and there are usecases where it allows for more elegant code than the equivalent in PyTorch. My main beef is with having to rewrite all my imagesvideosaudio... into a new, inacessible storage format first. If I then stumble upon a bugsurprising result, and then want to check that my raw data was ok, I can't just open the input file in my file browser. I first have to write some code to parse a TFRecords file first. That's annoying.  []ppwwyyxx 2 points3 points4 points 6 months ago(0 children) In addition to what you said, `tf.data` (or using graph to read data) does not make a lot of sense to me either. Why would you ever want to use a graph to describe your work? I can see the following benefits: 1 is not needed in data processing. 2 is officially not recommended (the official guide recommend doing data processing on CPU for best performance) 3 is rarely needed, IMHO, for data processing. Perhaps only needed for inference 4 is true. But in a training, as long as the data pipeline is fast enough (to keep the GPU full util), getting faster does not bring more benefits.  []danijar 0 points1 point2 points 6 months ago(1 child) You can create a data pipeline from a Python generator that yields dicts of numpy arrays. For example, it can read or generate data in any way and do some cheap preprocessing. Then you can add more preprocessing (shuffling, batching, data augmentation) that should run in parallel as additional steps to the pipeline. Best of both worlds :)  []logicchains 5 points6 points7 points 6 months ago(0 children) Creating a tf.Dataset from a generator won't work properly if you try to distribute the training on GCP, losing one of the biggest advantages of Tensorflow.  []BatmantoshReturns 0 points1 point2 points 6 months ago(1 child) The only advantage TFRecords has over normal data is that it's better supported in Google's internal services (where Protobuf backed things are the norm). So e.g. accesses across networked storage is more efficient, at least within Google (AFAIK). However, for users outside Google (i.e., the rest of us), this is not a plus (it might be on GCP, I wouldn't know). eli20?  []BeatLeJuce 1 point2 points3 points 6 months ago(0 children) TFRecords is based on Protobufs. Within google (who is the main user of Tensorflow), protobufs are super-easy and normal to use. Outside of google, they don't bring any benefit. So while TRecords might be great for internal use at google, everyone else doesn't get the same benefits.  []DataScienceUTA 6 points7 points8 points 6 months ago(0 children) I see pytorch being preferred by academics from here on out  []mesmer_adama[S] 4 points5 points6 points 6 months ago(0 children) Awesome, thanks for your input! One more point goes to PyTorch then :)  []Naunauyoh 4 points5 points6 points 6 months ago(4 children) Did you observe any difference in the training time? Because, while I agree that PyTorch is much nicer to use, it seems to be a bit slower than TF in certain task (in our case it was variational auto-encoders).  []BeatLeJuce 18 points19 points20 points 6 months ago(1 child) To be honest, it's been a long time since I benchmarked -- these days, I care more about my productivity than 10% execution speed difference. If a training run takes more than 1 day (which is typical for most of my research), then I don't care if I have to wait a few hours more or less, I have context-switched to some other task in the meantime anyways. With that said: I'd be interested in a recent benchmark, if anyone has one to offer. But I'd be surprised if there are significant differences, given that in the end most runtime boils down to the same calls in cuDNN.  []Karyo_Ten 14 points15 points16 points 6 months ago(0 children) I'm sure you didn't try TensorflowKeras RNNs :P Hint: use the CudnnLSTMCudnnGRU instead as TF RNNs are a disaster speedwise.  []WolfThawra 2 points3 points4 points 6 months ago*(2 children) But from a purely development perspective (and definitely from a research perspective), PyTorch is much nicer to work with Tell me about it.... I'm using TF (for research, well, that's what I tell myself). convoluted APIs (that constantly keep changingbeing deprecated from under you) Oh I don't have to worry about that, we're not updating the NVIDIA driver on our server, so we can't update CUDA, so I can't update TensorFlow. TF 1.9 it is, I suppose. EDIT: slightly confused about the downvotes? As in, I genuinely would like to know what people are unhappy about.  [][deleted] 6 months ago*(1 child) [deleted]  []WolfThawra 1 point2 points3 points 6 months ago(0 children) Yeah I mean I'd love to upgrade but floating the idea of maaaaaybe updating the NVIDIA driver on the server was not received particularly well.  []logicchains 15 points16 points17 points 6 months ago(4 children) Using Julia's Flux. Was previously using Tensorflow, with a C++ pipeline doing some heavy data preprocessing. Moving to Julia meant I could move that pipeline into pure Julia (it's nearly as fast as C++), and turns out preprocessing on the fly and reading the results from RAM is faster than TF reading the (dense) preprocessed data from disk. Flux is also way faster for custom recurrent architectures than tf.while_loop (and will be orders of magnitude faster once Zygote is complete https:github.comFluxMLZygote.jl).  Some of my colleagues still have to use Tensorflow, as they're working with larger datasets and models so need the excellent distributed training support that Google Cloud Platform has for Tensorflow. The TF API (and regular deprecations) is a constant source of complaints, so much so that Google has even taken on another meaning for us. What, you mean Estimators are being deprecated? Oh man, that really Googled my shit right up, so much stuff to rewrite! \u201cYou lost half a day before you realised they'd mixed the loss(labels, predictions) and loss(predictions, labels) conventions? Man, they really Googled you good!  Tensorflow API design seems motivated to some degree by the needs of Google employees to get promoted by releasing new features, whereas Pytorch in contrast seems much more stable (although its 1.0 was much more recent). Tensorflow managed to go through three higher-level neural net APIs in as many years (tf.nn, tf.layers, tf.estimator), and is now on yet another, tf.keras.  []CampfireHeadphase 1 point2 points3 points 4 months ago(1 child) A bit late, but: How does flux compare to tf performance-wise? Couldn't find andy good benchmarks..  []logicchains 0 points1 point2 points 4 months ago(0 children) I haven't used Flux for traditional architectures, but for custom recurrent architectures it's much faster as the framework overhead is significantly less. You can also build recurrent structures naturally, whereas in TF you have to use TF.while_loop as if you just build an unrolled graph directly it can take over a minute to construct the graph (at least in TF 1, haven't tried TF 2). https:arxiv.orgabs1810.07951 has some benchmarks of Flux's next gen backprop approach against Pytorch, and Pytorch perf is generally similar to TensorFlow's for stuff that involves lots of ops on large matrices.  []FlynnClubbaire 0 points1 point2 points 6 months ago(1 child) Uuughhh. I'm just getting into machine learning, and let me tell you, pre-processing is taking literally 90% of my compute time. In theory, everything I'm doing should be doable on the fly, but numpy is just terrible at it. How good do you reckon Julia would be at transposing data a few times, basic normalization, and reshaping?  []logicchains 0 points1 point2 points 6 months ago(0 children) With a little work Julia can get within 70-80% of optimal C++ speed. But if everything you're doing fits into Numpy, have you tried Numba (http:numba.pydata.org)? Just a single annotation can optimise a pure Numpy-using function into something pretty fast, even if it uses iteration. Where Julia really helps performance is when you want to do anything that doesn't fit well into Numpy, e.g. fancy IO, or using a dictionary, class or dataframe, as Numba can't help with that. But if Numba supports what you're doing, it shouldn't be significantly slower than the same thing in Julia (and much easier to use if you've already written the Numpy code). If you want to work on actual streaming data, then Julia would be much faster as Numpy arrays don't really support streaming.  []Rezo-Acken 15 points16 points17 points 6 months ago(1 child) I learned DL through Keras and use Pytorch nowadays. I have limited experience with TF but always hated it.  I'm happy with Pytorch. It's simple to use and still gives good control of your models.  []Overload175 0 points1 point2 points 6 months ago(0 children) If you're experienced with Keras, the TensorFlow 2.0 api should feel natural. It allows specification of models that are essentially stacked tf.keras layers.  []marmakoide 13 points14 points15 points 6 months ago(0 children) I use Pytorch because I found the API easier to learn. When I first used it to experiment ideas with, figuring out how to do things was intuitive and didn't took a lot of searching to get from A to B. I used only to test ideas, never used in a production application running 2424 77 over many servers.  []soft-error 18 points19 points20 points 6 months ago(3 children) I use Flux, for Julia, purely for academic purposes. But I've been thinking about learning PyTorch on the side for a while, just by seeing how popular it's on academia.  [][deleted] 1 point2 points3 points 6 months ago(2 children) Do you know what the GC situation is in Flux ? Last I checked they were having OOM errors while doing AD.  []soft-error 0 points1 point2 points 6 months ago(1 child) Frankly, I never experienced it, but my problems all fit into memory. Which version were you running?  Googling around got me this: https:discourse.julialang.orgthow-do-i-release-flux-tracker-allocated-memory146634  [][deleted] 0 points1 point2 points 6 months ago(0 children) I haven't personally run it, but there was a bug report earlier which reported being limited to miniscule batch-sizes on Inception because of the lack of a memory-pool based management. I should give it another try.  []yaroslavvb 10 points11 points12 points 6 months ago*(1 child) I've been using PyTorch for larger experiments, mostly because a few PyTorch implementations were easy to get working on multiple machines. Initially I started with multi-machine TensorFlow by following the High-Performance Models guide and it ended up being too much work to get decent performance.  Other details: - no docker, startuppull time was too slow. Just put things in requirements.txt and pip install that inside conda env. - for reruns, edit things locally, use syncing script to keep local folder in sync with remote machines, mosh+tmux on remote machine - no Kubernetes, pod allocation was too slow, raw AWS instead - gitgithub for version control - Tensorboard to monitor runs and make different collections of runs by manually symlinking folders, plan on trying Weights&Biases to simplify  []ceceshao1 0 points1 point2 points 6 months ago(0 children) Would also suggest Comet.ml  []seraschka 9 points10 points11 points 6 months ago(0 children) Personally, I prefer PyTorch because it's easier to use when experimenting with custom stuff. I still use Tf occasionally in collaborations, and it's okay too, not as terrible as it sometimes sounds like. Tf has a few usability trade-offs though due to also having the multi-cluster setups at Google and deployment in mind. I don't need these features, which is why it doesn't really make sense for me to use Tf for my research projects over PyTorch, but like I said, Tf is still a solid, and usable framework.  It's like using a good pocket knife for eating your steak at lunch. It works well if it's a good knife, but there is no reason to use it over a regular steak knife if you don't need all the extra features.  []approximately_wrong 8 points9 points10 points 6 months ago(0 children) Sometime earlier last year, almost everyone in my lab was using TensorFlow. Then, by sheer coincidence, everyone in my lab independently made the switch to PyTorch. I still use tensorboard though.  []Made-ix 6 points7 points8 points 6 months ago(4 children) For those in the pytorch camp, how are you deploying your models? I'm not in love with tensorflow, but the deployment story is pretty smooth and well integrated.  []dhruvdh 11 points12 points13 points 6 months ago(2 children) There's a number of ways. But if you like the way tensorflow does it you can just export PyTorch models as ONNX and then load it up in TF and do whatever you want.  []realhamster 0 points1 point2 points 5 months ago(1 child) Have you tried the ONNX -> TF thing? I've heard there are some operations not supported, and that in general you'll usually find some corner case which will slow you down.  []dhruvdh 1 point2 points3 points 5 months ago(0 children) No I've never really come across operations that don't work. I've never needed to port anything that uses any fancy operations, so maybe that's why. If you care about performance, and mostly are looking into using the network for inference, I would say also look at TVM. They're doing some great, underappreciated work.  []Brudaks 26 points27 points28 points 6 months ago(5 children) Keras has a nice API which is the key weak point of TF 1.x, so I've been using that. With TF 2.0 I'll keep using the same Keras API, so I'll get merged back in to TF fold which is kind of nice as it'll be more integrated with the convienience features of TF such as Tensorboard.  TPU availability is a big thing, though; the free TPU on Colab gets me 27x faster training than I can do on a GPU, which makes quite a difference. last I checked, PyTorch was supposed to have it any time soon but didn't have it yet, perhaps it does now.  []bobchennan 3 points4 points5 points 6 months ago(0 children) https:github.compytorchxlablobmastercontribcolabPyTorch_TPU_XRT_1_13.ipynb  []BatmantoshReturns 2 points3 points4 points 6 months ago(0 children) TPU with Keras on colab? It looks like you don't even need a google cloud account with that. The weights save to colab right?  []rowanobrian 2 points3 points4 points 6 months ago(2 children) When does TPU work well? I mean in which scenario? When I saw TPU in colab, tried it out on mnist, found no difference.  []Brudaks 6 points7 points8 points 6 months ago*(0 children) MNIST (and CIFAR) is a bad benchmark for speed tests because they run matrix operations on really tiny matrices, which is not representative for anything where speed matters (I guess MNIST would become IO limited? or the overheadlatency of using the remote TPU becomes too much for many tiny operations) - in pretty much every large task you get much, much bigger layers; the same goes for batch sizes - generally for speed reasons you'd use as large batches as your GPU RAM allows (which won't be much if the memory needs for every sample is large), and then the raw performance difference becomes significant; but for MNIST that'd mean having a minibatch of all samples at once which isn't good for accuracy and convergence. On TPU I see a slight boost (~3x) when doing the same paramaters as on GPU, but the big boost happens by having 16 times larger batches and doing so in large portions to streamline IO (the first couple minutes are always much slower for some reason while the remaining hours are at a very stable speed). AFAIK RNN's are worse on parallelization and so it might not get a boost (I haven't tried that), but language tasks are now moving to transformer (self-attention) based architectures and TPUs are well suited for that, that's where I get that 27x speed difference; as far as I see from what others are trying with CNNImageNet tasks, you get a ~20x speed difference on that as well over colab's GPUs (K80).  []SedditorX 1 point2 points3 points 6 months ago(0 children) I'm pretty sure you can solve MNIST even with linear regression so TPUs might be overkill. You wouldn't use GPUs to multiply two integers quickly.  []Captain_Flashheart 20 points21 points22 points 6 months ago(1 child) Keras Tensorflow here. PyTorch seems more popular in academia the past 12 months or so, but I've yet to meet anyone working with PyTorch at work. It's always tensorflow or something really old they picked up while still at university. Maybe I'll become a tensorflow dinosaur as well? ;)  []Richard_wth 11 points12 points13 points 6 months ago(0 children) I met a Snapchat engineer at a conference. He said they used PyTorch for research as well as product...but I'm a TensorFlow dinosaur as well.  []sieisteinmodel 6 points7 points8 points 6 months ago(1 child) What about JAX?  []farmingvillein 5 points6 points7 points 6 months ago(0 children) Like part of the Google Brain team, interesting point: https:github.comtensorflowtensor2tensortreemastertensor2tensortrax  []ice_aggregate 4 points5 points6 points 6 months ago(0 children) TensorFlow + Keras since I'm much more familiar with the tools and have been using them for 3-4 years now. Switching would mean relearning another system and unless I have a strong reason to move, I'm sticking with what I know.  []grappling_hook 3 points4 points5 points 6 months ago(0 children) I use tensorflow, never really tried pytorch though. Maybe for my next project.  []Professor_Entropy 3 points4 points5 points 6 months ago*(0 children) I wanted to move from pure keras to using tf.keras since that is what tf 2.0 is all about. Decided to never write any tf code from scratch after wasting time due to this old tf.keras issue which hasn't been fixed yet. When I was getting the error I questioned my abilities and it affected my confidence.  Tried pytorch, learnt it in a day, loved it. Intuitive, customizable, fast, consistent and less headache. I still need to use tensorflow since google and openai release their models in tf. We can train BERT on TPU only through google's official tf repository, as pytorch repositories don't support it yet.  []GradMiku 2 points3 points4 points 6 months ago(1 child) I'm going to move to PyTorch, because i tested 2 different implementations of the same architecture(CapsNet) and in kerastensorflow the training time was 6h and the same epochs and data the training in PyTorch only takes one hour  []dhruvdh 0 points1 point2 points 6 months ago(0 children) You should note that CapsNet is non-traditional so it makes sense that neural network frameworks are not optimized for those. In my experience PyTorch is slightly faster too, but I don't think just trying out CapsNet and concluding PyTorch is faster is a good idea.  []Reiinakano 6 points7 points8 points 6 months ago*(9 children) Every time this question comes up, I've seen Pytorch completely win the usability argument. I really want to switch everything to Pytorch but I depend on a lot of Google research code written only in Tensorflow.  [][deleted] 6 months ago(2 children) [deleted]  []SedditorX 6 points7 points8 points 6 months ago(0 children) For all of tensorflow's flaws, this is something obvious that's always neglected in the debate. Pytorch users on this forum loudly crowd out these kinds of threads and sometimes downvote dissenting opinions. IMO the best way to make the ecosystem better for everyone is to be constructive with criticism. A rising tide raises all ships and all that.  []NicolasGuacamole 1 point2 points3 points 6 months ago(5 children) Port the models to pytorch. It's usually straightforward.  []Reiinakano 2 points3 points4 points 6 months ago(1 child) Is it straightforward for something like https:github.comtensorflowlucid?  []NicolasGuacamole 0 points1 point2 points 6 months ago(0 children) Probably straightforward but time consuming  []Ouitos 4 points5 points6 points 6 months ago(2 children) It's clearly not. It can take some time to figure out the subtle differences between the different default parameters of your module and how you can make the algorithm behave exactly the same (and for some papers, it's critical to have the exact same algorithm or you might not converge at all). This is not due to the fact that tensorflow and pytorch are too different, but rather than most research code is poorly documented (as is tradition) and theoretical knowledge of the algorithm is often not enough to port it. However, I do advise you to port to pytroch, as it can be a rewarding experience in terms of what you'll learn.  []NicolasGuacamole -5 points-4 points-3 points 6 months ago(1 child) Thanks for the lecture I didn't ask for. Funnily enough I've got lots of experience porting models and implementing from papers in the dark ages of Caffe (1) and piles of theano spaghetti. It was doable if tricky then and I've every confidence it is now with these nicer modern frameworks.  []Ouitos 4 points5 points6 points 6 months ago(0 children) I'm sorry you took my comment as condescending, the you was aimed toward general discussion than you specifically. That being said, I invite you to try now too. My point is independant to ease of use, you will get a minimum theoretical clone easily but you won't get easily the exact same results: as long as some differences will lie between frameworks (they tend to disappear but they do exist, such as convolution padding and weight initialization functions to name a few), the port won't be straight forward since these subtle differences will be implicitly here.  []Overload175 1 point2 points3 points 6 months ago(0 children) Still use TensorFlow, admittedly out of sheer force of habit though  []MagicMurderBagYT 1 point2 points3 points 6 months ago(0 children) Pytorch. It's just better for me than tensorflow.  []trexdoor 4 points5 points6 points 6 months ago(18 children) Neither, I have my own self-written ANN framework. It is not very advanced but I had used it in several projects successfully, of which the latest was license plate reading. Is there anyone else here who are using their self made libraries? Or am I the sole outlier?  []Necryotiks 1 point2 points3 points 6 months ago(1 child) Is it on github.  []trexdoor -1 points0 points1 point 6 months ago(0 children) No.  []MachineIntelligence 1 point2 points3 points 6 months ago(0 children) Funny enough what got me hooked on Machine Learning was continuing a project from my intro to AI course in college. After graduating I spent multiple months, without researching the topic any further, building out a ANN framework in Java with its own network configuration file. Object oriented neurons (ReLU,Sigmoid) , no matrix algebra, only stochastic gradient descent, no additional optimizers. Very naive but it works and I was super proud of it because I learned a lot from it! And I find the simplicity of it charming, despite it's inefficiencies. https:github.comNicholasLeParSimple-Java-Neural-Network Obviously after taking Deeplearning.ai and reading more content out there I realized how stubborn I was not to explore the space more and learn the proper way of building these frameworks. Leveraging vectorization, regularization, mini-batches, different optimizers. I am now building out another framework in Python because I love the idea of just building out these models in a simple way that is easily digestable for myself. I'll keep you posted when I push that one out on github.  []Mastiff37 1 point2 points3 points 6 months ago(0 children) I have some capabilities written up in C++, like basic dense and convolutional networks, max pooling, a handful of activations, etc. They worked, but I eventually caved because stuff is moving fast and every new feature is a fair bit of work to get working properly. For mine I didn't do autodiff, so any new loss function needed the gradients to be worked out manually. There's also the whole issue of GPU support if that matters for your application. I would expect many large corporations would have their own libraries going though, for deployment in embedded applications if nothing else.  []enjakuro 1 point2 points3 points 6 months ago(4 children) Why would someone vote this down?  [][deleted] 4 points5 points6 points 6 months ago(2 children) I didn't downvote, but I will say this: unless OP is Joe Redmon wizard level, I don't think custom ML libs should be used for projects beyond coursework final projects or as learning exercises.  Coding up autodiff, convnets, etc. from scratch can be invaluable experiences. You learn great many things, including how easy it is to mess things up by introducing silent failures. Consequently, using a custom library for research or production level deployment is a big no-no. Too risky for silent bugs, the code isn't easily understood by nor readily shareable with the community, etc.  Maybe this is why people downvote. Maybe people view it as boasting. Or maybe they're jealous someone can code from scratch what they can only do from keras API. This is Reddit after all.  []trexdoor 1 point2 points3 points 6 months ago(1 child) I disagree on a few points but thank you for your input. I have to say, there are many companies that are using their own libraries. One of them where I worked started making their OCR engine about 20 years ago. There were no publicly available libraries then so they had to make everything from scratch - and they stuck to it because it worked well... And I had to say, in some ways their libraries were more advanced than what is used in academic research.  []sorrge 0 points1 point2 points 6 months ago(0 children) Not Pytorch!  []enjakuro 0 points1 point2 points 6 months ago*(8 children) Why would someone vote this down? Edit: thanks for your answers! I'm studying multilingual text analysis and just made 'first contact' with tensorflow xD I can see now why you should use common libraries  []farmingvillein 1 point2 points3 points 6 months ago(7 children) Probably because it is unlikely that the user here is doing anything meaningful in either production or research environment. This makes the user's response look a lot like noise. No one doing SOTA research is using anything custom, unless they are very specifically trying to solve something exceedingly specific that the current frameworks don't support. This clearly isn't the case here. No one is doing production-level work by rolling their own framework, unless 1) they've been in production for many, many years and have legacy code or 2), again per research, they have exceedingly specific needs (maybe getting onto very specific hardware). License plate reader not germane here. Since the user isn't doing anything serious, this is a little like taking a poll like What game development framework do you use? Unity or XYZ?, and someone responding that they built their own in Lisp.  Technically responsive to the question, but really just indicative of highly idiosyncratic choices made for entirely idiosyncratic reasons; not helpful or representative.  []trexdoor 1 point2 points3 points 6 months ago(6 children) You are making tons of false assumptions there. Thanks for your response anyway. I suspect this is mostly representative of the way of thinking in this community.  []farmingvillein 0 points1 point2 points 6 months ago*(5 children) What false assumptions? You're definitely not doing SOTA research. Production--there's literally zero good engineering reason to build your own framework, unless #1 or #2 that I listed qualifies. Which I highly doubt.  Put another way, what are you doing so special that it your use cases requires a different framework than Google, Facebook, Amazon, Salesforce, most people deploying to mobile, etc.? Educate us. No one knows your use case but you.  []trexdoor 2 points3 points4 points 6 months ago(4 children) First, what is this snobbish remark regarding me not doing SOTA research? Even if it is true, why do you even bring it up? This is not a subreddit strictly for academic researchers and people playing with the latest SOTA techniques, is it? Seriously, are you gate keeping or what? I am doing machine learning at my job, if I count well for more than 15 years now. I think I have the same entitlement and right to participate in rMachineLearning as the young guns who who are in majority here. I felt that OP's question was addressed to me just as to any other member. I gave my humble answer, so what? Second, to give you an idea of what I had been doing recently, a licence plate reader engine. I used ANN's to find lines of text, to read the text, to check for additional logos on the plates. That's a whole bunch of machine learning, isn't it... And not because I want to brag but I have to add, yes, all written by me in C++. You should know that these LPR engines cannot be deployed in the cloud at some cases for security reasons, they are usually running on the customer's closed server park, or even better, on the camera itself. Now these cameras have very limited HW capabilities, only an Atom or an ARM processor without any special HW acceleration that could be used for ANN calculations. That's the reason for the custom library. Very limited HW resources, and the task demands a very fast engine that can detect plates as small as 20 pixels in height on a 2 megapixel image at min 10 fps. You have no chance if you want to do this with the big frameworks mentioned here by others.  []farmingvillein 0 points1 point2 points 6 months ago(3 children) You have no chance if you want to do this with the big frameworks mentioned here by others. You can almost certainly do this on Tensorflow with Google's tooling. The footprint can get exceedingly small and tight. This sounds like you aren't actually familiar with the current frameworks. Unless you're dealing with truly ancient technology, in which case see my original disclaimer in both my notes re:hardware requirements.  []trexdoor 2 points3 points4 points 6 months ago(2 children) Lol, at this point you are just throwing shit at me. Our library is the product of many years of hard work by me and my teammates. It has several features that is not or not easily available in the mainstream libraries. If we need something new then we can add it to our framework, you know, we are skilled programmers. You seem to know nothing about the challenges of writing fast algorithms on embedded systems, and you surely do not know the capabilities of our framework, yet you are readily suggesting that it is ancient technology. Such a nice guy you are.  []farmingvillein 1 point2 points3 points 6 months ago*(1 child) Again, as I wrote originally: No one is doing production-level work by rolling their own framework, unless 1) they've been in production for many, many years and have legacy code or 2), again per research, they have exceedingly specific needs (maybe getting onto very specific hardware). You are telling me you have 1) legacy code and 2) specific hardware needs. You choose to get and responded in an offended manner, when both of these qualifiers apply to you. And then you dribble out the fact that they apply to you on a post-by-post basis, getting progressively more offended. Further, this reinforces my original point, which was that someone asking what frameworks do you use for ML? and then stating that you use a custom platform...which was apparently built up over years...is relevant to basically no one looking to understand framework trade-offs because 1) you have legacy concerns and 2) you have apparently deep, specific hardware concerns which neither bothered to qualify up front, or to articulate in a way that advances the discussion, i.e., highlights what specific hardware limitations require something so custom. Yes, we're all aware that there is actually a lot of COBOL still sitting on IBM mainframes, but this is not how anyone would chose to cold-start solving a problem today.  I'm sure I'm not unique in this thread--I know plenty of people who do and have worked on embedded systems, including higher-end requirements than 2 megapixel image at min 10 fps for a license plate reader. In the vast majority of cases, eg TF is and will be more than sufficient, and there is a large array of support that will push that further. If you are actually trying to contribute to the discussion, then explain where current solutions are deficient.  [O]nly an Atom or an ARM processor without any special HW acceleration that could be used for ANN calculations is not a sufficient reason, in that TF can support many of those use cases. We started getting going a while back when TF support for the edge was worse and built up a library is totally a valid engineering reason--but that goes back to my aforementioned legacy code qualifier, and is largely uninteresting to anyone trying to understand current framework tradeoffs. Lastly, I was originally responding a poster asking why this post was being voted down. I am not a solo agent taking down your post--while I cannot speak for everyone's reasons as to why your post was voted down, I can provide context on why I think the collectivity of the subreddit is not finding your post additive to the discussion. Try to take my commentary in the context of this broader feedback, rather than any particular point you want to disagree with me on. I am apparently not alone here.  []TheFlyingDrildo 0 points1 point2 points 6 months ago(0 children) pytorch. so easy and painless to just set up some custom model and start training. thinking of switching to flux tho based on how much i love julia  []vajra_ 0 points1 point2 points 2 months ago(0 children) Tensorflow is shite - kept alive by money pumping from Google - and trying to copy Pytorch concepts. Its a freaking mess. The only reason I have to use TF is because of weirdos who started DL with TF and can't move on.  [+]CyberDainz comment score below threshold-9 points-8 points-7 points 6 months ago*(5 children) Google is much bigger corp than facebook, so it is future-proof, and I bet on tf.keras  []vegetableagony 4 points5 points6 points 6 months ago(2 children) Based on market cap it looks like Google is only about 50% larger than Facebook. Both are extremely large advertising driven US-based tech firms.  []CyberDainz -3 points-2 points-1 points 6 months ago(1 child) market cap is nothing. Google is giga corp, that provides a lot of free services to users, and develops large amount of software. Facebook is just a social network. Sorry but no, pytorch has no future. Good luck stay on the dying nn framework.  []GradMiku 0 points1 point2 points 6 months ago(0 children) The quality of tour projects depends of your abilities not your framework fanbase. The number developers of those frameworks are very similar, so PyTorch is not dying are you chosen your frameworks only because popularity? and i recently test PyTorch and is much faster that Tensorflow at training time Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 23522 on r2-app-030c3614bd34d7a45 at 2019-11-20 21:41:49.223814+00:00 running a5676e6 country code: US. "
  },
  "24":{
    "title":"[D] TensorFlow is dead, long live TensorFlow! : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,364 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[D] TensorFlow is dead, long live TensorFlow! (self.MachineLearning) submitted 7 months ago by milaworld Article about the TensorFlow's decision to drop legacy functionally to embrace Keras full-on. In a nutshell: TensorFlow has just gone full Keras. Those of you who know those words just fell out of your chairs. Boom! Why must we choose between Keras's cuddliness and traditional TensorFlow's mighty performance? What don't we have both? \u201cWe don't think you should have to choose between a simple API and scalable API. We want a higher level API that takes you all the way from MNIST to planet scale. \u2014 Karmel Allison, TF Engineering Leader at Google https:hackernoon.comtensorflow-is-dead-long-live-tensorflow-49d3e975cf04?sk=37e6842c552284444f12c71b871d3640 Post a comment!  []tidier 114 points115 points116 points 7 months ago(8 children) My experience with PyTorch and TF so far is: PyTorch is designed as a Python module, first and foremost. TF always felt like a (very powerful) external library with a Python API. With PyTorch, I can reason through most of it from a Python perspective, from architecting code to debugging errors. I'm tired of seeing look at this 5-liner to train MNISTImageNet examples. That tells me nothing about how it would be like actually working with the framework. I want to see examples that go Here's some code with an error. It's actually because of this subtle mistake. This is how you would actually find out and fix it. Or We had this crazy idea to do X. No one would think of doing X, and you wouldn't actually think it's possible but actually you can with our framework and here's how you would do it. A code excerpt that looks like tells me absolutely nothing.  []RoboticElfJedi 32 points33 points34 points 7 months ago(5 children) This. I'm pretty done with MNIST at this point. Now take your model and show me how to freeze one layer and retrain, look at the activations of a particular neuron, create a layer of a new kind.   [+][deleted] 7 months ago(4 children) [removed]  []RoboticElfJedi 13 points14 points15 points 7 months ago(2 children) Jesus reddit.  The point isn't that things shouldn't be good for beginners, they should. I also want to see the internals and the ability to do more advanced stuff.   []JonnyK74 7 points8 points9 points 7 months ago*(1 child) Yeah, and it's even more than that. I'm a beginner at TF but a very experienced programmer, and beginner cookie cutter examples make it very difficult to actually move on from being a beginner. Showing real-world examples of solving moderately difficult problems is much more useful to beginners.  [][deleted] 3 points4 points5 points 7 months ago(0 children) Exactly this. Treating an already obscure topic as a black box where even the things you know and can reason about are part of it, renders the topic impossible to understand and internalize.  I find PyTorch to be great on alleviating the whole even the code is a black box. There are plenty of models online that show what's happening and allow me to search and learn on my own without having a mental pagefault every time I branch to read about something.  []AyEhEigh 5 points6 points7 points 7 months ago(0 children) I mean, you can always have beginner examples and advanced examples. They aren't exclusive of each other.  []GoofAckYoorsElf 8 points9 points10 points 7 months ago(0 children) This needs to be a sticky in every fucking programming forum!  []mmirman 7 points8 points9 points 7 months ago(0 children) I started out working with TF. I would have never had the idea to differentiate over raytracing or abstract interpretation had I continued.  []perceptron01 129 points130 points131 points 7 months ago(14 children) Jesus Christ what is this post? It has some random funny meme image after each paragraph  []wiltors42 60 points61 points62 points 7 months ago(0 children) welcome to the world of medium ML articles  []MorrisMustang 48 points49 points50 points 7 months ago(2 children) EU proofing the article. no biggie.  []whatdidusaybro 1 point2 points3 points 7 months ago(1 child) what are you talking about, that will all get taken down after the shiznitz by EU gets in effect it's counter proofing to be exact or is this the joke? swoosh?  []MorrisMustang 3 points4 points5 points 7 months ago(0 children) Like child proofing...  []DeepBlender 21 points22 points23 points 7 months ago(0 children) It is a touch over the top for my taste too. However, it closely resembles my experience with TensorFlow 2.0. It is such a huge improvement and I really enjoy working with it!  []osipov 14 points15 points16 points 7 months ago(2 children) It's a weird Googler fetish for meme images. Source: former Googler.  []gokstudio 1 point2 points3 points 7 months ago(0 children) Blame Thank MemeGen  []GoofAckYoorsElf 1 point2 points3 points 7 months ago(0 children) Go away, Axel Voss!  []d8sconz 2 points3 points4 points 7 months ago(0 children) Like that damn plinky plink music they add to every. single. video. tutorial now.  []killingisbad 2 points3 points4 points 7 months ago(0 children) Really makes concentrating hard  []dt_bui 1 point2 points3 points 7 months ago(0 children) Google is a meme loving company  []Vikram8 0 points1 point2 points 7 months ago(0 children) Just like Siraj's videos\ud83d\ude02  []siblbombs 133 points134 points135 points 7 months ago(23 children) If I survived switching from Theano to Tensorflow, I'll survive switching to TF2  []cucuska2 175 points176 points177 points 7 months ago(14 children) I never really got over the fact that hats were introduced into TF2.  []lstama 61 points62 points63 points 7 months ago(10 children) TensorFlow 2, Team Fortress 2, Titanfall 2.  []cucuska2 52 points53 points54 points 7 months ago(7 children) Tensor Fortress 2, Team Fall 2, TitanFlow 2?  []oblivioncntrlsu 56 points57 points58 points 7 months ago(5 children) Team Tensor Titan 2: Fortress Flow  Coming This Fall.  []Cypher1235 15 points16 points17 points 7 months ago(3 children) 2 Tensor 2 Flow  []UnhappySquirrel 20 points21 points22 points 7 months ago(1 child) 2 Tensors 1 Flow  I immediately regret typing that.  []badpotato 2 points3 points4 points 7 months ago(0 children) Look like the project finally reached some sort of maturity.   []2Punx2Furious 0 points1 point2 points 7 months ago(0 children)   []cucuska2 8 points9 points10 points 7 months ago(0 children) You are a next-level madlad.  []sk_2013 5 points6 points7 points 7 months ago(0 children) Tensor Fall 2 was a weak followup, I felt like.  []GoofAckYoorsElf 0 points1 point2 points 7 months ago(0 children) TF2 (ROS transform framework)  []emilazeri92 -1 points0 points1 point 7 months ago(0 children) rtitanfall2  []automated_reckoning 10 points11 points12 points 7 months ago(0 children) ... I'd buy a TF2 hat.   []stilloriginal 3 points4 points5 points 7 months ago(0 children) That was so much later it really should have been called tf3  []siblbombs 1 point2 points3 points 7 months ago(0 children) At least its free2play.  []p-morais 26 points27 points28 points 7 months ago(4 children) If I survived switching from Theano to Tensorflow, I'll survive switching to Pytorch FTFY  []maokei 2 points3 points4 points 7 months ago(2 children) DL4J anyone?  [][deleted] 7 months ago(1 child) [deleted]  []gokstudio 1 point2 points3 points 7 months ago(0 children) Most real-world companies  []siblbombs 3 points4 points5 points 7 months ago(0 children) No thanks, I'm not sold on eager (I don't do that much research).  [][deleted] 7 months ago(1 child) [deleted]  []siblbombs 6 points7 points8 points 7 months ago(0 children) I actually am a big fan of graph-based instead of eager, also I use tensorflow serving pretty aggressively, so pytorch doesn't really capture my interest.  []st8ic 58 points59 points60 points 7 months ago(16 children) This honestly looks a lot like an April fool's post. Judging by all the aNyOnE cAn Do aI nOw platitudes in this article, it looks like this is Tensorflow's attempt to compete for users in the ML hype war.  As a long-time Tensorflow user, I don't really give a shit about Keras but as long as they still have all the same functionality available without using their canned keras layers, I'm okay with learning the new API.  []TheOneRavenous 25 points26 points27 points 7 months ago(8 children) Writer works at Google...  []epicwisdom 1 point2 points3 points 7 months ago(7 children) And an executive, apparently - which is noteworthy since last I checked Googlers number in the tens of thousands, so works at Google may not mean much when it comes to making strong claims about the direction of one of their biggest open source libraries.  []osipov -1 points0 points1 point 7 months ago(6 children) @quaesita is not an executive. Just gave herself a self-inflated title.  []epicwisdom 0 points1 point2 points 7 months ago(5 children) https:www.gcppodcast.compostepisode-128-decision-intelligence-with-cassie-kozyrkov appears to be an official source which corroborates her job title experience, though of course executive is not a real title. Seems significantly higher up the chain than most SWEs.  []osipov -2 points-1 points0 points 7 months ago(4 children) corroborates on the podcast they just cite whatever she put as her job title in Teams, google's internal directory. she is an L7 at google with a lot of diversity headwind.   []epicwisdom 0 points1 point2 points 7 months ago(3 children) Diversity headwind is your subjective political bias, completely irrelevant. Most SWEs peak at L5 at Google; I never checked the exact distribution, but roughly speaking L7 ought to be the top 2-5%. It's about as high as you can get before VPSVP, unless you're Jeff Dean, so I'm inclined to listen to them about products they would reasonably have some insight on.  []osipov -2 points-1 points0 points 7 months ago(2 children) Diversity headwind is your subjective political bias here's another biased statement for you. look up the dictionary under IYI. your picture is there.  []epicwisdom 0 points1 point2 points 7 months ago(1 child) Lol. First, make a claim without evidence, that practically can't be verified, then resort to childish insults. I guess that's all you have to say on the matter.  []osipov -1 points0 points1 point 7 months ago(0 children) If you can't stand insults or need evidence to convince you that water is wet then get off the internet. Go back to your Pleasure Island of a safe space and don't forget to graze at your local MK.  []blitzzerg 11 points12 points13 points 7 months ago(4 children) To me it's just so stupidly dumb to label everything now as keras. I have never used tensorflow for doing neural nets  []st8ic 18 points19 points20 points 7 months ago(2 children) yeah they're definitely after the I want to learn how to write a MNIST classifier in a 10 minute tutorial market. I just hope that it's not more difficult for us who just need auto-differentiation in python.  []akshayka 4 points5 points6 points 7 months ago(0 children) You don't have to use Keras if you don't want to. There's nothing stopping you from using TF 2.0 as a library for numerical computation and automatic differentiation; in fact, 2.0 encourages this kind of differentiable programming, at least for advanced users. https:www.tensorflow.orgalphatutorialseager  []bones_and_love 3 points4 points5 points 7 months ago(1 child) Is there an official announcement from Google you can find?  []akshayka 2 points3 points4 points 7 months ago(0 children) https:www.tensorflow.orgalpha https:medium.comtensorflowwhats-coming-in-tensorflow-2-0-d3663832e9b8  []dfarber 21 points22 points23 points 7 months ago(1 child) is it 41 already?  [][deleted] 11 points12 points13 points 7 months ago(0 children) I got a good laugh from their memetic doge definition of Tensor. The entire piece is a joke, and obviously so assuming you have any experience in optimizing graphs of tensors and submitting them to tensorflow and using the keras library.  There's a lot of misinformation in the community, and this is a beautiful addition to that polluting disinformation.  []blitzzerg 27 points28 points29 points 7 months ago*(5 children) Well at least now I'm motivated enough to leave tensorflow and switch to pytorch  []youbehasbeen 5 points6 points7 points 7 months ago(3 children) Granted there's still a lot I don't know, but I don't understand how this would make you want to switch to pytorch? Is it just a matter of now I have to learn something new, may as well it be pytorch ?  []blitzzerg 7 points8 points9 points 7 months ago(2 children) to me this is the problem: Tensorflow was originally created as an automatic differentiation library, which is useful for a lot of things, one of them is doing neural networks at low level (on the contrary Keras is making them at high level). With this update they seem to have made a huge shift (at least that's what I think) to integrate everything into Keras, even if it's just at a naming level, forgetting about other users like me that do not use Tensorflow to code neural nets.  []epicwisdom 6 points7 points8 points 7 months ago(0 children) I don't see why you think they're getting rid of that. There are obviously plenty of researchers at Google who use more features in TF than just Keras, so even from a purely self-serving point of view, there's no advantage in narrowing TF's applications.  [][deleted] 5 points6 points7 points 7 months ago(0 children) Yeah, I thought TensorFlow was great because it's a linear algebra library with auto-diff, pretty much. Keras already served the purpose it was needed for. I'm not sure why they need to integrate. Hopefully they keep some of the low-level API. I haven't read the article yet.  []BastiatF 0 points1 point2 points 7 months ago(0 children) Do it! Your future self will thank you.  [][deleted] 7 months ago(1 child) [removed]  []Overload175 1 point2 points3 points 7 months ago(0 children) Can you give an example?  []ThisIsMyStonerAcount 70 points71 points72 points 7 months ago(53 children) Who cares about TF as long as we have Pytorch.   []zzzthelastuser 26 points27 points28 points 7 months ago(1 child) nothing wrong with some healthy competition  []khanstein 7 points8 points9 points 7 months ago(0 children) And that is most probably why we had eager execution and now this transition maybe faster than it would take otherwise.   []bencbartlett 8 points9 points10 points 7 months ago(1 child) Who cares about TF as long as we have Pytorch. C O M P L E X T E N S O R S   []koolaidman123 28 points29 points30 points 7 months ago(9 children) i love pytorch for its ease of use, but currently almost all the rl work is done in tensorflow, so a lot of the resources aren't available to pytorch users. i've had to start learning tf to run some rl experiments  []rantana 18 points19 points20 points 7 months ago(0 children) It seems odd to use an end to end framework for one of the fields with the least end to end solutions (non-differentiable environment, reward, actions, etc).   []cwaki7 10 points11 points12 points 7 months ago(1 child) Really? It seems to be that pytorch is especially better in RL because of the way the graph computations are done and how easy it is to do some logic on the cpu then seamlessly go back to computing on the gpu. I learned tensorflow a long time ago and had little knowledge at the time so perhaps I missed the benefit tf has to offer in RL?   []koolaidman123 0 points1 point2 points 7 months ago(0 children) i mean that there's comparatively few resources for rl using pytorch. most tutorialsclasses all seem to use tf so it was hard for me to find nice implementation of algorithms done in pytorch  []p-morais 8 points9 points10 points 7 months ago(4 children) I don't know. Using static graphs in RL feels clumsy and weird, like you're mixing two totally different languagesstyles of programming. I know some big groups use TF (namely BerkeleyOpenAI and Google) but I think there's enough resources on GitHub that switching to Pytorch should be painless.  []koolaidman123 2 points3 points4 points 7 months ago(3 children) i'd love it if you can point out some pytorch resources for learning RL! the only one i've found to be fairly popular is this one https:github.comqfettesDeepRL-Tutorials, but i'm not sure if there are otherbetter resources for code implementation  []p-morais 9 points10 points11 points 7 months ago(1 child) https:github.comikostrikov is great  https:github.comvitchyrrlkit is also good I think those two cover most mainstream algorithms, and are what I usually look at for reference.  []koolaidman123 0 points1 point2 points 7 months ago(0 children) that's fantastic, thanks! pytorch is infinitely easier to read than legacy tf code   []Nikhil_Iyer6 0 points1 point2 points 7 months ago(0 children) https:github.comnavneet-nmkpytorch-rl  []MoreDonuts 10 points11 points12 points 7 months ago*(13 children) Pytorch pros and cons: Pros: Better syntax, ease of use, flexibility Cons: No Flatten layer included, because the devs really hate Sequential Edit: Disgusting cancer according to Pytorch devs: Instead, feast your eyes on the best practices:  []jcjohnss 6 points7 points8 points 7 months ago(9 children) One of the first things I do in a new PyTorch project is usually implement Flatten and GlobalAvgPool layers so I can write the former: https:github.comfacebookresearchclevr-iepblobmasteriepmodelslayers.py#L51 https:github.comgooglesg2imblobmastersg2imlayers.py#L62  []MoreDonuts 1 point2 points3 points 7 months ago(8 children) Yeah, same. I'm sure we're not alone! The devs don't want to implement it because of some bizarre application of the slippery slope fallacy.   [][deleted] 7 months ago*(7 children) [deleted]  []MoreDonuts 0 points1 point2 points 7 months ago(6 children) You can't use it with Sequential, so you either have to write your own Flatten every time, or not use Sequential. The point is that the devs really hate Sequential, even though its declarative style is ideal for many use cases.   [][deleted] 7 months ago*(5 children) [deleted]  []MoreDonuts 1 point2 points3 points 7 months ago(4 children) I just find things written with Sequential to be much more readable and easy to work with. It reminds me of my previous time in fields that actually cared about software engineering, lol   []dpineo -1 points0 points1 point 7 months ago(2 children) I suppose Sequential is like ML's version of the goto statement. It's a perfectly good programming structure that is sometimes the ideal tool to concisely deal with the job at hand, but there's people that for some reason would rather twist and torture their code in an effort to avoid it.  []epicwisdom 2 points3 points4 points 7 months ago(0 children) Sequential makes the easy thing easy. There are no added complications; it only limits what you can do without making larger changes, if in the future Sequential isn't a good fit. goto might make some things deceptively easy, but it also adds significant complications. I've never thought to myself, Wow, a goto here would make life so much easier. (Obviously this doesn't hold if you deal with very low-level programming, but I doubt that includes much of the rmachinelearning audience.)  []RUSoTediousYet 0 points1 point2 points 7 months ago(0 children) It's not that hard to make a Flatten (or in general, a Reshape Layer) though :>.   []Billy737MAX 0 points1 point2 points 7 months ago(1 child) How nice and pretty the code looks doesn't matter, the quality of the API is what matters, and that above example seems very limitedunextendable  []MoreDonuts 0 points1 point2 points 7 months ago(0 children) You're probably not thinking about it the right way. It requires declarative patterns rather than imperative ones. It kind of reminds me of React js, for instance. I might do a blog post at some point.   []bartturner 9 points10 points11 points 7 months ago(21 children) TF is more popular but this move helps with the biggest weakness with TF. It is a lot harder to find things that use PyTorch. Most use TF.  []BeatLeJuce 18 points19 points20 points 7 months ago(20 children) I guess this strongly depends. I personally feel like almost no-one uses TF anymore. Everyone I know and every research paper I read in the last year or two (that isn't from Google) uses PyTorch.  []soraki_soladead 13 points14 points15 points 7 months ago(3 children) This is probably strongly based on field. I almost exclusively see TF papers but if you're in NLP I could see Pytorch appearing more popular there since RNNs in TF were terrible for a long time.  []mmirman 1 point2 points3 points 7 months ago(2 children) I'm working in adversarial (as in security) ML, and RL. The vast majority of the new non-googledeepmind papers that I've seen have been for PyTorch.  []soraki_soladead 0 points1 point2 points 7 months ago(1 child) Out of curiosity, excluding GoogleDeepMind and OpenAI, which RL labs use PyTorch?  []mmirman 0 points1 point2 points 7 months ago(0 children) Most of the interesting RL papers I'm concerned with haven't been coming out of \u201cRL Labs as much as a wide variety of individual research groups studying adversarial ml at universities. its true though that googledeepmindopenai do have an outsized impact with the small percentage of the papers they do contribute though.   []techlos 8 points9 points10 points 7 months ago(12 children) from my own experience - people who mostly work in development and deployment tend to use tensorflow, whereas the research crowd has always been about torchpytorch. TF seems great if you're running a model across dozens of TPUs and write code focused on scalability... but at least for me, it's absolutely terrible for rapid prototyping of ideas. Try implementing the backpropamine paper in old TF, it's a nightmare to get it working properly  []UncleOxidant 5 points6 points7 points 7 months ago(8 children) Trying to read old TF code is a nightmare as well.  []chocoladisco 11 points12 points13 points 7 months ago(7 children) Trying to read research code is a nightmare in general.  []UncleOxidant 2 points3 points4 points 7 months ago(6 children) Indeed. Why is coding such a neglected skill among academics?  []samsungofme 9 points10 points11 points 7 months ago(5 children) for most academics in MLAI, code is secondary, and more of a proof of concept and a necessity for running experiments. that code is abandoned as soon as your paper's published, the project funding it drops, the graduate student who wrote it leaves your group or loses interest in supporting it, etc. The story changes if you are a systems researcher, or being funded by an agency like DARPA that really wants code (only somewhat, though). At the end of the day, we're academics, not coders; we get paid for developing new ideas, not writing great code.   []UncleOxidant 7 points8 points9 points 7 months ago(2 children) we get paid for developing new ideas, not writing great code. As someone working in a research group in a large corp, I understand that. However, as someone who's lately had to take some of that academic code and try to make heads or tails of it, I also think that maybe academics might do well to develop their coding skills. Lots of papers are being published in ML now with code (made available on github usually) and results - the code is really as important as the text of the paper for reproducing the paper's results. We're finding a lot of cases where we run code from the paper and yet cannot reproduce the results from the paper. We also find cases where the text of the paper is lacking in describing how the actual algorithm works - so we look at their code to try to figure out what they were intending.   []techlos 6 points7 points8 points 7 months ago(1 child) The worst part is researchers not commenting their code. It barely takes any time to write a couple of lines to explain what a chunk of code is doing, and yet a good chunk of research code has barely anything to let other people know what's going on. I'd take a thousand lines of well commented spaghetti code over a thousand lines of neat, undocumented code any day.    []mmirman 4 points5 points6 points 7 months ago(0 children) A major source of citations is from code adoption, and code adoption comes from having maintained good code. Bigger and better institutions are highly incentivized to push for maintained code as they need students who go on to be professors with lots of citations to maintain their status as top institutions.  []cafedude 2 points3 points4 points 7 months ago(0 children) But the code is big part of communicating your new ideas these days. If the code is inscrutable you're not getting your ideas across effectively.   []sseveran 1 point2 points3 points 7 months ago(2 children) This seems pretty true. We are a TF shop and that is not likely to change, in fact we are investing more into adopting and extending things like tensorboard and TFX. I still appreciate the pytorch is out there even if we would not use it as it's useful to have multiple tools exploring the design space. I would not want a mono-culture. I think TF is fine rapid prototyping if you work in right way. As someone with years of Haskell experience TF has been pretty natural, static compilation and lazy which was pretty easy to work with. We also write a lot of tests. I hated the original queues and like tf.data a lot better. XLA is cool and powerful but under documented. I still prefer estimators to Keras but maybe in TF2 keras will have all the same capabilities.  There is still a significant amount papers being written that use TF and I don't really understand assertions otherwise. Still its super easy to port pytorch to TF usually so its not a big deal when something is written in pytorch.  []techlos 6 points7 points8 points 7 months ago(1 child) I think TF is fine rapid prototyping if you work in right way i'm going to have to respectfully disagree, at least with a clarification - for rapid prototyping using already implemented modules, it's just as flexible. However, at least with the old version of tensorflow... well you're stuck with a static graph. I've been working on a hybrid reinforcement learning method where the weights, layer sizes, layer types, and recurrence of the network are themselves defined within a CPPN - essentially, the network architecture changes on the fly, similar to the hyperNEAT idea, but without a fixed base network and using gradient descent rather than neuroevolution.  With pytorch, it was pretty easy to find a method to insert or remove layers on the fly, and while it was tricky i did find a working method so that the structure of the model itself was differentiable. I wouldn't even know how to approach the idea with tensorflow, because so much of the design behind it is based off the assumption that you define a model, and then you train that model. Static graphs are great for efficient use of computation resources, but they're limiting.  []sseveran 1 point2 points3 points 7 months ago(0 children) I mean that will always be an issue with a static graph, which is the one real reason to use pytorch over TF, and one which is very under discussed. In TF it would be quite painful (or at least a lot of work) since you would need to build some of your own abstractions. If there is no base network to work from then indeed one would be really trying to do something that TF was not designed to do. And I applaud your actual criticism instead of what many people say which is something along the lines of TF is stupid and no one but google uses it.  []bartturner 2 points3 points4 points 7 months ago(2 children) TF is easily the most popular. Plus YT videos and even University use TF more often than anything else. Same with examples and articles, etc. Fixing the API will only increase. TF has over 120k stars on GitHub https:github.comtensorflowtensorflowtensorflowtensorflow - GitHub  []MasterSama 11 points12 points13 points 7 months ago(0 children) Usually those who stared TF, dont go back and remove it when they start using a new framework!  Apart from that, What differentiates PyTorch from TFKeras IMHO, is the beautiful syntaxAPI it offers.  Its just lovely, they way everything is set is just lovely. I came from a MS C#dotnet background and I just love it.  Its very neat and straight forward thats just lovable.  Still, this is great news, Keras API is 100 times better than the tensorflows lowlevel API.  As long as the companies behind these frameworks, don't screw us all, all competition is welcomed and appreciated greatly :)  []BernieFeynman 3 points4 points5 points 7 months ago(0 children) Idk what universities you are referring too, pytorch is def used the most in academia. The litmus test is whatever stanford uses, which is pytorch for most dl classes.  []DeepBlender 0 points1 point2 points 7 months ago(0 children) TensorFlow users.  [][deleted] 7 months ago(1 child) [deleted]  []BastiatF 0 points1 point2 points 7 months ago(0 children) But it's no longer the research standard  [][deleted] 7 months ago*(5 children) [deleted]  []dtrillaa -2 points-1 points0 points 7 months ago(4 children) TF 2.0 is going to be the exact same API as PyTorch 1.0 essentially  [][deleted] 7 months ago*(3 children) [deleted]  []DeepBlender 1 point2 points3 points 7 months ago(2 children) Not sure if I can follow you. According to my understanding, you basically wrote that TensorFlow 2.0 is not going to be good because the pre-release version has technical issues?  [][deleted] 7 months ago*(1 child) [deleted]  []DeepBlender 0 points1 point2 points 7 months ago(0 children) As I am working only with a single GPU, I can't say anything about multi GPU issues. However, for single GPU, it works like a charm (with the usual pre-release flaws here and there). While the performance is comparable or better in several of my cases, the real benefit is that it is a lot simpler to work with it. The code overall is way more readable and experimenting is a lot easier. If I screw something up, the error messages are not anymore cryptic, but they point you to the right place. TensorFlow 2.0 allowed me to work on the problem I care about. So far I didn't have to fight with obscure technicalities.  []MasterSama 10 points11 points12 points 7 months ago(0 children) So the masochist engineers behind tensorflow are fired now? is over for good?! seems to me Pytorch had a great impact on TensorFlow to change path! like this.  This is great news for everyone! the porting from one framework to other (basically from all others to Tensorflow! and vice versa) is now a breeze!  Down with the cursed TF1! looking forward to more beautiful APIs in keras now ( I really love PytorchMxNet APIs they look so natural and easy to useremember)  The bad thing all the great tutorials for TF1 now need to change to reflect the new change and it takes couple of months I guess. but still great news.    []ReinforcementBoi 8 points9 points10 points 7 months ago(12 children) PyTorch FTW \\m  []Overload175 1 point2 points3 points 7 months ago(11 children) Wait till you need to serve your model, TF is still superior in that respect.   []logicchains 9 points10 points11 points 7 months ago(0 children) Not if you want to serve it in C++. PyTorch gives you a nice header file and static library, TF gives you... Port your entire build system over to Bazel or shove off!.  []ReinforcementBoi 0 points1 point2 points 7 months ago(9 children) What do you mean by serve a model  []Overload175 0 points1 point2 points 7 months ago(8 children) Export it, deploy it to production. With TF serving you could deploy your model through a Docker container with a preinstalled TF Serving image. Does PyTorch offer an alternative to exporting the graph to ONNX?   []dpineo 9 points10 points11 points 7 months ago(5 children) PyTorch allows you to trace your model, essentially recording all the computations so they can be replayed on live data like a macro. The trace can be loaded with torchlib, which is the pure C++ library that's part of PyTorch, completely removing any Python dependency. I'm using it on my current project for deploying to production and it works great!  []Overload175 1 point2 points3 points 7 months ago(1 child) TIL, thanks! Is this a 1.0 feature?   []dpineo 3 points4 points5 points 7 months ago(0 children) Yes. See: https:pytorch.orgtutorialsadvancedcpp_export.html Also note that there is a related but slightly different PyTorch concept called TorchScript for dealing with data-dependent control flow: https:pytorch.orgdocsstablejit.html  []brunocas 0 points1 point2 points 7 months ago(2 children) Excuse my ignorance, but how feasible would be to run this c++ code on a micro controller such as teensy? I have a few ideas for a uC project and I've wondered about this.  []dpineo 0 points1 point2 points 7 months ago(1 child) PyTorch supposedly works on ARM, but you will need to build it yourself, and you still have to worry about your memory + time budget.  You might want to look into NVIDIA's new Nano board. It's a bit more expensive ($99), but I'm confident will give a much more problem-free development experience.  []brunocas 0 points1 point2 points 7 months ago(0 children) Ok I will have a look at the nano, it will be too big for most of my applications though.  []amnezzia 1 point2 points3 points 7 months ago(1 child) i really don't get this, if you are talking about dockers, how is deploying pytorch any different from deploying any other python model, be it scikit-lear, any gbm libs, or just some custom monstrosity, what is so special about that tf serving image?  []slaweks 0 points1 point2 points 7 months ago(0 children) Python not involved.  []Overload175 2 points3 points4 points 7 months ago(0 children) Google seems to be at that liminal stage right now, transitioning from v1 to v2. But updating the tutorials to include eager execution and tf.keras ASAP is a good idea, they're a bit of a mess at the moment.   []terrrp 5 points6 points7 points 7 months ago*(0 children) Tf1 was such a shit show. Every 3 months previous code didn't work. There at least like 4 official high level apis with ample samples that did not work after 1 month of publishing. The graph programming model was completely at odds with python and loading or fine-tuning was a pain in the ass. They kept adding new shit that only became more complex and felt like some marketed corporate product rather than an open source library. 0 reasons to use over pytorch, and they've already lost the favor of a large share of ppl like me who are not taking time to learn the 'beginner' keras. Not to mention pytorch has better internal docs and I can get any model working in c++ in minutes  []mrathi12 3 points4 points5 points 7 months ago(0 children) Amazing!  []Ularsing 4 points5 points6 points 7 months ago(0 children) Sweet. Tf.keras was the bomb. Glad to see they're going all-in.  []progfu 1 point2 points3 points 7 months ago(0 children) Am I the only one who is missing the tf.layers and static graphs? Yes I know they're still there and that I can trace them with @tf.function. But before we had nice declarative interface, and now everything is imperative.  []LegacyAngel 1 point2 points3 points 7 months ago(0 children) Everytime a post like this comes out, my decision to switch to pytorch ages ago makes me giddier  []MarxSoul55 3 points4 points5 points 7 months ago(0 children) I know folks here don't really like fchollet, but I think this is a great accomplishment for him. He created a really nice API (IMO) and it ended up being a big part of one of the most popular deep learning frameworks. I respect him for that, regardless of all the petty politics.  []pvl 2 points3 points4 points 7 months ago(0 children) Not all is bad in TF, the logo is cool. Oh wait, they changed it!   []BastiatF 3 points4 points5 points 7 months ago*(0 children) Or you could switch to Pytorch which has had a sensible API since the beginning, doesn't keep adding incoherent ones every other day and doesn't have a marketing department desperately trying to sell you fake revolutions.   []SyAbleton 0 points1 point2 points 7 months ago(0 children) I called this ages ago. https:www.darrenabramson.comthis-is-good-and-true No it wasn't my meme, so technically someone else called it before I did.  []rajarahulray 0 points1 point2 points 7 months ago(0 children) Such a big decision by Google... I just started with tensor flow...but it's good to know that Keras is the new Tensorflow...  []senortipton 0 points1 point2 points 7 months ago(0 children) Funny this should happen. I was reading a book about ML and python a month back and some dude interrupted me to say I should learn Keras.   []dziejopiswawel 0 points1 point2 points 7 months ago(0 children) gluon s better  []skool_101 0 points1 point2 points 7 months ago(0 children) ThanosFlow  []artificial_intelect 0 points1 point2 points 7 months ago(0 children) 3 months ago Tensorflow realized that Pytorch is about to rock their world. TF execs: Let's rewrite TF to effectively be Pytorch.... I mean let's make it usable. My friend makes this joke: With the tf 2.0 release was like, soooooo lets take TF and just make it a wrapper for pytorch.  []skhadem 0 points1 point2 points 7 months ago(0 children) I've used PyTorch for personal projects and have had to use TF for work recently. I loved the level of abstraction with PyTorch: subclassing nn.model and overriding .forward(), calling .zero_grad() and .backward() with each loop, you really understand what is going on while still using the powerful auto grad tools. Although it seems possible with tensorflow eager execution, you have to dig reallyyy hard to find how to do it. Torch has references for many levels of abstraction, but tensorflow only emphasizes their how to train a neural network in 10 lines. To me it seems PyTorch is starting to be geared towards industry as well as research much better; tensorflow is focusing on non-developers, and I don't see why.   []Pronoob_me 0 points1 point2 points 7 months ago(0 children) As someone who has recently started with tensorflow and have only heard about keras being a higher level api, could someone explain it?  []yangzhou95 0 points1 point2 points 7 months ago(0 children) Wasted too much time learning tf  []vajra_ 0 points1 point2 points 2 months ago(0 children) Tensorflow is SHIT. Knew it from the very first few lines of code I read of it. Kept alive by forceful money pumping by Google, wannabe nerds who just show off and don't really know anything and Asian seniors trying to code random GAN paper for their CVs and don't understand wtf they're doing. The amount of human effort that is being wasted in this shit is astounding.  []AhnoWrong -1 points0 points1 point 7 months ago(0 children) WOW! I fell from my chair!  []chessbucket -1 points0 points1 point 7 months ago(0 children) YAY  []examachine 0 points1 point2 points 7 months ago(0 children) Good news, interoperability is worth it :?  []bartturner -5 points-4 points-3 points 7 months ago(8 children) This was really smart on Google.  Fran\u00e7ois Chollet now works at Google and makes sense to use Keras.  []dfarber 5 points6 points7 points 7 months ago(7 children) i don't get it, google has like 1000 smarter people working on DL than fchollet. plus like 100000 who are better SW engineers.  []bartturner 16 points17 points18 points 7 months ago(6 children) He came up with a good API. Numbers do not always matter. Also being smarter does not always mean creative.  [+]dfarber comment score below threshold-14 points-13 points-12 points 7 months ago(5 children) yeah, but creative doesn't always mean good. and his api is dogshit.  []bartturner 5 points6 points7 points 7 months ago(3 children) I disagree. Did you prefer the TF API?  []NewFolgers 9 points10 points11 points 7 months ago(2 children) Which one? I kid, I kid (but not really).  []bartturner 5 points6 points7 points 7 months ago(1 child) How about any before 2.0? I also purchased an edge TPU to play and another reason glad to see this move by Google.  []NewFolgers 1 point2 points3 points 7 months ago*(0 children) I think you've misunderstood me [Edit: well, actually.. I think I probably misunderstood your reply to me, and you were just filling in the detail of what I'm alluding to a bit]. TensorFlow 1.x API is unquestionably a disaster (due to too much choice, and annoying complexity and overhead for most things, with no value gained in introductory uses). Reading the official introductions to TensorFlow, it reads like a parody. It leaves me wondering WTF I'm even supposed to begin in terms of doing anything, and it also fails to introduce what the hell is really going on with the graph for people interested in more advanced developmentdeployments, perhaps better tooling, etc. I say this after having completed projects using a few of its APIs, and working with others' code using a few of them. Today, I still feel the best way to learn TensorFlow is to learn PyTorch and then convert the code to TF. I hope that changes with TF 2.0.  []Dagusiu 4 points5 points6 points 7 months ago(0 children) The Keras API was the first one get it at least half-right. From a beginner's standpoint, it's night-and-day better than anything that existed before it.  [+]Jehovacoin comment score below threshold-31 points-30 points-29 points 7 months ago(0 children) With the change to TF2, it will be much easier to implement functions that move towards AGI. This is exciting. Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 2424 on r2-app-08a86a06ad3cd72de at 2019-11-20 21:51:39.277330+00:00 running a5676e6 country code: US. "
  },
  "25":{
    "title":"[D] Why is PyTorch as fast as (and sometimes faster than) TensorFlow? : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 1,926 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[D] Why is PyTorch as fast as (and sometimes faster than) TensorFlow? (self.MachineLearning) submitted 2 months ago * by student_at_uw Since both libraries use cuDNN under the hood, I would expect the individual operations to be similar in speed. However, TensorFlow (in graph mode) compiles a graph so when you run the actual train loop, you have no python overhead outside of the session.run call. In PyTorch, you are in Python a lot due to the dynamic graph, so I would expect that to add some overhead. Not to mention the fact that having a static graph means you can graph optimizations like node pruning and ordering operations. But in many benchmarks I see online, PyTorch has no problems keeping up with TensorFlow on GPUs. A specific example is the Adam implementations in both libraries: https:github.compytorchpytorchblobmastertorchoptimadam.py https:github.comtensorflowtensorflowblobmastertensorflowpythontrainingadam.py PyTorch has all the ops as you would expect. For TensorFlow in the {_resource}_apply_dense case (which is the common case, AFAIK), TensorFlow has a dedicated C++ implementation. So here, TensorFlow does not spend extra time in Python AND it has an optimized implementation in C++. In this case, why isn't the TensorFlow version straight up faster?  I've heard that PyTorch is better optimized on the cuDNN level. Can anyone provide more details about this? What's preventing TensorFlow from doing the same thing? The only optimization I know of is that PyTorch uses the NCHW format (which is better optimized for cuDNN) whereas TensorFlow by default uses NHWC. UPDATE: Looks like the new Volta GPUs perform better with the NHWC format (TensorFlow's default): https:devblogs.nvidia.comtensor-core-ai-performance-milestones  I saw these two discussions but did not see a satisfactory answer: https:www.reddit.comrMachineLearningcomments7ujc6yd_can_someone_give_a_technical_explanation_as_to https:www.reddit.comrMachineLearningcomments8iguawd_why_is_tensorflow_so_slow  UPDATE: Looks like the new Volta GPUs perform better with the NHWC format (TensorFlow's default): https:devblogs.nvidia.comtensor-core-ai-performance-milestones Post a comment!  []SR2Z 103 points104 points105 points 2 months ago(5 children) The key here is asynchronous execution - unless you are constantly copying data to and from the GPU, PyTorch operations only queue work for the GPU. Python calls to torch functions will return after queuing the operation, so the majority of the GPU work doesn't hold up the Python code. That moves the bottleneck from Python to CUDA, which is why they perform so similarly.  []huberloss 25 points26 points27 points 2 months ago(4 children) TF builds an execution graph that is then processed by the C++ backend, meaning that the only work you do in Python is setting up the graph (at least in TF1.x). I don't think TF will move data tofrom GPU unless needed (i.e., op1 runs on CPU and op2 on GPU and op3 on CPU -> this will result in copying tofrom GPU). TF has executors which will run everything asynchronously as well (meaning that independent nodes will be executed in such a way that you get to satisfying the output nodes ASAP).  Regarding OP's point, I actually don't know why one is sometimes faster. The only guess I can have is that the data format, or maybe the ways certain ops call CUDAcuDNN.  []student_at_uw[S] 13 points14 points15 points 2 months ago(2 children) Quick update on data format, looks like the new Volta GPUs perform better with the NHWC format (TensorFlow's default): https:devblogs.nvidia.comtensor-core-ai-performance-milestones. But, PyTorch still beats TF in many cases on Volta according to this: https:github.comu39kundeep-learning-benchmark.  []tedivm 7 points8 points9 points 2 months ago(0 children) The benchmarks you're pointing to are a year and a half old and use what are not considered to be older versions of, well, everything. I'd definitely take it with a grain of salt.  []huberloss 5 points6 points7 points 2 months ago(0 children) I guess another issues is what instruction set are those libraries compiled against also... I can imagine that an AVX enabled build will be faster than one without that. Honestly there are a lot of variables that are involved so unless you control the full tool chain you can't compare things correctly. Another example ... in anaconda I can't find pytorch and TF linked against the same version of cuDNN or CUDA.  []piadodjanho 2 points3 points4 points 2 months ago(0 children) The data input pipeline in tensoflow is separated from the training logic. If you don't follow the optimization technics described on the tf docs, it is easy to to starve the GPU (or TPUs).  []entarko 56 points57 points58 points 2 months ago(6 children) Like you said, both libraries use cuDNN so most of the algorithms used at a low level are similar.My understanding is that the execution in PyTorch is, by default, asynchronous. What this means is that a python command that executes something on GPU makes a call but does not wait for the result of that call, unless the very next operation needs that result. So the only difference in speed for GPU operations is the time needed by the python calls, which in total is small compared to the actual computations on the GPU.This does not explain why sometimes PyTorch is faster, and this comes in part from the NCHW (although it is not always true because NHWC is better optimized for convolutions with groups which need concatenations on the C axis) but also from the choice of the algorithms for different operations (benchmark flag of cuDNN).  []student_at_uw[S] 12 points13 points14 points 2 months ago(4 children) Interesting, I didn't know about the benchmark flag (discussion here)! That's a neat optimization; I'm curious as to why this flag doesn't exist in TensorFlow. Perhaps the graph compilation does the low-level algorithm selection for you?  []entarko 11 points12 points13 points 2 months ago(2 children) Probably, although I am not very familiar with TF (inability to have deterministic behavior was a no-go for me). You need to be careful with the benchmark flag, it is not always the best thing to use and can make computations much slower than not using it. Also take a look at the deterministic flag, which typically uses more memory but results in deterministic behaviors and most of the time a speed improvement compared to no flag at all.  []realhamster 0 points1 point2 points 2 months ago(1 child) With deterministic you mean setting the rng seeds right? Not sure why I had assumed that TF had better support for deterministic behaviour than pytorch, as far as I am aware there are some pytorch operation that don't support being deterministic, but its good to know that TF also has problems with this!  []entarko 0 points1 point2 points 2 months ago(0 children) Nope. I mean obtaining the same result for the same computation. There are indeed some operations that don't support it but I don't need them. In TF, AFAIK there simply is no way of doing it.  []dbalchev 2 points3 points4 points 2 months ago(0 children) It exists, but it's not documented and it's On my defaulthttps:stackoverflow.comquestions45063489first-tf-session-run-performs-dramatically-different-from-later-runs-why  []soulslicer0 0 points1 point2 points 2 months ago(0 children) yes, and you can use cudasynchronize to sync your stuff. The C++ Pytorch API gives you even lower control like cuda streams  []BatmantoshReturns 17 points18 points19 points 2 months ago(2 children) While we're on this topic, does anyone know why the huggingface pytorch Bert so lightweight compared to the Tensorflow versions. People have trouble running the official Tensorflow Bert, even on multiple powerful GPUs, where as Huggingface Bert runs smoothly on a Colab k80 GPU.  []kale_divergence 1 point2 points3 points 2 months ago(1 child) I think the Huggingface version is stripped down significantly, with just bare pytorch and a model loader + tokenizer  []BatmantoshReturns 2 points3 points4 points 2 months ago(0 children) Anyone know what parts are stripped down? As far as I can tell it has the same architecture.  []patrickkidger 7 points8 points9 points 2 months ago(5 children) Not directly related to PyTorch vs TensorFlow, but since NCHW vs NHWC got brought up, I'm hopeful someone here might know the answer... Why is it that batch size N is first and not last? It seems to me that having it last would typically give a speedup due to cache locality. The fact that most datasets aren't in that format shouldn't be a big deal: the overhead of a single copy into an N-last format (i.e. prior to feeding it into a neural network) should be minor compared to the benefits of having an N-last format throughout the rest of the computation.  []programmerChilli 5 points6 points7 points 2 months ago(1 child) I think you're right in that putting batch dimension last could allow for some very interesting optimizations (think about all the vectorization). However, what happens if you have a small batch size? Think a batch size of 2 (maybe 3 would be even worse). All of a sudden the amount of vectorization you can do along the batch dimension is miniscule and you've spread the rest of your data for not much gain. There's actually a couple of frameworks that used this format back in the day. Check out Neon from Nervana. Actually, looking around just now, I found this very interesting comment from someone who worked on Neon. I think it's roughly what I said, although it seems the reality is a lot more complicated. https:github.comsoumithconvnet-benchmarksissues93#issuecomment-192621350 I guess the main answer is that people tried, but for a confluence of reasons, some politicalsome technical, we've currently settled on batch size as the first dimension.  []patrickkidger 1 point2 points3 points 2 months ago(0 children) I take your point about small batch sizes. I think that it should be possible to efficiently vectorise even then by repeat interleaving the kernel but it's true that sounds like less of a sure-fire thing. Thanks for the link, that was very interesting reading!  []ProfessorPhi 1 point2 points3 points 2 months ago(0 children) Hmm, my initial thinking is that it's a row major vs column major thing. Neural nets in Julia use n in the last dim.  []Ecclestoned 1 point2 points3 points 2 months ago(0 children) Yeah you have your orders reversed. NCHW is a width-major format, i.e. the width is what is packed first. Each cache line will contain sequential spatial data for a single channel of a single input, not independent batches.  This means that the locality of convolutions in the width and height dimensions is exploited first, with the operations naively parallelized across batches (because they are totally independent).  That being said, if you look at the cuDNN library calls, it will sometimes auto-magically convert to an NHWC format under the hood. This is all part of the automatic algorithm selection that it does and works with practically all frameworks. It might end up using matrix multiply, Winograd, or FFT convolutions depending on dimensions.  []LtCmdrData 1 point2 points3 points 2 months ago*(0 children) I think you have your thinking reversed or I don't understand what you mean.  cuDNN Tensor descriptors are represented in descreasing order of the strides (just like CC++Numpy arrays are). This is called row major or lexicographical access order. The last index has most locality, first has least. This is why N is first.  []geneing 1 point2 points3 points 2 months ago(3 children) It's surprising that TF is not able to optimize static graphs any better than the dynamic graphs of Pytorch and TF Eager. Anybody knows why?  []SR2Z 1 point2 points3 points 2 months ago(0 children) This presentation goes over some of the optimizations TF does. The basic gist of it is that a lot of these optimizations provide small benefits or are also handled when you write PyTorch code: TF discards nodes from the graph if they aren't involved in the output - this is a feature inherent to the lazy gradient evaluation in PyTorch, and it wouldn't calculate anything you didn't explicitly ask for anyways. TF does simple math optimizations like factoring and pre-evaluating constants that are really easy to do by hand in PyTorch. TF tries to re-order and merge nodes in the graph so that it can take better advantage of cache locality, but many of the CUDA operations that people do on the GPU already have their own schemes to minimize cache stalls. It's really hard for TF to make further meaningful optimizations after both the user and the CUDA guys have picked all of the low-hanging fruit.  []student_at_uw[S] 0 points1 point2 points 2 months ago(1 child) TF in graph mode is definitely much faster than TF Eager, but is no faster than PyTorch it seems. I hope someone has an answer, this is my main question as well.  []programmerChilli 2 points3 points4 points 2 months ago(0 children) Despite my downvotes on my comment :'(, I maintain that it's a combination of the facts that 1. Both rely heavily on operator libraries like cudnn, 2. Tensorflow inherently has a lot of overhead, 3. Tensorflow sometimes makes bad decisions.  You're not gonna find a single obvious issue. Tensorflow's static graph API gives strictly more opportunity to optimize. If there was a simple reason why questions like this keep popping up do you really think Google wouldn't have fixed it? They already changed their whole API because people like eager more - you think they wouldn't do what it takes to fix an obvious performance issue? Also, although not completely relevant since Tensorflow doesn't use XLA by default (I wonder why...), here's some examples of where XLA made things slower. This is for the people who disagreed that Tensorflow messing up could be part of the reason. https:stackoverflow.comquestions44523812tensorflow-xla-makes-it-slower https:github.comtensorflowtensorflowissues30791 https:devtalk.nvidia.comdefaulttopic1058729mixed-precision-and-tensor-corestensorflow-ssd_resnet50-runs-faster-on-tensorflow-gpu-than-xla-enabled-19-07-py2-container-  []un_om_de_cal 1 point2 points3 points 2 months ago(0 children) In my experience moving a project from PyTorch to TF, while the basic training operations - forward prop, backprop, update parameters - were faster in PyTorch, some more unusual operations involving argmax and slicing were way slower compared to TF. So maybe PyTorch is better optimized for the more common operations that are typically benchmarked, while TF is optimized for a wider range of operations? Another thing that comes to mind is that the Pytorch tutorials do the data augmentation on the CPU, while the TF ones on the GPU (at least the ones I saw some 1-2 years ago when I started working with it). If you implement your project like this you will do some of the computing on the CPU, which could be more efficient as long as you don't run out of CPU. (I've had the problem of the CPU becoming a bottleneck).  [+]programmerChilli comment score below threshold-10 points-9 points-8 points 2 months ago(12 children) What I've heard is that Tensorflow tries to do a lot of optimizations automatically, and usually gets at least one of them wrong. Thus, it ends up having subpar performance in practice compared to PyTorch.  []piadodjanho 11 points12 points13 points 2 months ago(11 children) I don't think this is accurate.  []programmerChilli -2 points-1 points0 points 2 months ago*(10 children) Can you elaborate? If anything, I thought my comment was too generically true.  []piadodjanho 4 points5 points6 points 2 months ago*(9 children) If you know the processing unit architecture you can generate code that is proven to be faster than the naive implementation. In some cases, the performance won't be optimal because the cost of optimization might be too large (like in ML, you can fall into a local minima). Each vendor focus in creating the best numeric library for your system. For instance, MKLL for Intel CPUs, CuDNN for NVdias GPUs and XLA for Google TPUs. But the XLA is also optimized to generate code that run fast at high end GPUs as well. But as today it performs terrible in Intel GPUs, but better if nothing was done. By the way, AFAIK the XLA is the only one that actually does optimization. The others are only implement a tunned kernel. In theory (I haven't look), you can use XLA with CuDNN or MKLL for example. If you are comparing apples to apples, the optimize code will always run faster. There are a few really good optimizer with tuning capabilities to achieve great performance, such TVL and Halide with gradients. Both perform better (as today) than tf or torch. TVL can be used in conjunction with others frameworks as well.  []programmerChilli 5 points6 points7 points 2 months ago*(8 children) First of all, XLA utilizes MKLDNN on CPU's and CuDNN for Nvidia GPUs. The only place where they (by necessity) implement their own kernels is for TPU's. Second, I think you're confusing a couple different concepts here. XLA is not comparable to MKLDNNCuDNN. MKLCuDNNEigencublas are operator libraries - they're handwritten implementations of basic operations often provided by the vendors. XLA is a whole IR (like LLVM) which compiles down operations and tries to optimize them. It's true that XLA actually does code-gen for your framework, but it's still a distinctly higher level of abstraction than the other libraries you mention. Also, I don't know why you're so confident that optimized code will always run faster than non-optimized code. For example, it's well known that -O3 will often run slower than -O2 for CPP compilers. Optimization isn't some magic flag that you apply to your code that makes it run faster - it involves making tradeoffs that you hope will speed up your code. Re: your last point. Halide is a completely different approach to optimizing image pipelines. It simply separates the functional description from the schedule. It has an autotuner that can try to automatically come up with your schedule. TVM is essentially Halide + some operator libraries + some other goodies. Basically, it's a completely different model from the other things you mentioned. Reading over your comment, I guess my main question is: what in your comment contradicts what I said other than the statement about optimized code running faster?  []piadodjanho -2 points-1 points0 points 2 months ago(7 children) Most of the thing you pointed out have been addressed in your comment have been addressed in my OP. First of all, XLA utilizes MKLDNN on CPU... Second, I think you're confusing a couple different concepts here. XLA is not comparable to MKLDNNCuDNN. What I said: By the way, AFAIK the XLA is the only one that actually does optimization. The others are only implement a tunned kernel. In theory (I haven't look), you can use XLA with CuDNN or MKLL for example. Then, you said: I don't know why you're so confident that optimized code will always run faster than non-optimized code.  What I said: If you are comparing apples to apples, the optimize code will always run faster. We are talking about fusing, alignment, storage optimization etc. I thought this could be inferred by the context. Next time I will try not to make assumptions about the reader. What you said: Basically, it's a completely different model from the other things you mentioned. What I said: There are a few really good optimizer with tuning capabilities to achieve great performance, such TVL and Halide with gradients. Both perform better (as today) than tf or torch. TVL can be used in conjunction with others frameworks as well.  []huberloss 2 points3 points4 points 2 months ago(3 children) FYI - before getting to XLA grappleralso does graph rewriting.  []piadodjanho -1 points0 points1 point 2 months ago(2 children) XLA is a numeric library. I shouldn't know about the graph.  []programmerChilli 2 points3 points4 points 2 months ago(1 child) XLA definitely does graph rewriting. It's not the same graph as the Tensorflow graph, but it has enough information for them to do graph level optimizations. Search up XLA HLO. That's what they call their IR.  []piadodjanho 0 points1 point2 points 2 months ago(0 children) The input language to XLA is called HLO IR, or just HLO (High Level Optimizer). From XLA Architecture. But you are right, it does preserve some information that could potentially be used to reconstruct the graph.  []programmerChilli 1 point2 points3 points 2 months ago(2 children) Your comment is still confusing me. Let's focus on the only relevant point as opposed to the other points I'm contending. I'll fuse your context into your initial comment. You:  If you are comparing apples to apples, the optimize code will always run faster... We are talking about fusing, alignment, storage optimization etc. Me: I don't know why you're so confident that optimized code will always run faster than non-optimized code.  I don't understand how adding the context addresses the point. You don't always want to fuse ops, and storage optimization basically describes 90% of the decisions in Halide schedules. My point stands. It's possible to apply fusion or storage optimizations and make your code slower.  []piadodjanho 0 points1 point2 points 2 months ago(1 child) I mean, technically things go awary. But if you know the computer architecture, you can create a darn good scheduler, especially if you only have to worry about the limited use case of neural networks. Honestly, I don't know the intricacies of XLA compiler. I'm talking about my experience with other schedulers. Also, the tensorflow can mess up in the graph optimization before passing to the XLA. But I still find hard to believe the a -O0 XLA code will run slower than -O3 code. Maybe if you do something very unusual... IDK. XLA is different normal compilers, but I can see a non XLA code running faster than a -O3 XLA code though. But this is apples to oranges comparation.  []programmerChilli 1 point2 points3 points 2 months ago(0 children) I suppose that if we're simply talking about the performance of Tensorflow vs PyTorch, XLA isn't even relevant (it's not on by default and many benchmarks don't enable it). I think you're completely right that it's possible that the graph is messed up before it even gets to XLA. Tensorflow has many optimization passes, and it's not clear how they interplay with each other. Fundamentally, there's no reason why PyTorch should be faster than Tensorflow. PyTorch's eager mode doesn't give a lot of room for optimization (no graph to optimize over), not compared to the years of effort that Google has put into graph optimizations. Really, the only reason Tensorflow should run slower than Pytorch on a benchmark is either A. The overhead is too high, B. They're messing up somewhere.  [+]CyberDainz comment score below threshold-8 points-7 points-6 points 2 months ago(2 children) faster , slower, better, stronger. Anyway I bet on google rather than facebook.  also plaidML faster than both torch and tf.  []diggerdu 0 points1 point2 points 2 months ago(1 child) plaidML incredible, opencl slower than highly optimized CUDA  []CyberDainz 0 points1 point2 points 2 months ago(0 children) my tests show different. plaidML generates superior optimized gpu kernel, this is why it faster than other frameworks.  [+]jacksparrowlegends comment score below threshold-29 points-28 points-27 points 2 months ago(0 children) because package written in c++ . c++> literally everything else in speed Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 17941 on r2-app-030c3614bd34d7a45 at 2019-11-20 22:45:11.869977+00:00 running a474380 country code: US. "
  },
  "26":{
    "title":"[Discussion] Next AI winter? Do you think that deep learning solutions will be used in production? : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 1,847 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[Discussion] Next AI winter? Do you think that deep learning solutions will be used in production? (self.MachineLearning) submitted 13 days ago by jasabdom MLAI is gaining traction, but actually not a lot of solutions are being deployed in production, especially in corporations. In startups sure, because very often the main product is AI-based, but I have serious doubts whether regular corporations will continue investing in machine learning engineers. There is still enthusiasm, but not a lot of successful stories, both in CV and NLP (https:medium.comtooploox-aihow-to-optimize-your-operations-with-ai-nlp-d3180f2670e3). Post a comment!  []poiluducul 19 points20 points21 points 13 days ago(5 children) CV using MLAI is used by A LOT of major companies in production. Every phone manufacturer use some form of it for there camera.Every car companies is using it for self driving car.Many industrial manufacturer use it for default detection and more.Sotware companies like Adobe are pushing it with AI Sensei.Any security based on camera's uses it.John deere use of AI in the agricultural market is almost unmatched (and consurning). That's just of the top of my head and without talking about what FANGs do ...  []seanv507 2 points3 points4 points 11 days ago(0 children) I guess we disagree on terms. ML AI solutions are definitely being used by the big manufacturers.. Samsung etc This will continue.. so there will be no ai winter. However, the bubble that any company can improve their processes with ML will burst. It's only those corporation with a lot of data that benefit.  []elcric_krej 1 point2 points3 points 13 days ago(2 children) Every phone manufacturer use some form of it for there camera. Hasn't Google switched to a much simpler traditional signal processing algorithm for hdr photos ?  []poiluducul 3 points4 points5 points 13 days ago*(1 child) Hasn't Google switched to a much simpler traditional signal processing algorithm for hdr photos ? HDR is based on computational photography but their Pixel Neural Core doen't support tensorflow for no reason. HDR is not just a one step process, they can use ML during the image stabilisation, choosing wich shots to ignore, optimise the HDR parameters depending on the subject type, just the identification of the subject is ML and a lot more. And after the photo is taken the Google photo app takes over for classification, tagging, editting etc. And HDR is only one thing, stabilisation, denoising, night sight, color correction, depth detection, setting selection. ML is just one part of the equation, it is often just a complement to computational photography. And honestly the line blures a lot between ML and computational photography depending on the technics used. Specialy since MLAI doesn't actually means the same thing deping on who you ask or anything at all ^^  []roeschinc 2 points3 points4 points 12 days ago(0 children) Lots of Apple's camera processing uses ML as well and is one of the main applications using their ML HW accelerator in the phones.  []sfsdfd 1 point2 points3 points 11 days ago*(0 children) When it comes to incorporating machine learning and specifically machine vision into everyday computing, we're really at the beginning of the curve, not the end. This moment is equivalent to the Internet in 2006, maybe two years into web 2.0 - or computer graphics in 2005. It might have been easy to say: \u201cThe interesting and groundbreaking stuff has been done; what comes next is just boring incremental refinement. Neither claim would have been correct. The applicability and power of each new capability for computing just needed people who were creative and clever enough to do what others hadn't done yet. The modern era of machine learning has a long way to go before it even starts to slow down. The needs and priorities of the profession will change, just as they do in every rapidly evolving field. People who get on board now will begin the process of developing a long, deep knowledge base - by 2025, they'll be able to explain comprehensively how we got there. It will be both a technically and financially valuable skill set.  []KindaKnowKarate 4 points5 points6 points 12 days ago(0 children) There's still significant friction around infrastructure that holds companies back from using ML in production. The big gap for production ML software isn't around the ML itself, but around pipeline issues like model serving and maintenance, in my experience. Some of the most exciting projects in ML right now, imo, are around infra:   []mimighost 8 points9 points10 points 13 days ago(0 children) Google had improved their 10% of the search requests by using BERT: https:www.blog.googleproductssearchsearch-language-understanding-bert This alone could probably bring them like hundreds of millions of dollars. Winter? Maybe not, but cool down might be the word. The realization that we might have approach the limit what we can achieve within current paradigm, doesn't mean its winter, it is back to reality.  []theakhileshrai 2 points3 points4 points 12 days ago(0 children) Apple's iPhone has a dedicated machine learning chip this year. Google has lots of algorithms that are AI specific in pixel. How does this qualify as AI winter?  []elcric_krej 5 points6 points7 points 13 days ago(0 children) We're not entering an AI winter, we're entering a normalisation period where growth stops, or rather goes back to the standard 1-6% y rate of other domains. Salaries stop going up, new positions are scarce, you either need a PhD or some pretty impressive credentials to get hired in a semi-desirable position... Things which are standard in most scientific fields.  []phobrain 3 points4 points5 points 13 days ago*(2 children) The villagers will come with torches and pitchforks in the night to reclaim their souls, and you will blend in with them thanks to your wizardly use of full-on GPT-2, snatching them into your own sack and selling on to the highest anonymous bidder. Edit: .. and that bidder's last name will end in 'utin', so all the more sensible villagers will be directed into the most confusingly addictive, disabling relationships imaginable by any dating apps they attempt to use, and news about harm from something they do a lot like vaping might not make it into their news feed. You really have to break some eggs to rescue capitalism from people who think about alternatives, at least that's how they kept communism running so long. We are soooo much better at mind control now!  []farmingvillein 2 points3 points4 points 12 days ago(1 child) Gpt-2, is that you?  []phobrain -1 points0 points1 point 11 days ago*(0 children) [answering a different, offscreen question:]  I was busy once, and as far as I can tell, I haven't thought deeply since - the high is just too overpowering and you can never fully regain the perspective of indolence, so matter how hard you work at aping its forms, norms, odors, appearance, and attitude. Life is cruel but just -- and temporary, anyway, so Viva! For once, just try not changing an item of clothing for a year -- you'll see it doesn't work. :-( I was merely opting to ape gpt-2, for fun until accidentally inhaling my snot during a typing triathlon caused a paradigm shift, leaving me unable to see around things that aren't even there, but cautiously optimistic that we will somehow live within the machines we leave behind. Edit: just got matrix i,j vs. j,i traversalsorting right at last, pending verification, so I really need that ape job bad.  []Chocolate_Pickle 4 points5 points6 points 13 days ago(1 child) A lot of Business Intelligence stuff can be solved with data+regression.  You only commit time, effort, and money into fancy stuff when the basic stuff isn't good enough.  []Northwulf 0 points1 point2 points 12 days ago(0 children) Well said.  []Murillio 2 points3 points4 points 13 days ago(2 children) I don't know where you got the notion that dl isn't used in production. I've worked for multiple companies that use it in production - and no, it isn't just startups.  []jasabdom[S] 1 point2 points3 points 13 days ago(1 child) From my experience in banking industry, but I guess it is linked with interpretability and robustness issues. Banking is heavily regulated.  []poiluducul 4 points5 points6 points 13 days ago(0 children) From my experience in banking industry, but I guess it is linked with interpretability and robustness issues. Banking is heavily regulated. I have friends who worked for banks in different countries and they use a lot of general ML with time series, NLPCV for document classifications, NLPCV for identity verification and general ML for fraud prevention.  []yusuf-bengio 3 points4 points5 points 13 days ago(0 children) In my experience, CV is intensively used in industry and NLP is gaining momentum. In terms of classical domains, such as salesdata warehouse analysis, good-old-fashion Machine Learning models are still dominating. Domains where AIDeep Learning failed to deliver value to companies is Reinforcement Learning, automation (except CV applications), anything remotely related to safety and legally constrained applications.  []--iRON-- 2 points3 points4 points 13 days ago(0 children) Currently for CV nothing works better than CNNs and they are extensively used in production. Deep learning not only allows to create solutions that actually work, but oftentimes requires much less research and effort.  [+]TritriLeFada comment score below threshold-6 points-5 points-4 points 13 days ago(1 child) I think one of the reasons CNNs are not used in production is they are not robust, maybe because they have bad representations. For example those representations should be 3D, not 2D. Also our models should be able to detect by themselves the subparts of an image. They should also be naturally biased toward using shape and not texture.Having those properties would also make models more interpretable, which is another reason they are not used, I think.So I don't think the current architectures will be used in production.  []Taxtro1 0 points1 point2 points 9 days ago(0 children) But... they are used. Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 1294 on r2-app-0d287fb0f47e023d8 at 2019-11-21 15:41:03.749499+00:00 running 65cc4c1 country code: US. "
  },
  "27":{
    "title":"[D] Regression tasks with \"duplicate samples\" : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 1,858 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[D] Regression tasks with duplicate samples (self.MachineLearning) submitted 13 days ago * by Doo0oog Assume there is a data set {(x_i, y_i)}, 0<=i<n, there exist some samples that have the same x value but different y values (x_i == x_j && y_i != y_j) because there is noise when collecting data. A common method maybe grouping them with a single y, like their mean value. But are there any researches resolving this kind of data without grouping them together? If yes, what is this kind of problems called? Or some keywords for doing search. Thank you in advance! Post a comment!  []SwordOfVarjo 2 points3 points4 points 13 days ago*(3 children) This is an ill posed problem as there is not a unique solution (another way of saying this is that the conditional probability of your dependent data is multimodal). No need to group data with mean y values, an l2 loss will actually result in regressing to the mean anyway (which may or may not be acceptable, depending on the data and your goal).  []Doo0oog[S] 0 points1 point2 points 12 days ago(0 children) I'll do some search about this.  []clurdron 0 points1 point2 points 8 days ago(1 child) The conditional distribution doesn't have to be multimodal for this to happen.  []SwordOfVarjo 0 points1 point2 points 8 days ago(0 children) Yep, that's true, my original post was a bit unclear. It is sufficient to impose ill-posedness however.  []DaLameLama 2 points3 points4 points 12 days ago(1 child) Do a literature review on supervised learning with noisy labels. Sounds like that's what you're looking for.  []Doo0oog[S] 0 points1 point2 points 12 days ago(0 children) That's exactly what I'm looking for. Thanks a lot.  []M4mb0[\ud83c\udf70] 1 point2 points3 points 12 days ago(3 children) There are two cases:1. either it is just noise, in this case standard methods can do2. it is not noise but a real effect. The latter can happen when the dataset is missing some important features. For example consider building a model for the average height of a person, given their age. Girls, on average, hit puberty earlier than boys, while boys on average grow a bit bigger than girls when reaching adulthood. If the dataset does not contain the gender as a feature, it will appear that we are trying to model a multi-valued function. But there are actually models that can dirtectly deal with this situation such as mixture of experts. The key is that, what we model is the conditional probability p(y|x). Now usually, this will be an unimodal distribution such as a gaussian and we can simply predict the mean yhat(x) = E[y|x]. However, in the situation you describe it will be a multimodal distribution like for example a mixture of Gaussians. Then the mean is pointless, and instead we have to use for example use the modes as our prediction, and for a given x, there will be multiple predictions y_1, y_2,... . In fact this technique can allow us to discover that there missing features!  []Doo0oog[S] 0 points1 point2 points 12 days ago(2 children) You are right. And we just treat the second case as the source of noise. Could you recommend some papers or links about this case?  []M4mb0[\ud83c\udf70] 0 points1 point2 points 12 days ago*(1 child) I don't know how this is really handled in the state of the art, but a starting point could be to look at the mixture of experts model which is for example covered in Bishop.  []Doo0oog[S] 0 points1 point2 points 12 days ago(0 children) Thank you. I'll look for that.  []clurdron 0 points1 point2 points 8 days ago*(0 children) Classical statistical methods (which are the basis for a lot of ML) assume you have an error term in your dependent variable. If your independent variable is low-dimensional and can only take countably many values, as was often the case when these methods were developed, then you'll likely have identical x values. If you have identical x values, you'll encounter this 'duplicate sample' situation with probability 1. So this isn't a weird scenario. Almost all statisticalML methods deal with this situation without taking any special steps. Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 29608 on r2-app-08a86a06ad3cd72de at 2019-11-21 15:43:25.249577+00:00 running 65cc4c1 country code: US. "
  },
  "28":{
    "title":"[R] New Graph Classification Data Sets : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 1,856 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Research[R] New Graph Classification Data Sets (self.MachineLearning) submitted 13 days ago by nd7141 Graph classification has been popular recently, which led to rich development of Graph Kernels and Graph Neural Networks. All papers more or less verify the results on 10-15 benchmark data sets. We found that these data sets (and 40 others) have a lot of isomorphic graphs which leads to (1) train-to-test leakage and (2) incorrect validation comparison. Absurdly, some isomorphic graphs have different classification labels, making it impossible to classify correctly such instances. We explain the reasons why these isomorphic instances appear in data sets in the first place (e.g. meta-data, sizes of graphs, or origin of a data set) and open-source new clean data sets, both in GitHub and in PyTorch-Geometric.  Here is a link to the paper: https:arxiv.orgabs1910.12091  Here is more informal blog post about findings. Post a comment!  []matja22 5 points6 points7 points 13 days ago(3 children) Hi, I think so if two graphs are isomorphics, but those graphs nodeedges has different features, it's correct that has different labels.  []nd7141[S] 1 point2 points3 points 13 days ago(2 children) Yes, sure, adding nodeedge labelsattributes into account helps to distinguish the graphs in some cases, but not in all. Table 7 presents the number of isomorphic graphs after considering node labels during isomorphism testing. Also, some data sets don't have any extra information (e.g. IMDB).  []jmmcd 1 point2 points3 points 12 days ago(0 children) On the same issue - it's not absurd IMO. in the real world such data can arise. I can imagine an algorithm which eg fails to converge in this situation. So it's good to test algorithms in this way. In fact the inevitable error rate that occurs here is a known thing, the Bayes error.  []seismic_swarm 0 points1 point2 points 11 days ago(0 children) Yeah, good find, but it's not absurd. Lot's of datasets are impossible to predict correctly, and totally violate the principle of being a function; e.g., f(x1) = y1, f(x1) = y2, yet y1 == y2 in the dataset. This stuff slips in because we can't cull the dataset completely and oftentimes x and y are high dimensional. It's the job of the regression model to pick the best of the multiple options to minimize the overall loss function as much as possible. So maybe choose f(x1) = y1 everytime (instead of f(x1) = y2) if there are more instances of the first kind. If that all bothers you, then don't do regressionclassification, calculate the full posterior probability distribution.  []TotesMessenger 0 points1 point2 points 11 days ago(0 children) I'm a bot, bleep, bloop. Someone has linked to this thread from another place on reddit: If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads. (Info Contact) Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 24557 on r2-app-00c1e7e3b6ef84d9b at 2019-11-21 15:46:35.636549+00:00 running 65cc4c1 country code: US. "
  },
  "29":{
    "title":"[D] To use triplet loss or not when classes labels are given. Question about theoretical\/experimental expectations. : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 1,899 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[D] To use triplet loss or not when classes labels are given. Question about theoreticalexperimental expectations. (self.MachineLearning) submitted 13 days ago by kmkolasinski Hi, I look for some theoretical (or experimental) evidences for the superiority (or not) of the triplet loss over cross-entropy loss. Do you know some research papers which try to benchmark following setup? Since the dataset and model architectures are fixed (I assume all other hyperparameters are also fixed, maybe expect learning rate and number of epochs), we will have two models trained to minimize different objectives. I wonder if there is some common knowledge to answer following questions: TLDR: different objective function should result in different models. Can we expect the performance differences without training any model? Post a comment!  []atlatic 9 points10 points11 points 13 days ago(1 child) This is exactly what you're looking for: https:arxiv.orgabs1902.09229  tl;dr: triplet loss is worse than cross-entropy with labels. The former is an upper bound on the latter.  []kmkolasinski[S] 0 points1 point2 points 10 days ago(0 children) Great and thanks! Indeed I was looking for such type of paper.  []melgor89 2 points3 points4 points 12 days ago(1 child) Some time ago best option was to use both at once as triplet loss convergence is slower than pure softmax. Now, there is common belive and theorem tham pure softmax with implementation like in ArcFace is much better than any triplet loss.  []kmkolasinski[S] 0 points1 point2 points 10 days ago(0 children) Thanks a lot for pointing me to the ArcFace paper.  []david_picard 1 point2 points3 points 13 days ago(2 children) How do you evaluate your model trained with the triplet loss? If you use anything else than a k-nn, then I would bet you are going to get lower accuracy than the model that has an embedded classifier and is trained for that task. Remember that metric learning has some origin in improving k-nn classifiers (see the old LMNN paper by Weinberger), which makes it perfectly suited for the few-shot task, but not really for regular classification. In a nutshell, the loss that works best is the one that corresponds to your evaluation setup. In ML, you are already optimizing on the wrong data (compared to regular optimization), you should try to avoid also optimizing on the wrong problem.  []kmkolasinski[S] 0 points1 point2 points 13 days ago(1 child) Thanks for comment. For example I want to create a search engine, should I label images with classes (which is hard when number of classes is very large) or should I label pairstriplets (which is much easier but requires more data). You were right that model trained with triplet loss was worse in the k-nn benchmark in my simple experiments. But the question is if one can guarantee it will be always worse regardless dataset, architecture etc. I'm asking for the existence of general rule. Additionally, what about new classes which will appear every day in my system, which approach is more suitable for such case etc. I'm looking for some papers which try to look at both approaches and discuss their potential advantages.  []I_draw_boxes 1 point2 points3 points 4 days ago(0 children) I'm asking for the existence of general rule. Softmax takes into account all classes in each gradient update.  Additionally, what about new classes which will appear every day in my system, which approach is more suitable for such case etc. Retrain the model with the new class or if possible simply deal with the embedding vector in a one shot learning approach. Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 21963 on r2-app-02bc31126f0e126dc at 2019-11-21 15:48:19.868768+00:00 running 65cc4c1 country code: US. "
  },
  "30":{
    "title":"[D] Please help me find this paper (Foundations of DL)! : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 1,721 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Discussion[D] Please help me find this paper (Foundations of DL)! (self.MachineLearning) submitted 12 days ago by HenryWJReeve Dear all, I have been searching for some time now for a paper I read a while ago but misplaced.  The paper was very interesting and showed that finding optimisation with with deep neural networks is, in some sense, easier than with shallow neural networks. The authors generated a data set by generating random input data and then using the predictions of a shallow neural network (A) to provide the ground truth labels of those data. They then tried to train another shallow network (B) with same architecture as (A) the one that created the labels, but with different initializations. It was shown that it was very difficult to find the optimal solution for this dataset. They then tried the same task with a deeper network (C) and found the optimal solution. If anyone knows the name of this paper then please let me know where I can find it. I would be eternally grateful! Many thanks! Post a comment!  []Z13Wolf 4 points5 points6 points 12 days ago(1 child) It's not The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks right?  From my understanding of this paper they are at least somewhat related. They mention Contemporary experience is that the architectures uncovered by pruning are harder to train from thestart, reaching lower accuracy than the original networks.1, might be one of the papers they quote in the first footnote.  []HenryWJReeve[S] 2 points3 points4 points 12 days ago(0 children) Thanks! I don't think this is the original paper we were after, but it is definitely a related phenomenon.  []jeremycohen_ml 4 points5 points6 points 12 days ago(1 child) See the 10:40 mark here: https:www.youtube.comwatch?v=uC2IGoTE2u4, though in the experiment reported there, (C) was _wider_ than (A), not deeper.  []HenryWJReeve[S] 1 point2 points3 points 12 days ago(0 children) Thanks! This looks very relevant. I will check this in detail.  []testable313 1 point2 points3 points 12 days ago(1 child) If I recall correctly, there's a few papers that describe this phenomenon  []HenryWJReeve[S] 1 point2 points3 points 12 days ago(0 children) Can you remember the namesauthors? Thanks.  []TSM- 1 point2 points3 points 10 days ago*(0 children) I came across this quote and reference and, maybe it is the one you were looking for. Recent studies have begun to formalize the expressive power of deep versus shallow networks, finding that classification boundaries acquire local curvature and expressivity as an exponential function of network depth but not breadth (Poole et al., 2016). (The referenced paper has about 160 citations on Google Scholar, which is also an indicator.) Possibly, if this isn't it the one, it may cite or be cited by the paper that you are looking for. Link to Poole et al's (2016) paper: https:arxiv.orgabs1606.05340  edit: Typos and to say, it does not look like exactly the paper you are looking for, but there is some overlap at least!  [][deleted] 12 days ago(1 child) [deleted]  []adventuringraw 2 points3 points4 points 12 days ago(0 children) you're assuming the two models had different numbers of parameters. It'd be trivial to have a 'shallow' network with say, 2 hidden layers, and a 'deep' network with 10, and make sure both have exactly the same number of free parameters. A lot of the more foundational mathematical papers (Multilayer feedforward networks are universal approximators for one canonical example) focus on proving convergence for some property as the number of hidden nodes in a single hidden layer goes to infinity. Shallow does not imply low numbers of parameters. In fact, I think I saw another paper somewhere showing that deep networks require significantly less parameters to get the same training efficiency as shallow networks. Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 6212 on r2-app-0c0cc74c60fa2d5f3 at 2019-11-21 15:55:08.203938+00:00 running 65cc4c1 country code: US. "
  },
  "31":{
    "title":"[P] How to Identify Business Processes That Can Be Machine Learning-Enabled : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 1,568 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Project[P] How to Identify Business Processes That Can Be Machine Learning-Enabled (self.MachineLearning) submitted 13 days ago by cdossman Identifying Business Processes That Can Be Machine Learning-Enabled \u27a1 What to look for and expect when analyzing workflows for tasks can be automated with Machine Learning  https:medium.comai%C2%B3-theory-practice-businessidentifying-business-processes-that-can-be-ml-enabled-529cbe90aa84 Post a comment! there doesn't seem to be anything here Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 16339 on r2-app-0d287fb0f47e023d8 at 2019-11-21 16:35:56.737170+00:00 running 65cc4c1 country code: US. "
  },
  "32":{
    "title":"[R]Theoretical research paper in GAN's : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,231 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Research[R]Theoretical research paper in GAN's (self.MachineLearning) submitted 13 days ago by Mushr00mParadise Hello, I am doing some research about GAN's, and I am looking for some mathematicaltheoretical articles. I have noticed a lot of papers presenting new types of GAN's with (sometimes) just some minor alterations to the original. Two papers that fall within the category that I am looking for are: 'On the limitations of First-Order Approximation in GAN Dynamics' -> https:arxiv.orgpdf1706.09884.pdf  'Which training methods for GANs do actually converge?' -> https:arxiv.orgpdf1801.04406.pdf  They both start from a simple model and are then able to mathematically prove some properties and then to empirically demonstrate them.  I hope the question is clear, and thank you in advance! Post a comment!  []atlatic 2 points3 points4 points 13 days ago(0 children) Numerics of GANs.  []r-scholz 2 points3 points4 points 12 days ago(0 children) You may be interested in this paper: https:arxiv.orgabs1905.12103  []StellaAthena 2 points3 points4 points 13 days ago(3 children) You haven't asked a question at all. You've made an observation and linked to two papers. What is your question? Is it \u201cdo les there exist more theoretically oriented research on GANs?  []Mushr00mParadise[S] 2 points3 points4 points 13 days ago*(2 children) Yeah do there exist more theoretically oriented research on GANs is my question, thank you for the answer!  []StellaAthena 2 points3 points4 points 13 days ago(1 child) [Some Theoretical Properties of GANs](www.lsta.upmc.frBIAUbcst.pdf) might be a good paper to look at, and it references a significant amount of the existing literature.  []Mushr00mParadise[S] 0 points1 point2 points 13 days ago(0 children) Some Theoretical Properties of GANs Thank you for the help!  []tpapp157 1 point2 points3 points 13 days ago(0 children) There's been some interesting theoretical exploration of GANs from a Game Theory perspective and convergence to a Nash Equilibrium.  []Reiinakano 0 points1 point2 points 12 days ago(1 child) I know that this person has some theoretical papers on GANs that gain quite some attention on Twitter. However, I have not read them as I do not have the necessary math background https:scholar.google.comcitations?user=0qytQ1oAAAAJ&hl=en Also check out this blog https:www.inference.vcabout  []Mushr00mParadise[S] 0 points1 point2 points 10 days ago(0 children) Thank you! Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 612 on r2-app-0d287fb0f47e023d8 at 2019-11-21 17:22:14.841289+00:00 running 65cc4c1 country code: US. "
  },
  "33":{
    "title":"[R] My DeepMind internship project: Large-Scale GANs (BigGAN) : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,380 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Research[R] My DeepMind internship project: Large-Scale GANs (BigGAN) (arxiv.org) submitted 1 year ago by ajmooch Post a comment!  []ajmooch[S] 42 points43 points44 points 1 year ago(3 children) uchisai_mikan asked: In the same motivation as releasing pre-trained ImageNet weights, or architecturesaugmentation methods found using architecture search, it'll be great if the pre-trained networks (however large they may be) can be released for the community to use and tinker with. I can't promise anything, but I'm working through the process for this--there are a lot of institutional barriers, but I'm a big believer in open-source, and I'm doing what I can. If we can't get the official weights out I'll at least make sure there's some representative code on my github so that people have a reliable implementation to point to. One thing to bear in mind is that even with the training code and gradient accumulation it's literally impossible to train some of these models with anything less than the resources we use (due to cross-replica batchnorm being critical), so I'm currently thinking about ways to ameliorate that (maybe with another normalizer, or some BN tricks I've got squirreled away).  uRickMcCoy asked: So my question is, what's your main method for hyperparameter searching? What percentage of your research time was dedicated for it? I list basically all of the hyperparam searching in the paper, particularly in the final Appendix. I didn't really invest any time in hyperparameter search--I did a grid sweep over learning rates at the very start (before we'd scaled, so this was on N x 8GPU) and found that the settings from SA-GAN were best, so I just stuck with those until instabilities at large-scale gave me reason to halve the learning rates, and then I just stuck with those. The other sweeps aren't really hyperparameter searches in the sense of trying to optimize some metric, they're much more about investigating an effect--how does sample quality convergencecollapse speed stability vary as I change the amount of dropout L2 ortho reg? As I mention in the paper, I monitor gradient norms, losses, spectra, and sample evolution in detail, and there's a lot to be learned there beyond just scores. I think a lot about how those who have access to these incredible compute resources (me for one more week!) should best use them, and while I'm sure if you hooked up a couple of TPU pods and ran grid fine-grained grid search you could eke a few more Inception points out of these models, I just don't care about that kind of stuff. I try to avoid being in situations where I need to run grid search, and instead point resources towards actual experiments. I've launched over 3500 jobs since I got here in June, and while tons of those are just one offs (e.g. run this model and collect sample sheets and metrics, test that this new feature is working) basically all of the rest are what I'd call actual experiments--how does this idea affect perf, how do these two things interact, why does this work the way it does. When you're operating at this scale, you also have to be exceptionally parsimonious in how you schedule and manage your experiments and ideas. There literally isn't enough compute in existence for me to run grid search on the 512x512 models, so it was critical to be discerning in what settings to use, what to measure, etc.   []elder_price666 7 points8 points9 points 1 year ago(0 children) Congrats! This work is incredible and loved the detail regarding hyperparamsfailed experiments. We understand it can be sometimes tough to release code given institutional constraints (some of practical, some of it bureaucratic), but the community would appreciate it very much if the code (or even pretrained models!) would be released.  []FishZebra 0 points1 point2 points 1 year ago(1 child) Maybe in the future you can use random search for parameter optimization as explained here. It seems to perform better than grid search since many of the parameters actually do not contribute that much. Good job anyways!  []ajmooch[S] 10 points11 points12 points 1 year ago(0 children) As explained above, the goal of my HP searches is not to find the best value, but to measure, quantify, and understand an effect. Searching on regularly specified values is more suited to this.  []zergylord 40 points41 points42 points 1 year ago(5 children) I'm convinced half of this sub is secretly DeepMind employees ;)  []VernorVinge93 -5 points-4 points-3 points 1 year ago(4 children) Seems unlikely considering how many deepmind employees there... Google on the other hand...  []RSchaeffer 7 points8 points9 points 1 year ago(3 children) They have around 800 research scientists now, I think...  []bulletcross 3 points4 points5 points 1 year ago(2 children) What are they doing?  []ajmooch[S] 13 points14 points15 points 1 year ago(1 child) I can't speak for anyone else but I guess I just spend all day contemplating dogball.  []RSchaeffer 1 point2 points3 points 1 year ago(0 children) Beyond that, I don't know what anyone else could conceivably do :D  []ajmooch[S] 50 points51 points52 points 1 year ago(1 child) Hi everybody, I've been hanging around here slinging memes and dreams for the last couple of years, and it's been my absolute pleasure and privilege to be able to turn my career from loop jockey to, well, whatever you'd call what I'm doing now. This project is sort of an accidental work that started out as me implementing baselines for another project, but by the time I had a working baseline we'd already moved the SOTA by 15 points, so, uh, I never stopped making the baseline better. All of the investigations into instabilities started out with me saying oh these are probably just some simple numerical issue or bug in my code, let me get some diagnostics in there and see what's up. I don't think I've ever finished the exact research project I've set out to do (all but one of my publications thus far are like this), and I have no idea what the takeaway from that is, but this result is (I hope) fun and interesting (Dogball being my personal favorite result from this). I've been rolling a twitter thread about this work, what we learned, and a little bit of the insights we've gained from it, along some additional samples and lemonpicked samples that aren't in the sheets. I'll be answering questions here or on twitter; my DMs are open on both and of course you can always email me. And yes, I'm aware there's another thread on the front page, but I don't intend to nonymously comment on an anonymous submission page.  []IborkedyourGPU 30 points31 points32 points 1 year ago(0 children) What do you think of the double blind review process for conference papers?  []funny1man2 29 points30 points31 points 1 year ago(14 children) Perhaps you shouldn't be posting about this now. The review process is supposed to be double blind.  []i_know_about_things 33 points34 points35 points 1 year ago(3 children) Yes, nobody would have a clue who used all those TPUs if it was 100% double blind.  []gwern 14 points15 points16 points 1 year ago(2 children) The 293m image dataset was also a dead give-away. I was like, 'this is probably some Googler, since I haven't seen much use by researchers of the TPUs...' and then I hit the 293m image dataset they've used in some of the earlier scaling-CNNs papers and that clinched it.  []ajmooch[S] 5 points6 points7 points 1 year ago(1 child) To be clear, all of the samples are from ImageNet.  []gwern 1 point2 points3 points 1 year ago(0 children) Sure, but merely having access to that dataset is a dead give-away, which was my point. Although now that you mention it, it would be fun to compare samples, but since it's proprietary I'm not surprised that you can't share even samples (and definitely not models).  []maxToTheJ 9 points10 points11 points 1 year ago(0 children) The review process is supposed to be double blind. Seems to me the \u201cdouble blind review process is broken because isnt the whole point of the process to prevent a bias towards the big labs like google or its deepmind or FAIR.  At the same time these are the exact same groups both most likely to organize posting it publicly in ArXiv and have a PR team with an associated tech comms team push the result. Nobody is suggesting double blind to prevent a bias against \u201cjoe basic and the summary he posted on his blog with 10 visitors a month.  []Screye 7 points8 points9 points 1 year ago(3 children) double blind double blind is dead. It is very easy to tell even if the authors are anonymous, where the paper came from.With Axriv, it is almost entirely done for. I think open review has an interesting approach, where the paper is public and anonymous until conference results come in.However, submissions there are innately tied to a submission, and not independent like axriv. (correct me if I am wrong)  []maxToTheJ 1 point2 points3 points 1 year ago(2 children) Open Review also has its own biases especially against more theory and visualization side. I have read interesting papers from an understanding and visualization side that get mediocre reviews for not having SOTA results on some baseline set  []Screye 0 points1 point2 points 1 year ago(1 child) That has nothing to do with open review though. That is just the conference that it hosts. If an application visualization based conference is hosted on open review, then you can expect to see the papers you mention, get accepted .  []maxToTheJ 0 points1 point2 points 1 year ago(0 children) That has nothing to do with open review though. That is just the conference that it hosts. Kind of does because open review isn't effective as sold  []ajmooch[S] 6 points7 points8 points 1 year ago(3 children) I'm posting the arXiv paper, which is allowed by the conference rules for NIPSICMLICLR. I cannot prevent people from finding a public anonymous submission, and I cannot prevent people from sharing it and linking tohyping an anonymous submission page. Posting it publicly allows discussions to proceed nonymously, lets me get feedback and answer questions, and pulls the focus to the arXiv version rather than the anonymous page. Should the conference rules change to prohibit arXiv posts I will of course comply.  If you think this should change then consider contacting the conference chairs or starting a public discussion (preferably not in my thread =p).  []AGI_aint_happening 25 points26 points27 points 1 year ago(2 children) You are objectively violating the spirit, if not the letter, of double-blind reviewing. You could very easily get feedback and answer questions after the reviewing process is complete in 2.5 months. I know at least a couple senior researchers within Google feel the same.  []dwf 9 points10 points11 points 1 year ago(0 children) The spirit of something is by definition unwritten and therefore subjective. On the contrary, the rules explicitly state that arXiv preprints are allowed. If this is a faux pas, conference organizers have a powerful tool in their hands with which to shape behaviour: change the rules. Either forbid deanonymized preprints or abandon the pretense and go back to singly-blind.  []maxToTheJ 8 points9 points10 points 1 year ago(0 children) You are objectively violating the spirit Yup pulls the focus to the arXiv version rather than the anonymous page  []alexmlamb 4 points5 points6 points 1 year ago(0 children) So posting to arxiv and promoting is considered to be okay. In effect it's single blind + opt-out double-blind. So you can stay anonymous if you want, but you can also promote your paper.    []nicoulaj 4 points5 points6 points 1 year ago(1 child) Thank you for this great paper !  I didn't really get the section about discriminator instability: This confirms that D is indeed memorizing the training set; we deem this in line with D's role, which is not explicitly to generalize, but to distill the training data and provide a useful learning signal for G. This is very counter intuitive to me. So that means, if the training set is too large for the discriminator to memorize, that would degrade the performance of the model ?  []ajmooch[S] 7 points8 points9 points 1 year ago(0 children) Not necessarily. Our specific point with this statement is that it's not necessarily bad from a modeling standpoint if D memorizes the training set. It may be a piece of the stability puzzle (particularly with the hinge loss, if signal from the real samples starts to attenuate as D's outputs all end up beyond the margin) but it's not clear that D learning a compressed-but-memorized version of the training data is harmful to the learning process. FWIW we appear to observe that our models are stable for far longer when trained on a larger dataset, so this may in fact be a critical piece of the puzzle, but we don't have enough evidence to make a solid claim on that. A side note, even if G was memorizing the training data exactly, a generative model which (1) memorizes all the training points and (2) can smoothly and meaningfully interpolate between them, with lots of space in between, is something many people would consider a pretty good model.   []mathijs_p 7 points8 points9 points 1 year ago(1 child) This looks very interesting, I wish I had the computer power you have! I investigated sampling z-vectors from a more 'narrow' distribution in my bachelor's thesis, finding similar results (increasing quality, decreasing variety).    []ajmooch[S] 7 points8 points9 points 1 year ago(0 children) Nice! There's actually a couple concurrent works that do some of the same things we do, but yours definitely predates all this--I'll be sure to add the reference when we next update!  []mrconter1 3 points4 points5 points 1 year ago(2 children) Impressive! If you wouldn't mind answering me, what do you personally see as the most promising areas where GANs can be used?  []ajmooch[S] 9 points10 points11 points 1 year ago(1 child) Thanks! This is a good question--right now there's a lot of things that look like they could potentially be powerful, like real-time postprocessing super-res style things in games, maybe style transfer or CycleGAN style domain shifts that are directly learned and interpretably controlled, but for both algorithmic and computational reasons we're still a long ways off from those really taking hold (though there's lots of research pushing this direction). There's some recent work on audio style transfer as well, although GANs for sequences are still embryonic.  I'm personally more interested in GANs for their power as a representation learning method--I want to build models that can reliably factor complex visual structure into low-dimensional (ideally interpretable but not necessarily) representations, and I want to build methods to reason about these representations, both in situ, in conjunction with other types of networks and learned representations, and for downstream tasks. I'm really inspired by a lot of the related work in this area but for practical reasons it tends to focus on VAEs, and I think there's a lot of cool things to be done with implicit models.   []mrconter1 0 points1 point2 points 1 year ago(0 children) Thank you!  []Screye 3 points4 points5 points 1 year ago(2 children) You just posted this to make us jealous didn't ya ?  I am acquainted with GANs on a surface level.Have a couple of questions: Lastly, congrats.  []ajmooch[S] 6 points7 points8 points 1 year ago(1 child) Jealous? Nay, friend, merely to spread the Good Word (that of Dogball) These are all good questions! I haven't played much with KID but between IS and FID, FID is generally more reliable (there are lots of silly things that can happen around collapse, which make the IS go up when sample quality goes down, which don't affect FID). They're both exceptionally flawed, however--IS only really tells you broadly how objecty your samples are, assuming you are training on class-conditional ImageNet (the only dataset for which it is suited--no, really, IS is IMO completely useless outside of this unless you have another class-balanced single-object image dataset). FID gives you a broad sense of how much the overall statistics of your samples match the statistics of the training data, but this too is flawed (and dependent on a lot of things like preprocessing--we can actually get lower FID scores if we compare against moments computed with different preprocessing!) Other metrics like AIS might be, perhaps, more informative, but are too slow to be of practical use during training (maybe for final model comparison) but I haven't used it at all so I can't really say one way or the other. All this to say that we definitely need to move away from these metrics, but I didn't have the time or the paper space to do that with this work. Usually nearest neighbors tests and interpolations are the go-to here. Sanjeev Arora has this Birthday Paradox test, but IMO it's flawed (it doesn't measure how similar the latent vectors are, just how similar the outputs are, so I think it dramatically underestimates the support of the generator; if any followup work does address this please correct me on this front!). I think that memorization is only a problem if it cooccurs with collapse--if a large subspace of the latent space maps to a single output point which strongly resembles a training point, then you're memorizing in a bad way. If, however, you have a model which maps a single point in the latent space to each training point, but can interpolate between each of those training points smoothly and meaningfully (with plenty of space in between), then I would call that a good generative model. Actually measuring this is difficult and an open research question atm. This is an interesting one--it's sort of the cherrypicking vs random samples vs lemonpicking argument. The truncation trick is, in a way, a method for automatically filtering down to the best samples the model can produce, which could be analogous to paging through all your samples manually and grabbing the best ones. I included a link to the drive with sample sheets primarily so people could get a more complete sense for what the full range of our samples look like. I don't think that one has to give up consistency of sample quality to push the peak sample quality forward (IMO even our average-worst samples are higher quality than some of the previous best samples ~1 year ago), especially if you split along class lines. One of the reasons this is difficult to properly assess is that sample quality varies dramatically between classes, and the way this varies tends to be more a function of the data (how complex it is and how much of it you have relative to its complexity) rather than the quality of your model itself. ImageNet is, funnily enough, sort of a few-shot learning problem for generative models, at least for many of the classes (crowds and people in particular) where there's at most a few thousand images. I think that if you wanted to get consistent-across-class sample quality for all of ImageNet you might need to look into low-shot learning techniques; while you can definitely overfit a GAN to a small single-class dataset like CelebA-HQ, it's really hard to overfit a conditional model to a subset of the dataset, especially if that subset is different.  []Screye 0 points1 point2 points 1 year ago(0 children) Hey, thanks a lot. Those answers do clear a lot of my questions.  []unguided_deepness 2 points3 points4 points 1 year ago(0 children) Anyone want to see how well this can be applied to the danbooru dataset?  []matesteinforth 2 points3 points4 points 1 year ago(1 child) https:www.youtube.comwatch?v=YY6LrQSxIbc  []austospumanto 0 points1 point2 points 9 months ago(0 children) Thank you. Love this.  []zergling103 1 point2 points3 points 1 year ago(1 child) You should make a video like what the Progressively Growing GAN people did with their work.  []ajmooch[S] 3 points4 points5 points 1 year ago(0 children) Normally I do (I animated my last one and it took, unsurprisingly, a verrry long time) but I don't quite have enough time to put that together right now. I would really like to show some hour-long interps if I can get around to it!  []Phantine 1 point2 points3 points 1 year ago(1 child) Let's talk the biggest, most practical application possible: How feasible do you think it be to use bigGAN to generate card images for Roborosewater?  []ajmooch[S] 1 point2 points3 points 1 year ago(0 children) That's an excellent question--this may not only be the most practical application but possibly the single most important application possible. Maybe you could even condition it on the text--I would be very curious to see how the model tries to render Mointanspalk.  []scutzzy 1 point2 points3 points 1 year ago(4 children) Hello ajmooch, I have some trouble in understanding how to implement the shared embedding and how to combine these embedding with latent z in each cbn-layer. The channel size in each layer should be different and how do you projection the embedding to different channel size. I think that simply using the fully connected layer won't be able to reduce the number of parameters. Could you please tell me how you did it?  []ajmooch[S] 1 point2 points3 points 1 year ago(3 children) Normally: z = normal(size=(batch_size, 128)) Normally, this would go into the first linear layer which would be of shapeLinear(128, 1536 * 4*4) but if you use the hierarchical Z, you instead have (for example)) z = [normal(size=(batch_size, 16)) for _ in range(8)] Then the first linear layer receives z[0], and is thus of size Linear(16, 153644) Then every other layer, which would normally receive embedding = shared_embedding(y) in (batch_size, 128) And project that to the BN gains and biases via gain_i = projection_gain_i * embedding + 1 bias_i = projection_bias_i * embedding instead receives at each layer, concatenate([embedding, z[i]]) (concatenated along the channel axis). This slightly increases the number of parameters at each BN layer but sharply decreases the number of params in the first linear layer. It's a pretty simple architectural difference but it seems to speed up training (to convergencecrash)  []scutzzy 1 point2 points3 points 1 year ago(2 children) Thank you for your detailed explanation. But I still don't fully understand. Let's assuming h = concatenate([embedding, z[i]]) whose shape should be (batch_size, 128+16), and assuming i_th layer has input shape(batch_size,512,4,4). Then what you mean is to multiply h with a matrix (128+16, 512) to get each channel's gain_i and bias_i?   []ajmooch[S] 0 points1 point2 points 1 year ago(1 child) Yep! separate matrix for each gain_i and bias_i  []scutzzy 0 points1 point2 points 1 year ago(0 children) I get it, thank you very much.  []bartturner 2 points3 points4 points 1 year ago(0 children) Just love how GoogleDeepMind share this type of thing.   []Phantine 0 points1 point2 points 1 year ago(4 children) There's what, 12000+ total minutes of The Simpsons produced? The simpsons is animated at 24 frames per second ... How good a performance do you think you could by training it on 17 million frames from the same source?  []ajmooch[S] 2 points3 points4 points 1 year ago(3 children) Good question--I think convolutional architectures aren't as well-suited to doing line drawings and solid colors, (in an ideal world I'd use SPIRAL for that) but given the sheer amount of data it would probably work fine. I've actually been kicking around some ideas for something like this but with more heuristics and traditional CV in the mix to get data-driven user-controllable cartoon models, but it's not something I have the time or resources for right now, and it's not really something I could justify using the Google compute for. Maybe if we manage to get more compute-efficient (if you can get BigGAN results with 11000th of the FLOP-hours we used that would be a big win) it would be approachable on a home or 4GPU or 8GPU machine.  []Vermeille 2 points3 points4 points 1 year ago(2 children) GANime (IIRC) has some tricks to enforce lines with sharp edges: they feed, to the discriminator: so D really learns to discriminates against blurred edges and G learns sharp edges :)  []ajmooch[S] 1 point2 points3 points 1 year ago(1 child) Ah, clever! I hadn't considered that idea--augmenting the inputs to D to encourage a particular behavior is really interesting!  []Phantine 0 points1 point2 points 1 year ago(0 children) I suppose another option is training it to use a different output format entirely. In principle I could see having the outputs correspond to vector lines and polygons instead of individual pixels being easier to train with, since the output image isn't going to have a lot of shading. (Btw have you seen the previews for the next style2paints? https:twitter.comIlIIlIIIllIllII )  []gagarin_kid 0 points1 point2 points 10 months ago(0 children) Great work! What do you think about the future of generative methods? Where do you see further improvements - is it the scale in resources (like you did) or some algorithmic tweaks?  []mpividori 0 points1 point2 points 10 months ago(0 children) The example use code included in the TensorflowHub's modules: biggan-128, biggan-256 and biggan-512, seems to implement something different to the truncation trick described in the original paper: Large Scale GAN Training for High Fidelity Natural Image Synthesis.I mention the difference between both distribution in a github issue: https:github.comtensorflowhubissues214 uajmooch which latent distribution have you considered for the experiments mentioned in the paper? Is it a real truncated normal?  []Overload175 -1 points0 points1 point 1 year ago(0 children) These results(as evidenced by the Inception Score and the FID) are truly incredible.  Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 4074 on r2-app-021a24231eab972ba at 2019-11-21 23:41:17.946556+00:00 running 7e0f65a country code: US. "
  },
  "34":{
    "title":"[P] These Lyrics Do Not Exist : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,216 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  Project[P] These Lyrics Do Not Exist (self.MachineLearning) submitted 3 months ago * by itsmybirthday19 Post a comment!  []itsmybirthday19[S] 76 points77 points78 points 3 months ago(3 children) Input: \ud83c\udf7a (beer emoji) Output:  []johnminadeo 12 points13 points14 points 3 months ago(0 children) That's simply amazing!  [][deleted] 1 point2 points3 points 3 months ago(0 children) That's wonderful, makes me want to seek out a chord progression generator and try to make it work  []Observer0x1a4 40 points41 points42 points 3 months ago(4 children) Someone needs to run this through a text-to-speech and auto tune with a beat. My wife and I are cracking up over these!  []crikeydilehunter 4 points5 points6 points 3 months ago(3 children) Way ahead of you  []cpury 2 points3 points4 points 3 months ago(1 child) OMG please share your results!  []crikeydilehunter 0 points1 point2 points 3 months ago(0 children) Ok, so I decided to make a full song and it's taking longer. Don't worry, I didn't forget!  [][deleted] 1 point2 points3 points 3 months ago(0 children) please post results  []catandDuck 12 points13 points14 points 3 months ago(1 child) Verse 1And you're f**king your girlfriend Go out to the club. Stay home it's time to do it all night! And you're f**king your girlfriend Pre-ChorusShe asked if I had a girlfriend well I do That's what my girlfriend told me ChorusWasn't my girlfriend on the day? Wasn't my girlfriend on the day? I have a girlfriend and my heart is not that strong I have a girlfriend and my heart is not that strong Verse 2And you can't touch your girlfriend If she don't dig the men It doesn't make your friend a bad guy Yeah you can't touch your girlfriend Pre-ChorusHe's got a girlfriend too He's got a girlfriend too ChorusWasn't my girlfriend on the day? Wasn't my girlfriend on the day? I have a girlfriend and my heart is not that strong I have a girlfriend and my heart is not that strong BridgeHang up the phone girlfriend Hang up the phone girlfriend ChorusWasn't my girlfriend on the day? Wasn't my girlfriend on the day? I have a girlfriend and my heart is not that strong I have a girlfriend and my heart is not that strong  []itsmybirthday19[S] 4 points5 points6 points 3 months ago(0 children) Sounds like a country song \ud83e\udd20  []itsmybirthday19[S] 11 points12 points13 points 3 months ago(2 children) Example output:  []HDorillion 14 points15 points16 points 3 months ago(1 child) Was the input, by any chance, Love  []itsmybirthday19[S] 7 points8 points9 points 3 months ago(0 children) Ha yes! \ud83d\ude0a  []localkhaleesi 10 points11 points12 points 3 months ago(1 child) Input: fuck this is amazing.  []crikeydilehunter 5 points6 points7 points 3 months ago(0 children) Speaking the language of the gods  []the_zword 9 points10 points11 points 3 months ago(1 child) This is my new favorite way to pass time.  []itsmybirthday19[S] 4 points5 points6 points 3 months ago(0 children) Ha great to hear! \ud83d\ude01  []lily9423 6 points7 points8 points 3 months ago*(0 children) One thought about generating lyrics with consistent theme \u2014 repetition of the topic word is a good start, but could extend to a set of relative peripheral words, and further a storyline!! Think about writing a poem, we could express a topic from different perspectives. Not sure how your algorithm works but I'd suggest the above. Also curious about if there's any quantitative metric of evaluating the quality of generated lyrics?  []vincefav 4 points5 points6 points 3 months ago*(0 children) I've been working on this myself! How low were you able to get your loss during training? I'm stuck around 1.80 right now with 10,000 songs, and lowering the learning rate doesn't seem to help. Any tips? I implemented a few interesting features along the way and I'd be happy to share them.  []poindexter_one 4 points5 points6 points 3 months ago(0 children) Amusing Cheese. (c) 2019 all rights reserved. All I wanted was the cheeseOf a boy who had it all and wants to go backAnd I'll be caught at seaWhen you cheese you you always be inside so tight that you gotta bring me paper Pre-ChorusAnd he looked at me as though I were a piece of cheeseSpit in cheese how could it go wrong Who would own the rights to AI generated creative work? The first lawsuit will be \ud83d\ude02  []OPMaster494 4 points5 points6 points 3 months ago(1 child) Amazing! Looking forward to more cool projects you've got!  []itsmybirthday19[S] 1 point2 points3 points 3 months ago*(0 children) Thanks man! \ud83d\ude0e  []Brudaks 3 points4 points5 points 3 months ago(3 children) The text are nice and inspiring, but currently it's lyrics mainly because of the imposed lineversechorus structure, but If I'm thinking on how to put these words to some random music, then it's very hard to get any rhythm working. So my main suggestion for improvement would be to do some filtering based on syllable count or something like that; i.e. if you have two lines of five syllables then don't put a ten-syllable third line, regenerate it until you get something shorter.  []itsmybirthday19[S] 2 points3 points4 points 3 months ago(2 children) Thanks heaps for your feedback Yes, I am with another project that is working on melody creation (aka HookGen.com) I am currently working on syllable count, you can then select if you want solfege (i.e. movable do or fixed do) I am currently training on two separate RTX 2080 Tis; which are doing separate experiments to create new beats The idea is that the three projects; melody + lyrics + beat will join together by 2020 Currently I generated this song entirely with AI; if you DM me your email I can keep you updated Link: https:soundcloud.comdjanotherplanetbe-mine  []kayvane 1 point2 points3 points 3 months ago(0 children) Very Impressive!! Do you generate multiple midi's an overlay them? Or do you have a more complex network which takes inputs from multiple sequences (instruments) to generate the next note?  []LifeFeedback 0 points1 point2 points 3 months ago(0 children) Can you say where do you get your dataset? :P  []anor_wondo 2 points3 points4 points 3 months ago(0 children) some of it sounds like nier automata robots  [][deleted] 3 months ago*(1 child) [deleted]  [][deleted] 1 point2 points3 points 3 months ago(0 children) early Swans  []BubbyMk2 2 points3 points4 points 3 months ago(0 children) Wow, this looks really awesome! I love the idea, and full song generation seems doable (melody + lyrics + beat). The melody generation you've got going on over at HookGen seems really well done, too :) Can you provide any details of the network running under the hood? Are you directly using a pretrained model (GPT2, BERT, XLNet, etc.) + fine tuning autoregressively on lyrics, or is there something more complex going on?  Also, I saw you're planning on improving with longer training + 17x more data. Are you able to give any info on what that currently looks like? ie. how much data you're using + how long you're training on your 2080Tis?  []SOSRihanna 2 points3 points4 points 3 months ago(1 child) [removed] Why did you remove the text of the post? What's the link?  []itsmybirthday19[S] 1 point2 points3 points 3 months ago(0 children) Shows up for me? https:theselyricsdonotexist.com  []sgxadhith 1 point2 points3 points 3 months ago(1 child) Amazing!  []itsmybirthday19[S] 0 points1 point2 points 3 months ago(0 children) Thanks \ud83d\ude0a  [][deleted] 1 point2 points3 points 3 months ago(2 children) Shark, housework and death came up with nothing....  []itsmybirthday19[S] 3 points4 points5 points 3 months ago(1 child) yep, I banned a bunch of words including death, murder, kill and the n word etc  [][deleted] 4 points5 points6 points 3 months ago(0 children) I keep looking for a song to motivate me to do housework. Still has yet to be created.  [][deleted] 1 point2 points3 points 3 months ago(0 children) Topic: Pain  []DaddySatoshi 1 point2 points3 points 3 months ago(0 children) input: france output: Verse 1 Well she's confused now isn't France? How can you say something's a shame? I'll be your shelter from the storm France waited for me her girl is now all mine  Pre-Chorus Well she's out on the highway And if by chance France should meet another  Chorus She's the kind I was hoping for France sings I am sitting still And France likes the way France feels inside Ever since France told me France was  Verse 2 France come down from the stars late night France comes down from the stars late night Now I can feel this evening cold And France believe that she's already been this kind  Pre-Chorus With skin so pale France could be white France stayed out all night long  Chorus She's the kind I was hoping for France sings I am sitting still And France likes the way France feels inside Ever since France told me France was  Bridge France will stay with you France won't leave you no no  Chorus She's the kind I was hoping for France sings I am sitting still And France likes the way France feels inside Ever since France told me France was  []semikore 1 point2 points3 points 3 months ago(0 children) No results with 'gruesome deaths' and 'surgical procedure' :( I guess I'll have to write my own lyrics for my metal band ...  []occasionalupvote 1 point2 points3 points 3 months ago(0 children)  []Gardrothard 1 point2 points3 points 3 months ago(0 children) Awesome! Great job!  []KayNora 1 point2 points3 points 3 months ago(0 children) Simply amazing!  []frequenttimetraveler 1 point2 points3 points 3 months ago(0 children) Awesome!. Man you just need to train it to output chords on top of the verses will be hilarious.  []gorillaSpices 1 point2 points3 points 3 months ago(0 children) Type in Obama..  []bagi45654 1 point2 points3 points 3 months ago(0 children) Input weed <pre>Verse 1But you don't have to weed your life or you won'tBut you don't have to smoke just make the feeling stayI must for my nationHit him with this weed Pre-ChorusI see the weed I don't need the shotI'll be the flowers after you weed me out ChorusLook out I got kidsLook out I got weedLook out I got kidsLook out I got weed Verse 2Don't roll no weed on the floorAnd I dont gotta sell weed to get burntWhat's that I hear now you're right hereWhat's that I hear now I feel so proud Pre-ChorusI got weed? I got weed in my pocketSo I grow mine in a little shade ChorusLook out I got kidsLook out I got weedLook out I got kidsLook out I got weed BridgeGrowing like a weed without flowersGrowing like a weed without weather ChorusLook out I got kidsLook out I got weedLook out I got kidsLook out I got weed Result #4340Generated using TheseLyricsDoNotExist.com<pre>  []not_personal_choice 1 point2 points3 points 3 months ago(0 children) The author has confused eneutions about beer Verse 1I wouldn't care whose \ud83c\udf7a I drinkAs long as somebody somewhere has fun.If fun is what you're looking forWe weren't gonna drink no \ud83c\udf7a  Pre-ChorusHere come the boys with the wild \ud83c\udf7a fightingConstant to hate \ud83c\udf7a and silver and gold  ChorusIt's like throwing a ball that leaves a trail of \ud83c\udf7aIt's like throwing a ball that leaves a trail of \ud83c\udf7aI'll be your hope I'll be your light I'll be the one you reachWhen you need to reach.  Verse 2Well my old head momma said that drinking \ud83c\udf7aIs not the way that a good man should actI'm wrong I'm wrong and that's all right with meI want to buy you a \ud83c\udf7a  Pre-ChorusNot a drug or a \ud83c\udf7aWe've got cocaine and pills and \ud83c\udf7a  ChorusIt's like throwing a ball that leaves a trail of \ud83c\udf7aIt's like throwing a ball that leaves a trail of \ud83c\udf7aI'll be your hope I'll be your light I'll be the one you reachWhen you need to reach.  BridgeOne more \ud83c\udf7a and I'll be in no state to do the things I think I need toWasn't this \ud83c\udf7a drinking dog the one's that had to always run  ChorusIt's like throwing a ball that leaves a trail of \ud83c\udf7aIt's like throwing a ball that leaves a trail of \ud83c\udf7aI'll be your hope I'll be your light I'll be the one you reachWhen you need to reach.  []turbod33 1 point2 points3 points 3 months ago(0 children) Well they exist, now.  []preemptive_regret 1 point2 points3 points 3 months ago(0 children) This is absolutely hilarious. Sent to all my friends and family and we are having the best time with it. Thanks for sharing!  []ledewde_ 1 point2 points3 points 3 months ago(0 children) Verse 1The beach is moving but not so slowlyWhen it's just you and no one else to talk toWho wants to sit and talkHe is with us on the beach Pre-ChorusAnd its saturday morning and its his moms out on the beachAnd then I asked him what we were doing on the beach ChorusYou might see us on the beachI might start to freak outYou're getting what you can't have.I was born in the valley of bullshit diamond beach Verse 2Come and dance in the beach 'til the break of dawnBaby I'll be riding baby with the pages in the pageCome and dance in the beach 'til the break of dawnYou know I feel it coming on Pre-ChorusThe beach and drink till drunkSaturday I'll probably spend some time at the beach ChorusYou might see us on the beachI might start to freak outYou're getting what you can't have.I was born in the valley of bullshit diamond beach BridgeI'm that girl you used to see at the beachWhere we used to hang out ChorusYou might see us on the beachI might start to freak outYou're getting what you can't have.I was born in the valley of bullshit diamond beach  []usehand 0 points1 point2 points 3 months ago(0 children) What kind of model are you using here? Is it GPT-2?  []jetcoff 0 points1 point2 points 3 months ago(0 children) Total noob question, but did you write the songwriter in Python or JavaScript? If Python, how did you add the generator to a web page.  []dsengupta16 0 points1 point2 points 3 months ago(0 children) Wonderful man. This is what real learning should be about. At the end you should be able to experience the joy of building something by your own hands and intellect.  []lostmsu -1 points0 points1 point 3 months ago(0 children) http:billion.dev.losttech.software:2095song1070353314  []juancamilog -5 points-4 points-3 points 3 months ago(2 children) sorry to poop on your party, but these are not completely original.  []vineetk1998 -1 points0 points1 point 3 months ago(0 children) and they doesn't make any sense  [+][deleted] 3 months ago(1 child) [deleted]  []carlthome 2 points3 points4 points 3 months ago(0 children) You have to open your mind to the idea that some big ass God shit is a youthful way of referring to romantic love. Peace. Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 20214 on r2-app-0682b52d812d1f207 at 2019-11-22 00:16:22.126919+00:00 running bbb69ca country code: US. "
  },
  "35":{
    "title":"[N] Open-Unmix for Music Separation : MachineLearning",
    "body":"use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... 2,046 users here now Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try rLearnMachineLearning , rMLQuestions or http:stackoverflow.com For career related questions, visit rcscareerquestions Advanced Courses AMAs: Pluribus Poker AI Team 7192019 DeepMind AlphaStar team (1242019) Libratus Poker AI Team (12182017) DeepMind AlphaGo Team (10192017) Google Brain Team (9172017) Google Brain Team (8112016) The MalariaSpot Team (262016) OpenAI Research Team (192016) Nando de Freitas (12262015) Andrew Ng and Adam Coates (4152015) J\u00fcrgen Schmidhuber (342015) Geoffrey Hinton (11102014) Michael Jordan (9102014) Yann LeCun (5152014) Yoshua Bengio (2272014) Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions rMLjobs and rBigDataJobs rdatacleaning rDataScience rscientificresearch rartificial the front page of the internet. and join one of thousands of communities.  News[N] Open-Unmix for Music Separation (self.MachineLearning) submitted 2 months ago by faroit \ud83d\udcdcPaper: https:joss.theoj.orgpapers571753bc54c5d6dd36382c3d801de41d\ud83d\udd0aDemo: https:open.unmix.app\ud83d\udd25PyTorch: https:github.comsigsepopen-unmix-pytorch\ud83d\udd3bNNabla: https:github.comsigsepopen-unmix-nnabla\ud83d\udd36TF2: t.b.a.\ud83d\udcd3Colab: https:colab.research.google.comdrive1mijF0zGWxN-KaxTnd0q6hayAlrID5fEQ  It is our great pleasure to announce the release of Open-unmix, a MIT-licensed python implementation for DNN-based music separation. In the recent years, deep learning-based systems could break a long-standing crystal ceiling, and finally allow high-quality music separation. This provoked a raising interest from both the industry and the machine learning community (like rML) However, until now, no open-source implementation was available that matches the performance of the best systems proposed more than four years ago. This lead to a waste of time from both the points of view of sheer performance optimization and scientific comparison with the state of the art. Not being able to reproduce state of the art performance makes it difficult to clearly identify the sources for discrepancies and rooms for improvement. In this context, we release Open-Unmix (UMX) as closing this gap by providing a reference implementation for DNN-based music separation. It serves two main purposes. First, it is intended to academic researchers for serving as a baseline method that is easy to compare to and build upon. Second, the availability of a pre-trained model allows bringing music separation to the enthusiastic end users and artists. Open-unmix is presented in a paper that has just been published in the Journal of Open Source Software. You may download the paper PDF here Open-unmix comes in several DNN frameworks: https:sigsep.github.ioopen-unmix Open-unmix has been especially designed to combine well with the following datasets: => Both datasets are available at https:sigsep.github.iodatasetsmusdb.html Note: If you want to compare separation models to existing source separation literature or if want compare to SiSEC 2018 participants, please use the standard MUSDB18 dataset, instead. We provide pre-trained models trained on both MUSDB18 and MUSDB18-HQ that reach state-of-the-art performance of 6.32 dB SDR (median of medians) on vocals on MUSDB18 test data. This significantly outperforms any model we are aware of that was trained on MUSDB18 only. The pre-trained models are automatically bundleddownloaded when using the pytorch implementation. Further information for both models such as evaluation scores can be downloaded from zenodo: Open-unmix was recently proposed during a tutorial held at EUSIPCO 2019. This features: The slides of the tutorial as well as self-contained colab notebooks can be found on the tutorial site. Open-unmix is part of a whole ecosystem enabling easy research on source separation for Python users. Several distinct and independent projects were released in the recent years in this effort to make it possible for researchers to reproduce state of the art performance in this domain. A reliable python package that implements the multichannel wiener filter and related filtering methods. https:github.comsigsepnorbert We released the new version 0.3.0 of our popular musdb tools. This releases makes it simpler to use musdb inside your data loading framework thus we pro https:github.comsigsepsigsep-mus-db museval makes it easy to compare the performance of any new method under investigation to both Open-unmix and the participants of SiSEC18. https:github.comsigsepsigsep-mus-eval Please note that we are also working on some version of open-unmix that has been trained on a significantly larger dataset and that achieves unprecedented separation performance. Please feel free to contact us for demonstrations industrial collaborations licensing on this matter. We look forward to your feedback and we hope that you will find Open-unmix useful! Post a comment!  []hakantakiri 2 points3 points4 points 2 months ago(1 child) Awesome! I'll play with it for hours. Just to point out a little error: your inference.md reference in Using the PyTorch version section is giving a 404 error  []faroit[S] 0 points1 point2 points 2 months ago(0 children) thanks for the pointer. I just fixed the dead link  []data-alchemy 0 points1 point2 points 2 months ago(1 child) I've been giving it a try. As a humble engineer and as a musician, results are just impressive. Some songs may not work properly if (I guess) they are too far away from the original dataset, but some extracts are nearly perfect. Remix as an art is going to know a new era :)  []faroit[S] 1 point2 points3 points 2 months ago(0 children) thanks. We would actually like to share more commercial tracks. Do you know what would be the best place to share exerpts of separated tracks? youtube? soundcloud?  []sicp4lyfe 0 points1 point2 points 2 months ago(1 child) Very nice. Can your algorithm work online or does inference need to process the whole song?  []faroit[S] 1 point2 points3 points 2 months ago(0 children) the model has a parameter that allows to do inference online. However the pre-trained model we released (umx and umxhq) is using information from the full track because that does slightly improve the quality  []Maplernothaxor 0 points1 point2 points 2 months ago(1 child) How long does the notebook YouTube example take to run? Been running it on the provided video and it hasn't produced results in an hour.  []faroit[S] 0 points1 point2 points 2 months ago(0 children) Hi. Thanks for your feedback. I just testet it again (using the GPU kernel). The 30s REM track took less than 10s to process. You might want to watch if the kernel has crashed. That happens sometimes  []ChapDDR 0 points1 point2 points 1 month ago(1 child) I am looking into speechmusic separation to use in for example tv-shows. While the paper and examples mainly try to obtain separate instruments from the mixture, could this model also be trained to separate normal (acted) speech and music from noisy movie audio? Or should I use a more generic speech separation algorithm?  []faroit[S] 0 points1 point2 points 1 month ago(0 children) open-unmix is only trained on singing voice. While this is still a valid task in a broadcasting scenario, I would recommend to try a speech separation first. Use of this site constitutes acceptance of our User Agreement and Privacy Policy. 2019 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology Rendered by PID 28149 on r2-app-0d52253d52c07e3b5 at 2019-11-22 15:40:58.350770+00:00 running 611e8fd country code: US. "
  }
}